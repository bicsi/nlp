{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, recall_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVR, SVC\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as tfk\n",
    "import tensorflow.keras.layers as tfkl\n",
    "import gensim\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import gensim.downloader as api\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/roboself/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /Users/roboself/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<text8.Dataset at 0x1556f6910>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('universal_tagset')\n",
    "api.load('text8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_df(filename, names):\n",
    "    df = pd.read_csv(filename, sep='\\t', header=None, names=names)\n",
    "    df[\"s_sub_token_len\"] = [len(s.split()) for s in df[\"sub\"]]\n",
    "    df[\"s_sub_char_len\"] = [len(s) for s in df[\"sub\"]]\n",
    "    df[\"s_sub_mean_word_len\"] = df[\"s_sub_char_len\"] / df[\"s_sub_token_len\"]\n",
    "    df[\"s_capitalized\"] = [len([c for c in s if c.isupper()]) for s in df[\"sub\"]]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>l</th>\n",
       "      <th>r</th>\n",
       "      <th>n1</th>\n",
       "      <th>n2</th>\n",
       "      <th>c1</th>\n",
       "      <th>c2</th>\n",
       "      <th>p</th>\n",
       "      <th>s_sub_token_len</th>\n",
       "      <th>s_sub_char_len</th>\n",
       "      <th>s_sub_mean_word_len</th>\n",
       "      <th>s_capitalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>14002.00000</td>\n",
       "      <td>14002.000000</td>\n",
       "      <td>14002.000000</td>\n",
       "      <td>14002.0</td>\n",
       "      <td>14002.0</td>\n",
       "      <td>14002.000000</td>\n",
       "      <td>14002.000000</td>\n",
       "      <td>14002.000000</td>\n",
       "      <td>14002.000000</td>\n",
       "      <td>14002.000000</td>\n",
       "      <td>14002.000000</td>\n",
       "      <td>14002.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7001.50000</td>\n",
       "      <td>83.727753</td>\n",
       "      <td>92.100486</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.902014</td>\n",
       "      <td>0.860591</td>\n",
       "      <td>0.088130</td>\n",
       "      <td>1.220968</td>\n",
       "      <td>8.372732</td>\n",
       "      <td>6.816589</td>\n",
       "      <td>0.282745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4042.17357</td>\n",
       "      <td>66.602408</td>\n",
       "      <td>66.819266</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.949611</td>\n",
       "      <td>1.894848</td>\n",
       "      <td>0.181183</td>\n",
       "      <td>0.630302</td>\n",
       "      <td>5.086451</td>\n",
       "      <td>2.225215</td>\n",
       "      <td>0.577635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3501.25000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7001.50000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>10501.75000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>129.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>14002.00000</td>\n",
       "      <td>647.000000</td>\n",
       "      <td>656.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               idx             l             r       n1       n2  \\\n",
       "count  14002.00000  14002.000000  14002.000000  14002.0  14002.0   \n",
       "mean    7001.50000     83.727753     92.100486     10.0     10.0   \n",
       "std     4042.17357     66.602408     66.819266      0.0      0.0   \n",
       "min        1.00000      0.000000      2.000000     10.0     10.0   \n",
       "25%     3501.25000     32.000000     40.000000     10.0     10.0   \n",
       "50%     7001.50000     71.000000     79.000000     10.0     10.0   \n",
       "75%    10501.75000    120.000000    129.000000     10.0     10.0   \n",
       "max    14002.00000    647.000000    656.000000     10.0     10.0   \n",
       "\n",
       "                 c1            c2             p  s_sub_token_len  \\\n",
       "count  14002.000000  14002.000000  14002.000000     14002.000000   \n",
       "mean       0.902014      0.860591      0.088130         1.220968   \n",
       "std        1.949611      1.894848      0.181183         0.630302   \n",
       "min        0.000000      0.000000      0.000000         1.000000   \n",
       "25%        0.000000      0.000000      0.000000         1.000000   \n",
       "50%        0.000000      0.000000      0.000000         1.000000   \n",
       "75%        1.000000      1.000000      0.100000         1.000000   \n",
       "max       10.000000     10.000000      1.000000        11.000000   \n",
       "\n",
       "       s_sub_char_len  s_sub_mean_word_len  s_capitalized  \n",
       "count    14002.000000         14002.000000   14002.000000  \n",
       "mean         8.372732             6.816589       0.282745  \n",
       "std          5.086451             2.225215       0.577635  \n",
       "min          2.000000             2.000000       0.000000  \n",
       "25%          5.000000             5.000000       0.000000  \n",
       "50%          7.000000             7.000000       0.000000  \n",
       "75%          9.000000             8.000000       0.000000  \n",
       "max         49.000000            21.000000       9.000000  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COL_NAMES = [\"idx\", \"text\", \"l\", \"r\", \"sub\", \"n1\", \"n2\", \"c1\", \"c2\", \"p\"]\n",
    "df = read_df('data/train_full.txt', COL_NAMES)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "      <th>l</th>\n",
       "      <th>r</th>\n",
       "      <th>sub</th>\n",
       "      <th>n1</th>\n",
       "      <th>n2</th>\n",
       "      <th>c1</th>\n",
       "      <th>c2</th>\n",
       "      <th>p</th>\n",
       "      <th>s_sub_token_len</th>\n",
       "      <th>s_sub_char_len</th>\n",
       "      <th>s_sub_mean_word_len</th>\n",
       "      <th>s_capitalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7921</th>\n",
       "      <td>7922</td>\n",
       "      <td>His speech came as an already difficult relati...</td>\n",
       "      <td>77</td>\n",
       "      <td>85</td>\n",
       "      <td>strained</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>423</td>\n",
       "      <td>In northern Lebanon, meanwhile, residents said...</td>\n",
       "      <td>119</td>\n",
       "      <td>125</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4133</th>\n",
       "      <td>4134</td>\n",
       "      <td>The Taliban claimed responsibility for the ass...</td>\n",
       "      <td>58</td>\n",
       "      <td>65</td>\n",
       "      <td>heavily</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4227</th>\n",
       "      <td>4228</td>\n",
       "      <td>While the government has taken steps to suppor...</td>\n",
       "      <td>21</td>\n",
       "      <td>36</td>\n",
       "      <td>has taken steps</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6909</th>\n",
       "      <td>6910</td>\n",
       "      <td>Even a majority of Republicans hold a negative...</td>\n",
       "      <td>59</td>\n",
       "      <td>67</td>\n",
       "      <td>conflict</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4174</th>\n",
       "      <td>4175</td>\n",
       "      <td>The attack on Rastan came after Syrian forces ...</td>\n",
       "      <td>142</td>\n",
       "      <td>147</td>\n",
       "      <td>homes</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4664</th>\n",
       "      <td>4665</td>\n",
       "      <td>Israel's outgoing ambassador to Egypt arrived ...</td>\n",
       "      <td>155</td>\n",
       "      <td>161</td>\n",
       "      <td>forced</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1440</th>\n",
       "      <td>1441</td>\n",
       "      <td>Regulators in the US and elsewhere have stress...</td>\n",
       "      <td>198</td>\n",
       "      <td>214</td>\n",
       "      <td>unfair advantage</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.20</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9113</th>\n",
       "      <td>9114</td>\n",
       "      <td>U.S. military's future in Afghanistan Issued t...</td>\n",
       "      <td>231</td>\n",
       "      <td>240</td>\n",
       "      <td>increased</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13956</th>\n",
       "      <td>13957</td>\n",
       "      <td>This trip to Afghanistan is an attempt to shor...</td>\n",
       "      <td>176</td>\n",
       "      <td>183</td>\n",
       "      <td>defense</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6992</th>\n",
       "      <td>6993</td>\n",
       "      <td>China and the Philippines have been disputing ...</td>\n",
       "      <td>140</td>\n",
       "      <td>143</td>\n",
       "      <td>Sea</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5203</th>\n",
       "      <td>5204</td>\n",
       "      <td>The Philippines accused Chinese vessels of fir...</td>\n",
       "      <td>98</td>\n",
       "      <td>133</td>\n",
       "      <td>harassing an oil exploration vessel</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5</td>\n",
       "      <td>35</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8122</th>\n",
       "      <td>8123</td>\n",
       "      <td>It's sad, even if they were troublemakers.</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>'s</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6528</th>\n",
       "      <td>6529</td>\n",
       "      <td>Russian authorities should annul the parliamen...</td>\n",
       "      <td>114</td>\n",
       "      <td>123</td>\n",
       "      <td>Gorbachev</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10162</th>\n",
       "      <td>10163</td>\n",
       "      <td>Israel's internal security agency, Shin Bet, w...</td>\n",
       "      <td>259</td>\n",
       "      <td>267</td>\n",
       "      <td>upgraded</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3666</th>\n",
       "      <td>3667</td>\n",
       "      <td>Google Inc. (GOOG) won approval from Chinese r...</td>\n",
       "      <td>399</td>\n",
       "      <td>407</td>\n",
       "      <td>approved</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13428</th>\n",
       "      <td>13429</td>\n",
       "      <td>The Philippine government said Wednesday it ag...</td>\n",
       "      <td>80</td>\n",
       "      <td>87</td>\n",
       "      <td>resolve</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.15</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3838</th>\n",
       "      <td>3839</td>\n",
       "      <td>Rastan, 25 km (15 miles) north of Homs city, h...</td>\n",
       "      <td>98</td>\n",
       "      <td>103</td>\n",
       "      <td>times</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6198</th>\n",
       "      <td>6199</td>\n",
       "      <td>The gunman fled in a silver saloon car that wa...</td>\n",
       "      <td>21</td>\n",
       "      <td>34</td>\n",
       "      <td>silver saloon</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8330</th>\n",
       "      <td>8331</td>\n",
       "      <td>The Wall Street Journal reported earlier this ...</td>\n",
       "      <td>226</td>\n",
       "      <td>234</td>\n",
       "      <td>hardware</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8679</th>\n",
       "      <td>8680</td>\n",
       "      <td>He then gave a speech broadcast to Americans b...</td>\n",
       "      <td>45</td>\n",
       "      <td>49</td>\n",
       "      <td>back</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5183</th>\n",
       "      <td>5184</td>\n",
       "      <td>The Philippines and Vietnam complained last ye...</td>\n",
       "      <td>93</td>\n",
       "      <td>100</td>\n",
       "      <td>staking</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0.45</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9705</th>\n",
       "      <td>9706</td>\n",
       "      <td>Afghan security forces swarmed the  area and...</td>\n",
       "      <td>38</td>\n",
       "      <td>42</td>\n",
       "      <td>area</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5106</th>\n",
       "      <td>5107</td>\n",
       "      <td>Gardaí say witnesses at the scene describe a s...</td>\n",
       "      <td>59</td>\n",
       "      <td>62</td>\n",
       "      <td>car</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9824</th>\n",
       "      <td>9825</td>\n",
       "      <td>The Taliban said Karzai had no right to sign t...</td>\n",
       "      <td>107</td>\n",
       "      <td>116</td>\n",
       "      <td>Americans</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9479</th>\n",
       "      <td>9480</td>\n",
       "      <td>On Tuesday, Filipino sailors from the warship ...</td>\n",
       "      <td>105</td>\n",
       "      <td>110</td>\n",
       "      <td>large</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5052</th>\n",
       "      <td>5053</td>\n",
       "      <td>Deficit-cutting targets Spain separately annno...</td>\n",
       "      <td>133</td>\n",
       "      <td>141</td>\n",
       "      <td>European</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2928</th>\n",
       "      <td>2929</td>\n",
       "      <td>Banks which are unable to raise the extra capi...</td>\n",
       "      <td>72</td>\n",
       "      <td>86</td>\n",
       "      <td>five-year loan</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4143</th>\n",
       "      <td>4144</td>\n",
       "      <td>The Taliban claimed responsibility for the ass...</td>\n",
       "      <td>159</td>\n",
       "      <td>166</td>\n",
       "      <td>capital</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1702</th>\n",
       "      <td>1703</td>\n",
       "      <td>Brevard County Sheriff's Lt Tod Goodyear said ...</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>Goodyear</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6699</th>\n",
       "      <td>6700</td>\n",
       "      <td>Deputies identified the children as Pebbles Jo...</td>\n",
       "      <td>100</td>\n",
       "      <td>104</td>\n",
       "      <td>Joel</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>571</td>\n",
       "      <td>The Russian Union of Journalists on Wednesday ...</td>\n",
       "      <td>56</td>\n",
       "      <td>62</td>\n",
       "      <td>police</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7385</th>\n",
       "      <td>7386</td>\n",
       "      <td>The government is under intense pressure to re...</td>\n",
       "      <td>127</td>\n",
       "      <td>132</td>\n",
       "      <td>Union</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11390</th>\n",
       "      <td>11391</td>\n",
       "      <td>“We expect to close imminently.”</td>\n",
       "      <td>20</td>\n",
       "      <td>30</td>\n",
       "      <td>imminently</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2903</th>\n",
       "      <td>2904</td>\n",
       "      <td>Two independent auditing firms will be respons...</td>\n",
       "      <td>55</td>\n",
       "      <td>62</td>\n",
       "      <td>valuing</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.15</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4057</th>\n",
       "      <td>4058</td>\n",
       "      <td>A pastor at the church the family attended des...</td>\n",
       "      <td>43</td>\n",
       "      <td>52</td>\n",
       "      <td>described</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4905</th>\n",
       "      <td>4906</td>\n",
       "      <td>The bombardment by regime forces resumed follo...</td>\n",
       "      <td>33</td>\n",
       "      <td>40</td>\n",
       "      <td>resumed</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.15</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13591</th>\n",
       "      <td>13592</td>\n",
       "      <td>Part of the plan includes the deployment in fl...</td>\n",
       "      <td>55</td>\n",
       "      <td>60</td>\n",
       "      <td>areas</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12562</th>\n",
       "      <td>12563</td>\n",
       "      <td>Google will acquire 17,000 patents with the pu...</td>\n",
       "      <td>12</td>\n",
       "      <td>19</td>\n",
       "      <td>acquire</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12177</th>\n",
       "      <td>12178</td>\n",
       "      <td>Assembly Deputy First Minister Martin McGuinne...</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>Assembly Deputy</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>7.5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9315</th>\n",
       "      <td>9316</td>\n",
       "      <td>Shock: Locals in Bellaghy, County Derry, are s...</td>\n",
       "      <td>53</td>\n",
       "      <td>63</td>\n",
       "      <td>be stunned</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10464</th>\n",
       "      <td>10465</td>\n",
       "      <td>The country's leaders have to admit that there...</td>\n",
       "      <td>143</td>\n",
       "      <td>152</td>\n",
       "      <td>Gorbachev</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6642</th>\n",
       "      <td>6643</td>\n",
       "      <td>You don’t need to sign agreements, you need to...</td>\n",
       "      <td>47</td>\n",
       "      <td>55</td>\n",
       "      <td>focus on</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6962</th>\n",
       "      <td>6963</td>\n",
       "      <td>The Philippine government says its navy tried ...</td>\n",
       "      <td>56</td>\n",
       "      <td>63</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5935</th>\n",
       "      <td>5936</td>\n",
       "      <td>Shortly after, a special weapons team entered ...</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>Shortly after</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>6.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7332</th>\n",
       "      <td>7333</td>\n",
       "      <td>Northern Ireland's deputy first minister and M...</td>\n",
       "      <td>129</td>\n",
       "      <td>139</td>\n",
       "      <td>tragically</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5818</th>\n",
       "      <td>5819</td>\n",
       "      <td>The successive rounds of financial system refo...</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>successive</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1338</th>\n",
       "      <td>1339</td>\n",
       "      <td>She walked out and said 'come home.'\"</td>\n",
       "      <td>30</td>\n",
       "      <td>34</td>\n",
       "      <td>home</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12164</th>\n",
       "      <td>12165</td>\n",
       "      <td>Anti-Putin demonstrators take to streets as pr...</td>\n",
       "      <td>212</td>\n",
       "      <td>216</td>\n",
       "      <td>amid</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11327</th>\n",
       "      <td>11328</td>\n",
       "      <td>This brings the Internet search giant closer t...</td>\n",
       "      <td>32</td>\n",
       "      <td>37</td>\n",
       "      <td>giant</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         idx                                               text    l    r  \\\n",
       "7921    7922  His speech came as an already difficult relati...   77   85   \n",
       "422      423  In northern Lebanon, meanwhile, residents said...  119  125   \n",
       "4133    4134  The Taliban claimed responsibility for the ass...   58   65   \n",
       "4227    4228  While the government has taken steps to suppor...   21   36   \n",
       "6909    6910  Even a majority of Republicans hold a negative...   59   67   \n",
       "4174    4175  The attack on Rastan came after Syrian forces ...  142  147   \n",
       "4664    4665  Israel's outgoing ambassador to Egypt arrived ...  155  161   \n",
       "1440    1441  Regulators in the US and elsewhere have stress...  198  214   \n",
       "9113    9114  U.S. military's future in Afghanistan Issued t...  231  240   \n",
       "13956  13957  This trip to Afghanistan is an attempt to shor...  176  183   \n",
       "6992    6993  China and the Philippines have been disputing ...  140  143   \n",
       "5203    5204  The Philippines accused Chinese vessels of fir...   98  133   \n",
       "8122    8123         It's sad, even if they were troublemakers.    2    4   \n",
       "6528    6529  Russian authorities should annul the parliamen...  114  123   \n",
       "10162  10163  Israel's internal security agency, Shin Bet, w...  259  267   \n",
       "3666    3667  Google Inc. (GOOG) won approval from Chinese r...  399  407   \n",
       "13428  13429  The Philippine government said Wednesday it ag...   80   87   \n",
       "3838    3839  Rastan, 25 km (15 miles) north of Homs city, h...   98  103   \n",
       "6198    6199  The gunman fled in a silver saloon car that wa...   21   34   \n",
       "8330    8331  The Wall Street Journal reported earlier this ...  226  234   \n",
       "8679    8680  He then gave a speech broadcast to Americans b...   45   49   \n",
       "5183    5184  The Philippines and Vietnam complained last ye...   93  100   \n",
       "9705    9706    Afghan security forces swarmed the  area and...   38   42   \n",
       "5106    5107  Gardaí say witnesses at the scene describe a s...   59   62   \n",
       "9824    9825  The Taliban said Karzai had no right to sign t...  107  116   \n",
       "9479    9480  On Tuesday, Filipino sailors from the warship ...  105  110   \n",
       "5052    5053  Deficit-cutting targets Spain separately annno...  133  141   \n",
       "2928    2929  Banks which are unable to raise the extra capi...   72   86   \n",
       "4143    4144  The Taliban claimed responsibility for the ass...  159  166   \n",
       "1702    1703  Brevard County Sheriff's Lt Tod Goodyear said ...   32   40   \n",
       "6699    6700  Deputies identified the children as Pebbles Jo...  100  104   \n",
       "570      571  The Russian Union of Journalists on Wednesday ...   56   62   \n",
       "7385    7386  The government is under intense pressure to re...  127  132   \n",
       "11390  11391                   “We expect to close imminently.”   20   30   \n",
       "2903    2904  Two independent auditing firms will be respons...   55   62   \n",
       "4057    4058  A pastor at the church the family attended des...   43   52   \n",
       "4905    4906  The bombardment by regime forces resumed follo...   33   40   \n",
       "13591  13592  Part of the plan includes the deployment in fl...   55   60   \n",
       "12562  12563  Google will acquire 17,000 patents with the pu...   12   19   \n",
       "12177  12178  Assembly Deputy First Minister Martin McGuinne...    0   15   \n",
       "9315    9316  Shock: Locals in Bellaghy, County Derry, are s...   53   63   \n",
       "10464  10465  The country's leaders have to admit that there...  143  152   \n",
       "6642    6643  You don’t need to sign agreements, you need to...   47   55   \n",
       "6962    6963  The Philippine government says its navy tried ...   56   63   \n",
       "5935    5936  Shortly after, a special weapons team entered ...    0   13   \n",
       "7332    7333  Northern Ireland's deputy first minister and M...  129  139   \n",
       "5818    5819  The successive rounds of financial system refo...    4   14   \n",
       "1338    1339              She walked out and said 'come home.'\"   30   34   \n",
       "12164  12165  Anti-Putin demonstrators take to streets as pr...  212  216   \n",
       "11327  11328  This brings the Internet search giant closer t...   32   37   \n",
       "\n",
       "                                       sub  n1  n2  c1  c2     p  \\\n",
       "7921                              strained  10  10   6   1  0.35   \n",
       "422                                 Sunday  10  10   0   0  0.00   \n",
       "4133                               heavily  10  10   0   0  0.00   \n",
       "4227                       has taken steps  10  10   0   1  0.05   \n",
       "6909                              conflict  10  10   2   4  0.30   \n",
       "4174                                 homes  10  10   0   0  0.00   \n",
       "4664                                forced  10  10   0   0  0.00   \n",
       "1440                      unfair advantage  10  10   2   2  0.20   \n",
       "9113                             increased  10  10   0   0  0.00   \n",
       "13956                              defense  10  10   0   0  0.00   \n",
       "6992                                   Sea  10  10   0   0  0.00   \n",
       "5203   harassing an oil exploration vessel  10  10   0   0  0.00   \n",
       "8122                                    's  10  10   0   0  0.00   \n",
       "6528                             Gorbachev  10  10   0   0  0.00   \n",
       "10162                             upgraded  10  10   0   2  0.10   \n",
       "3666                              approved  10  10   0   0  0.00   \n",
       "13428                              resolve  10  10   2   1  0.15   \n",
       "3838                                 times  10  10   0   0  0.00   \n",
       "6198                         silver saloon  10  10   0   0  0.00   \n",
       "8330                              hardware  10  10   1   1  0.10   \n",
       "8679                                  back  10  10   0   0  0.00   \n",
       "5183                               staking  10  10   3   6  0.45   \n",
       "9705                                  area  10  10   0   0  0.00   \n",
       "5106                                   car  10  10   0   0  0.00   \n",
       "9824                             Americans  10  10   0   0  0.00   \n",
       "9479                                 large  10  10   0   0  0.00   \n",
       "5052                              European  10  10   0   0  0.00   \n",
       "2928                        five-year loan  10  10   0   0  0.00   \n",
       "4143                               capital  10  10   1   0  0.05   \n",
       "1702                              Goodyear  10  10   0   0  0.00   \n",
       "6699                                  Joel  10  10   0   0  0.00   \n",
       "570                                 police  10  10   0   0  0.00   \n",
       "7385                                 Union  10  10   0   0  0.00   \n",
       "11390                           imminently  10  10  10  10  1.00   \n",
       "2903                               valuing  10  10   3   0  0.15   \n",
       "4057                             described  10  10   1   0  0.05   \n",
       "4905                               resumed  10  10   1   2  0.15   \n",
       "13591                                areas  10  10   0   0  0.00   \n",
       "12562                              acquire  10  10   3   2  0.25   \n",
       "12177                      Assembly Deputy  10  10   1   0  0.05   \n",
       "9315                            be stunned  10  10   0   0  0.00   \n",
       "10464                            Gorbachev  10  10   0   0  0.00   \n",
       "6642                              focus on  10  10   0   0  0.00   \n",
       "6962                               Chinese  10  10   0   0  0.00   \n",
       "5935                         Shortly after  10  10   0   1  0.05   \n",
       "7332                            tragically  10  10   8   7  0.75   \n",
       "5818                            successive  10  10   4   3  0.35   \n",
       "1338                                  home  10  10   0   0  0.00   \n",
       "12164                                 amid  10  10   3   4  0.35   \n",
       "11327                                giant  10  10   0   0  0.00   \n",
       "\n",
       "       s_sub_token_len  s_sub_char_len  s_sub_mean_word_len  s_capitalized  \n",
       "7921                 1               8                  8.0              0  \n",
       "422                  1               6                  6.0              1  \n",
       "4133                 1               7                  7.0              0  \n",
       "4227                 3              15                  5.0              0  \n",
       "6909                 1               8                  8.0              0  \n",
       "4174                 1               5                  5.0              0  \n",
       "4664                 1               6                  6.0              0  \n",
       "1440                 2              16                  8.0              0  \n",
       "9113                 1               9                  9.0              0  \n",
       "13956                1               7                  7.0              0  \n",
       "6992                 1               3                  3.0              1  \n",
       "5203                 5              35                  7.0              0  \n",
       "8122                 1               2                  2.0              0  \n",
       "6528                 1               9                  9.0              1  \n",
       "10162                1               8                  8.0              0  \n",
       "3666                 1               8                  8.0              0  \n",
       "13428                1               7                  7.0              0  \n",
       "3838                 1               5                  5.0              0  \n",
       "6198                 2              13                  6.5              0  \n",
       "8330                 1               8                  8.0              0  \n",
       "8679                 1               4                  4.0              0  \n",
       "5183                 1               7                  7.0              0  \n",
       "9705                 1               4                  4.0              0  \n",
       "5106                 1               3                  3.0              0  \n",
       "9824                 1               9                  9.0              1  \n",
       "9479                 1               5                  5.0              0  \n",
       "5052                 1               8                  8.0              1  \n",
       "2928                 2              14                  7.0              0  \n",
       "4143                 1               7                  7.0              0  \n",
       "1702                 1               8                  8.0              1  \n",
       "6699                 1               4                  4.0              1  \n",
       "570                  1               6                  6.0              0  \n",
       "7385                 1               5                  5.0              1  \n",
       "11390                1              10                 10.0              0  \n",
       "2903                 1               7                  7.0              0  \n",
       "4057                 1               9                  9.0              0  \n",
       "4905                 1               7                  7.0              0  \n",
       "13591                1               5                  5.0              0  \n",
       "12562                1               7                  7.0              0  \n",
       "12177                2              15                  7.5              2  \n",
       "9315                 2              10                  5.0              0  \n",
       "10464                1               9                  9.0              1  \n",
       "6642                 2               8                  4.0              0  \n",
       "6962                 1               7                  7.0              1  \n",
       "5935                 2              13                  6.5              1  \n",
       "7332                 1              10                 10.0              0  \n",
       "5818                 1              10                 10.0              0  \n",
       "1338                 1               4                  4.0              0  \n",
       "12164                1               4                  4.0              0  \n",
       "11327                1               5                  5.0              0  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model!\n"
     ]
    }
   ],
   "source": [
    "#w2v_model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "#    './GoogleNews-vectors-negative300.bin', binary=True) \n",
    "w2v_model = api.load(\"glove-wiki-gigaword-100\")\n",
    "W2V_EMB_SIZE = 100\n",
    "print(\"Loaded model!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_val = train_test_split(df, test_size=0.1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_TAG_CACHE = {}\n",
    "\n",
    "def pos_tag(text):\n",
    "    global POS_TAG_CACHE\n",
    "    if text not in POS_TAG_CACHE:\n",
    "        POS_TAG_CACHE[text] = nltk.pos_tag(\n",
    "            nltk.word_tokenize(text),\n",
    "            tagset='universal',\n",
    "        )\n",
    "    return POS_TAG_CACHE[text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('', 2175), ('ADJ', 1192), ('NOUN', 7129), ('VERB', 2105)]\n",
      "[array(['', 'ADJ', 'NOUN', 'VERB'], dtype='<U4')]\n",
      "[('', 246), ('ADJ', 146), ('NOUN', 781), ('VERB', 228)]\n",
      "[array(['', 'ADJ', 'NOUN', 'VERB'], dtype='<U4')]\n",
      "[1. 3. 3. 0. 0. 0. 1. 0.]\n",
      "(12601, 8)\n"
     ]
    }
   ],
   "source": [
    "def get_syn(df, data, include_pos=True, testing=False):\n",
    "    cols = []\n",
    "    for col in df.columns:\n",
    "        if col.startswith(\"s_\"):\n",
    "            cols.append(df[col].values)\n",
    "    ret = np.column_stack(cols)\n",
    "    \n",
    "    if include_pos:\n",
    "        pos_tags = []\n",
    "        for idx in range(len(df)):\n",
    "            \n",
    "            tag = \"\"\n",
    "            tokens = nltk.word_tokenize(df['sub'].values[idx])\n",
    "            if len(tokens) == 1:\n",
    "                target = tokens[0]\n",
    "                for w, t in pos_tag(df['text'].values[idx]):\n",
    "                    if w == target:\n",
    "                        tag = t\n",
    "                        break\n",
    "                if tag != \"NOUN\" and tag != \"ADJ\" and tag != \"VERB\":\n",
    "                    tag = \"\"\n",
    "            pos_tags.append(tag)\n",
    "        \n",
    "        pos_tags = np.array(pos_tags).reshape(len(pos_tags), 1)\n",
    "        unique, counts = np.unique(pos_tags, return_counts=True)\n",
    "        print(list(zip(unique, counts)))\n",
    "        # print(pos_tags)\n",
    "        \n",
    "        if not testing:\n",
    "            enc = OneHotEncoder()\n",
    "            enc.fit(pos_tags)\n",
    "            data['syn'] = {\n",
    "                'pos_enc': enc,\n",
    "            }\n",
    "        enc = data['syn']['pos_enc']\n",
    "        print(enc.categories_)\n",
    "        \n",
    "        pos_tags_onehot = enc.transform(pos_tags)\n",
    "        # pos_tags_enc = pos_tags_onehot.toarray()\n",
    "\n",
    "        ret = np.hstack([ret, pos_tags_onehot.toarray()])\n",
    "    return ret\n",
    "\n",
    "X_syn_train = get_syn(df_train, DATA)\n",
    "X_syn_val = get_syn(df_val, DATA, testing=True)\n",
    "print(X_syn_train[10])\n",
    "print(X_syn_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKIPPING:  outcrops\n",
      "SKIPPING:  outcrops\n",
      "SKIPPING:  shoal\n",
      "SKIPPING:  euros\n",
      "SKIPPING:  n't\n",
      "SKIPPING:  euros\n",
      "SKIPPING:  euros\n",
      "SKIPPING:  euros\n",
      "SKIPPING:  Bankia\n",
      "SKIPPING:  Rastan\n",
      "SKIPPING:  Rastan\n",
      "SKIPPING:  Cloverhill\n",
      "SKIPPING:  Tallaght\n",
      "SKIPPING:  Inchicore\n",
      "SKIPPING:  Gardai\n",
      "SKIPPING:  Ballyfermot\n",
      "SKIPPING:  Hayaleen\n",
      "SKIPPING:  gunbattles\n",
      "SKIPPING:  Alawite\n",
      "SKIPPING:  Alawite\n",
      "SKIPPING:  “\n",
      "SKIPPING:  “\n",
      "SKIPPING:  Twitter\n",
      "SKIPPING:  Karzai\n",
      "SKIPPING:  ,\n",
      "SKIPPING:  've\n",
      "SKIPPING:  're\n",
      "SKIPPING:  Qaeda\n",
      "SKIPPING:  Putin\n",
      "SKIPPING:  Putin\n",
      "SKIPPING:  Putin\n",
      "SKIPPING:  Putin\n",
      "SKIPPING:  Putin\n",
      "SKIPPING:  Qaeda\n",
      "SKIPPING:  Qaeda\n",
      "SKIPPING:  UK\n",
      "SKIPPING:  GMT\n",
      "SKIPPING:  Rastan\n",
      "SKIPPING:  Rastan\n",
      "SKIPPING:  neighbouring\n",
      "SKIPPING:  Jazeera\n",
      "SKIPPING:  armoured\n",
      "SKIPPING:  armoured\n",
      "SKIPPING:  Rastan\n",
      "SKIPPING:  Rastan\n",
      "SKIPPING:  NNA\n",
      "SKIPPING:  Putin\n",
      "SKIPPING:  Putin\n",
      "SKIPPING:  Putin\n",
      "SKIPPING:  Hajar\n",
      "SKIPPING:  Daraa\n",
      "SKIPPING:  ,\n",
      "SKIPPING:  Idleb\n",
      "SKIPPING:  Daraa\n",
      "SKIPPING:  Idleb\n",
      "SKIPPING:  n't\n",
      "SKIPPING:  LCC\n",
      "SKIPPING:  LCC\n",
      "SKIPPING:  Qaeda\n",
      "SKIPPING:  Qaeda\n",
      "SKIPPING:  Bellaghy\n",
      "SKIPPING:  Toomebridge\n",
      "SKIPPING:  PSNI\n",
      "SKIPPING:  Putin\n",
      "SKIPPING:  Putin\n",
      "SKIPPING:  Putin\n",
      "SKIPPING:  n't\n",
      "SKIPPING:  nationalises\n",
      "SKIPPING:  Bankia\n",
      "SKIPPING:  Bankia\n",
      "SKIPPING:  bn\n",
      "SKIPPING:  bn\n",
      "SKIPPING:  Lewes\n",
      "SKIPPING:  Rastan\n",
      "SKIPPING:  EU\n",
      "SKIPPING:  Rastan\n",
      "SKIPPING:  neighbour\n",
      "SKIPPING:  neighbour\n",
      "SKIPPING:  neighbour\n",
      "SKIPPING:  Lt\n",
      "SKIPPING:  awoken\n",
      "SKIPPING:  awoken\n",
      "SKIPPING:  shoal\n",
      "SKIPPING:  shoal\n",
      "SKIPPING:  emphasise\n",
      "SKIPPING:  AFP\n",
      "SKIPPING:  DFA\n",
      "SKIPPING:  PHL\n",
      "SKIPPING:  Huangyan\n",
      "SKIPPING:  Panatag\n",
      "SKIPPING:  Shoal\n",
      "SKIPPING:  lagnoon\n",
      "SKIPPING:  Huangyan\n",
      "SKIPPING:  Rajoy\n",
      "SKIPPING:  nationalised\n",
      "SKIPPING:  Bankia\n",
      "SKIPPING:  Guindos\n",
      "SKIPPING:  35\n",
      "SKIPPING:  eurozone\n",
      "SKIPPING:  BBVA\n",
      "SKIPPING:  Bankia\n",
      "SKIPPING:  Tamana\n",
      "SKIPPING:  III\n",
      "SKIPPING:  Spratlys\n",
      "SKIPPING:  Spratlys\n",
      "SKIPPING:  Spratlys\n",
      "SKIPPING:  DFA\n",
      "SKIPPING:  Panatag\n",
      "SKIPPING:  (\n",
      "SKIPPING:  Shoal\n",
      "SKIPPING:  Panatag\n",
      "SKIPPING:  Panatag\n",
      "SKIPPING:  Shoal\n",
      "SKIPPING:  Zambales\n",
      "SKIPPING:  Karzai\n",
      "SKIPPING:  burqas\n",
      "SKIPPING:  U.S.\n",
      "SKIPPING:  U.S.\n",
      "SKIPPING:  Toome\n",
      "SKIPPING:  DFA\n",
      "SKIPPING:  DFA\n",
      "SKIPPING:  DFA\n",
      "SKIPPING:  DFA\n",
      "SKIPPING:  euros\n",
      "SKIPPING:  euros\n",
      "SKIPPING:  Putin\n",
      "SKIPPING:  Putin\n",
      "SKIPPING:  Putin\n",
      "SKIPPING:  Putin\n",
      "SKIPPING:  Barack\n",
      "SKIPPING:  Bagram\n",
      "SKIPPING:  Rastan\n",
      "SKIPPING:  Bankia\n",
      "SKIPPING:  euros\n",
      "SKIPPING:  eurozone\n",
      "SKIPPING:  eurozone\n",
      "SKIPPING:  eurozone\n",
      "SKIPPING:  IMF\n",
      "SKIPPING:  Peston\n",
      "SKIPPING:  JPMorgan\n",
      "SKIPPING:  bn\n",
      "SKIPPING:  Soraya\n",
      "SKIPPING:  Santamaria\n",
      "SKIPPING:  Soraya\n",
      "SKIPPING:  Santamaria\n",
      "SKIPPING:  Guindos\n",
      "SKIPPING:  Guindos\n",
      "SKIPPING:  euros\n",
      "SKIPPING:  euros\n",
      "SKIPPING:  bn\n",
      "SKIPPING:  euros\n",
      "SKIPPING:  Guindos\n",
      "SKIPPING:  Guindos\n",
      "SKIPPING:  euros\n",
      "SKIPPING:  incentivise\n",
      "SKIPPING:  incentivise\n",
      "SKIPPING:  incentivise\n",
      "SKIPPING:  offload\n",
      "SKIPPING:  offload\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  Motorolla\n",
      "SKIPPING:  VentureBeat\n",
      "SKIPPING:  VentureBeat\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  BBs\n",
      "SKIPPING:  AFP\n",
      "SKIPPING:  AFP\n",
      "SKIPPING:  “\n",
      "SKIPPING:  ”\n",
      "SKIPPING:  ”\n",
      "SKIPPING:  “\n",
      "SKIPPING:  Karzai\n",
      "SKIPPING:  Toome\n",
      "SKIPPING:  traumatised\n",
      "SKIPPING:  SDLP\n",
      "SKIPPING:  SDLP\n",
      "SKIPPING:  MLA\n",
      "SKIPPING:  Levanon\n",
      "SKIPPING:  AFP\n",
      "SKIPPING:  Levanon\n",
      "SKIPPING:  destabilizes\n",
      "SKIPPING:  destabilizes\n",
      "SKIPPING:  falsifications\n",
      "SKIPPING:  falsifications\n",
      "SKIPPING:  falsifications\n",
      "SKIPPING:  Putin\n",
      "SKIPPING:  Putin\n",
      "SKIPPING:  Putin\n",
      "SKIPPING:  neighbouring\n",
      "SKIPPING:  neighbouring\n",
      "SKIPPING:  Shoal\n",
      "SKIPPING:  Shoal\n",
      "SKIPPING:  Keqing\n",
      "SKIPPING:  enquiries\n",
      "SKIPPING:  enquiries\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  GOOG\n",
      "SKIPPING:  MMI\n",
      "SKIPPING:  AAPL\n",
      "SKIPPING:  ,\n",
      "SKIPPING:  ”\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  smartphone\n",
      "SKIPPING:  Libertyville\n",
      "SKIPPING:  Yaakov\n",
      "SKIPPING:  Bellaghy\n",
      "SKIPPING:  EU\n",
      "SKIPPING:  Rastan\n",
      "SKIPPING:  U.N.\n",
      "SKIPPING:  Rastan\n",
      "SKIPPING:  Rastan\n",
      "SKIPPING:  km\n",
      "SKIPPING:  U.N.\n",
      "SKIPPING:  Lt\n",
      "SKIPPING:  Merseyside\n",
      "SKIPPING:  Bidston\n",
      "SKIPPING:  Bidston\n",
      "SKIPPING:  Birkenhead\n",
      "SKIPPING:  Merseyside\n",
      "SKIPPING:  Wirral\n",
      "SKIPPING:  Merseyside\n",
      "SKIPPING:  Bidston\n",
      "SKIPPING:  Bidston\n",
      "SKIPPING:  Deupities\n",
      "SKIPPING:  BB\n",
      "SKIPPING:  BB\n",
      "SKIPPING:  WFTV\n",
      "SKIPPING:  Jaxs\n",
      "SKIPPING:  n't\n",
      "SKIPPING:  n't\n",
      "SKIPPING:  Rockledge\n",
      "SKIPPING:  Fla\n",
      "SKIPPING:  Qaeda\n",
      "SKIPPING:  Barack\n",
      "SKIPPING:  Rastan\n",
      "SKIPPING:  minatory\n",
      "SKIPPING:  minatory\n",
      "SKIPPING:  Alawite\n",
      "SKIPPING:  Alawite\n",
      "SKIPPING:  Alawite\n",
      "SKIPPING:  Alawite\n",
      "SKIPPING:  shabiha\n",
      "SKIPPING:  Alawite\n",
      "SKIPPING:  Barack\n",
      "SKIPPING:  Karzai\n",
      "SKIPPING:  Karzai\n",
      "SKIPPING:  Bagram\n",
      "SKIPPING:  Zabihullah\n",
      "SKIPPING:  Mujahid\n",
      "SKIPPING:  Haqqani\n",
      "SKIPPING:  Haqqani\n",
      "SKIPPING:  Waziristan\n",
      "SKIPPING:  Facebook\n",
      "SKIPPING:  Huangyan\n",
      "SKIPPING:  GMA\n",
      "SKIPPING:  Levanon\n",
      "SKIPPING:  Levanon\n",
      "SKIPPING:  Putin\n",
      "SKIPPING:  Putin\n",
      "SKIPPING:  Levanon\n",
      "SKIPPING:  Lior\n",
      "SKIPPING:  Dor\n",
      "SKIPPING:  Levanon\n",
      "SKIPPING:  Rajoy\n",
      "SKIPPING:  EU\n",
      "SKIPPING:  EU\n",
      "SKIPPING:  Bankia\n",
      "SKIPPING:  Levanon\n",
      "SKIPPING:  Quraya\n",
      "SKIPPING:  Deir\n",
      "SKIPPING:  Ezzor\n",
      "SKIPPING:  Deir\n",
      "SKIPPING:  Ezzor\n",
      "SKIPPING:  Qaboon\n",
      "SKIPPING:  Rajoy\n",
      "SKIPPING:  Bankia\n",
      "SKIPPING:  Guindos\n",
      "SKIPPING:  Guindos\n",
      "SKIPPING:  Guindos\n",
      "SKIPPING:  annnounced\n",
      "SKIPPING:  Ballyfermot\n",
      "SKIPPING:  Ballyfermot\n",
      "SKIPPING:  Finnegans\n",
      "SKIPPING:  Inchicore\n",
      "SKIPPING:  Tallaght\n",
      "SKIPPING:  Gardaí\n",
      "SKIPPING:  Goldenbridge\n",
      "SKIPPING:  Inchicore\n",
      "SKIPPING:  35\n",
      "SKIPPING:  eurozone\n",
      "SKIPPING:  BBVA\n",
      "SKIPPING:  euros\n",
      "SKIPPING:  Bankia\n",
      "SKIPPING:  euros\n",
      "SKIPPING:  euros\n",
      "SKIPPING:  Bankia\n",
      "SKIPPING:  bn\n",
      "SKIPPING:  euros\n",
      "SKIPPING:  Alawite\n",
      "SKIPPING:  Alawite\n",
      "SKIPPING:  Alawite\n",
      "SKIPPING:  Alawite\n",
      "SKIPPING:  Alawite\n",
      "SKIPPING:  Alawite\n",
      "SKIPPING:  Shi'ite\n",
      "SKIPPING:  Rastan\n",
      "SKIPPING:  Alawite\n",
      "SKIPPING:  Rastan\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  Bellaghy\n",
      "SKIPPING:  Gardaí\n",
      "SKIPPING:  Karzai\n",
      "SKIPPING:  Qaida\n",
      "SKIPPING:  Sediq\n",
      "SKIPPING:  Sediqi\n",
      "SKIPPING:  Kirill\n",
      "SKIPPING:  Kudryavtsev\n",
      "SKIPPING:  Nemtsov\n",
      "SKIPPING:  Triumfalnaya\n",
      "SKIPPING:  IESE\n",
      "SKIPPING:  Bankia\n",
      "SKIPPING:  Bankia\n",
      "SKIPPING:  Barack\n",
      "SKIPPING:  NYSE\n",
      "SKIPPING:  :\n",
      "SKIPPING:  MOT\n",
      "SKIPPING:  NYSE\n",
      "SKIPPING:  MOT\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  monetize\n",
      "SKIPPING:  Gmail\n",
      "SKIPPING:  YouTube\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  AdWords\n",
      "SKIPPING:  AdSense\n",
      "SKIPPING:  Goldenbridge\n",
      "SKIPPING:  Gardaí\n",
      "SKIPPING:  Tallaght\n",
      "SKIPPING:  Ballyfermot\n",
      "SKIPPING:  ’\n",
      "SKIPPING:  Cloverhill\n",
      "SKIPPING:  Inchicore\n",
      "SKIPPING:  Jazlin\n",
      "SKIPPING:  Jaxs\n",
      "SKIPPING:  texted\n",
      "SKIPPING:  Zabiullah\n",
      "SKIPPING:  Mujahid\n",
      "SKIPPING:  mujahedin\n",
      "SKIPPING:  Zabiullah\n",
      "SKIPPING:  Mujahid\n",
      "SKIPPING:  nationalisation\n",
      "SKIPPING:  Bankia\n",
      "SKIPPING:  nationalising\n",
      "SKIPPING:  Bankia\n",
      "SKIPPING:  Rajoy\n",
      "SKIPPING:  Jazlin\n",
      "SKIPPING:  Jaxs\n",
      "SKIPPING:  Putin\n",
      "SKIPPING:  n't\n",
      "SKIPPING:  Zabiullah\n",
      "SKIPPING:  Mujahid\n",
      "SKIPPING:  Zabiullah\n",
      "SKIPPING:  Mujahid\n",
      "SKIPPING:  Karzai\n",
      "SKIPPING:  Karzai\n",
      "SKIPPING:  n't\n",
      "SKIPPING:  Jaxs\n",
      "SKIPPING:  Jazzlyn\n",
      "SKIPPING:  n't\n",
      "SKIPPING:  DFA\n",
      "SKIPPING:  BRP\n",
      "SKIPPING:  PF\n",
      "SKIPPING:  15\n",
      "SKIPPING:  Palawan\n",
      "SKIPPING:  Zhonggou\n",
      "SKIPPING:  Shoal\n",
      "SKIPPING:  Haijian\n",
      "SKIPPING:  Zhonggou\n",
      "SKIPPING:  Haijian\n",
      "SKIPPING:  PF\n",
      "SKIPPING:  15\n",
      "SKIPPING:  Qaeda\n",
      "SKIPPING:  ABC\n",
      "SKIPPING:  ABC\n",
      "SKIPPING:  Keqing\n",
      "SKIPPING:  Shoal\n",
      "SKIPPING:  shoal\n",
      "SKIPPING:  Zambales\n",
      "SKIPPING:  shoal\n",
      "SKIPPING:  Spratly\n",
      "SKIPPING:  BRP\n",
      "SKIPPING:  euros\n",
      "SKIPPING:  euros\n",
      "SKIPPING:  Zhonggou\n",
      "SKIPPING:  Haijian\n",
      "SKIPPING:  Zhonggou\n",
      "SKIPPING:  Haijian\n",
      "SKIPPING:  Shoal\n",
      "SKIPPING:  Huangyan\n",
      "SKIPPING:  shoal\n",
      "SKIPPING:  Panatag\n",
      "SKIPPING:  Spratlys\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  neighbouring\n",
      "SKIPPING:  neighbouring\n",
      "SKIPPING:  MP\n",
      "SKIPPING:  MP\n",
      "SKIPPING:  PSNI\n",
      "SKIPPING:  eurozone\n",
      "SKIPPING:  GDP\n",
      "SKIPPING:  GDP\n",
      "SKIPPING:  LCC\n",
      "SKIPPING:  Qaboun\n",
      "SKIPPING:  Qaboun\n",
      "SKIPPING:  euros\n",
      "SKIPPING:  euros\n",
      "SKIPPING:  Libertyville\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  HTC\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  Karzai\n",
      "SKIPPING:  armoured\n",
      "SKIPPING:  armoured\n",
      "SKIPPING:  've\n",
      "SKIPPING:  Karzai\n",
      "SKIPPING:  Rajoy\n",
      "SKIPPING:  Rastan\n",
      "SKIPPING:  UK\n",
      "SKIPPING:  EMTs\n",
      "SKIPPING:  Fla\n",
      "SKIPPING:  n't\n",
      "SKIPPING:  Jaxs\n",
      "SKIPPING:  Toome\n",
      "SKIPPING:  Fr\n",
      "SKIPPING:  UTV\n",
      "SKIPPING:  Fr\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  fleshing\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  fleshing\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  smartphones\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  HTC\n",
      "SKIPPING:  Gardai\n",
      "SKIPPING:  Ballyfermot\n",
      "SKIPPING:  targetted\n",
      "SKIPPING:  Tallaght\n",
      "SKIPPING:  Inchicore\n",
      "SKIPPING:  Goldenbridge\n",
      "SKIPPING:  gardai\n",
      "SKIPPING:  Zhonggou\n",
      "SKIPPING:  Haijian\n",
      "SKIPPING:  Zhonggou\n",
      "SKIPPING:  Haijian\n",
      "SKIPPING:  Shoal\n",
      "SKIPPING:  Huangyan\n",
      "SKIPPING:  Shoal\n",
      "SKIPPING:  shoal\n",
      "SKIPPING:  Panatag\n",
      "SKIPPING:  Spratlys\n",
      "SKIPPING:  Putin\n",
      "SKIPPING:  Qaida\n",
      "SKIPPING:  Bagram\n",
      "SKIPPING:  Karzai\n",
      "SKIPPING:  U.S.\n",
      "SKIPPING:  Soraya\n",
      "SKIPPING:  Santamaria\n",
      "SKIPPING:  MP\n",
      "SKIPPING:  MP\n",
      "SKIPPING:  traumatised\n",
      "SKIPPING:  traumatised\n",
      "SKIPPING:  neighbouring\n",
      "SKIPPING:  neighbouring\n",
      "SKIPPING:  Bellaghy\n",
      "SKIPPING:  Bellaghy\n",
      "SKIPPING:  Fr\n",
      "SKIPPING:  n't\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  n't\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  Qaeda\n",
      "SKIPPING:  Rastan\n",
      "SKIPPING:  Quraya\n",
      "SKIPPING:  Deir\n",
      "SKIPPING:  az\n",
      "SKIPPING:  Zour\n",
      "SKIPPING:  Deir\n",
      "SKIPPING:  az\n",
      "SKIPPING:  Zour\n",
      "SKIPPING:  Qaboon\n",
      "SKIPPING:  Qaeda\n",
      "SKIPPING:  —\n",
      "SKIPPING:  Karzai\n",
      "SKIPPING:  Karzai\n",
      "SKIPPING:  Karzai\n",
      "SKIPPING:  Karzai\n",
      "SKIPPING:  Qaida\n",
      "SKIPPING:  Rastan\n",
      "SKIPPING:  km\n",
      "SKIPPING:  Orontes\n",
      "SKIPPING:  Aleppo\n",
      "SKIPPING:  Bellaghy\n",
      "SKIPPING:  Toome\n",
      "SKIPPING:  Caoimhe\n",
      "SKIPPING:  councillor\n",
      "SKIPPING:  Magherafelt\n",
      "SKIPPING:  Bellaghy\n",
      "SKIPPING:  councillor\n",
      "SKIPPING:  councillor\n",
      "SKIPPING:  Bellaghy\n",
      "SKIPPING:  BRP\n",
      "SKIPPING:  Haqyar\n",
      "SKIPPING:  alighting\n",
      "SKIPPING:  alighting\n",
      "SKIPPING:  Karzai\n",
      "SKIPPING:  're\n",
      "SKIPPING:  Karzai\n",
      "SKIPPING:  Karzai\n",
      "SKIPPING:  Karzai\n",
      "SKIPPING:  EU\n",
      "SKIPPING:  EU\n",
      "SKIPPING:  Karzai\n",
      "SKIPPING:  inked\n",
      "SKIPPING:  inked\n",
      "SKIPPING:  ,\n",
      "SKIPPING:  Karzai\n",
      "SKIPPING:  Karzai\n",
      "SKIPPING:  Dor\n",
      "SKIPPING:  Levanon\n",
      "SKIPPING:  Ya'akov\n",
      "SKIPPING:  Amita\n",
      "SKIPPING:  Keqing\n",
      "SKIPPING:  Shoal\n",
      "SKIPPING:  Shoal\n",
      "SKIPPING:  Spratly\n",
      "SKIPPING:  shoal\n",
      "SKIPPING:  Guindos\n",
      "SKIPPING:  Guindos\n",
      "SKIPPING:  Guindos\n",
      "SKIPPING:  Cdn\n",
      "SKIPPING:  falsifications\n",
      "SKIPPING:  falsifications\n",
      "SKIPPING:  falsifications\n",
      "SKIPPING:  AFP\n",
      "SKIPPING:  Putin\n",
      "SKIPPING:  euros\n",
      "SKIPPING:  n't\n",
      "SKIPPING:  euros\n",
      "SKIPPING:  euros\n",
      "SKIPPING:  ,\n",
      "SKIPPING:  Baset\n",
      "SKIPPING:  Megrahi\n",
      "SKIPPING:  Megrahi\n",
      "SKIPPING:  Megrahi\n",
      "SKIPPING:  Megrahi\n",
      "SKIPPING:  Megrahi\n",
      "SKIPPING:  shoal\n",
      "SKIPPING:  Zambales\n",
      "SKIPPING:  shoal\n",
      "SKIPPING:  shoal\n",
      "SKIPPING:  Spratly\n",
      "SKIPPING:  Spratly\n",
      "SKIPPING:  BRP\n",
      "SKIPPING:  Bankia\n",
      "SKIPPING:  Guindos\n",
      "SKIPPING:  euros\n",
      "SKIPPING:  Guindos\n",
      "SKIPPING:  n't\n",
      "SKIPPING:  Barack\n",
      "SKIPPING:  Karzai\n",
      "SKIPPING:  burqas\n",
      "SKIPPING:  GMT\n",
      "SKIPPING:  Kargar\n",
      "SKIPPING:  Noorughli\n",
      "SKIPPING:  Kargar\n",
      "SKIPPING:  Noorughli\n",
      "SKIPPING:  AFP\n",
      "SKIPPING:  Zabiullah\n",
      "SKIPPING:  Mujahid\n",
      "SKIPPING:  U.S.\n",
      "SKIPPING:  Zabihullah\n",
      "SKIPPING:  Mujahid\n",
      "SKIPPING:  Haqqani\n",
      "SKIPPING:  Waziristan\n",
      "SKIPPING:  LCC\n",
      "SKIPPING:  Burnhanieh\n",
      "SKIPPING:  Syian\n",
      "SKIPPING:  n't\n",
      "SKIPPING:  Tebbaneh\n",
      "SKIPPING:  Alawite\n",
      "SKIPPING:  Jabal\n",
      "SKIPPING:  Mohsen\n",
      "SKIPPING:  Chadi\n",
      "SKIPPING:  Mawlawi\n",
      "SKIPPING:  Qaeda\n",
      "SKIPPING:  Hajar\n",
      "SKIPPING:  Mawlawi\n",
      "SKIPPING:  Mawlawi\n",
      "SKIPPING:  II\n",
      "SKIPPING:  Merseyside\n",
      "SKIPPING:  Bidston\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  GOOG\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  MMI\n",
      "SKIPPING:  ,\n",
      "SKIPPING:  ”\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  iPhone\n",
      "SKIPPING:  smartphone\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  Hamza\n",
      "SKIPPING:  Skype\n",
      "SKIPPING:  Barack\n",
      "SKIPPING:  Gurkha\n",
      "SKIPPING:  Gurkha\n",
      "SKIPPING:  Karzai\n",
      "SKIPPING:  GMT\n",
      "SKIPPING:  Rastan\n",
      "SKIPPING:  Rastan\n",
      "SKIPPING:  Rastan\n",
      "SKIPPING:  FSA\n",
      "SKIPPING:  FSA\n",
      "SKIPPING:  FSA\n",
      "SKIPPING:  incubated\n",
      "SKIPPING:  Qaeda\n",
      "SKIPPING:  Qaeda\n",
      "SKIPPING:  Qaeda\n",
      "SKIPPING:  Qaeda\n",
      "SKIPPING:  Barack\n",
      "SKIPPING:  Barack\n",
      "SKIPPING:  burqas\n",
      "SKIPPING:  burqas\n",
      "SKIPPING:  Spratly\n",
      "SKIPPING:  Spratly\n",
      "SKIPPING:  BRP\n",
      "SKIPPING:  GMA\n",
      "SKIPPING:  Shoal\n",
      "SKIPPING:  Panatag\n",
      "SKIPPING:  Keqing\n",
      "SKIPPING:  Huangyan\n",
      "SKIPPING:  Olli\n",
      "SKIPPING:  Olli\n",
      "SKIPPING:  EU\n",
      "SKIPPING:  euros\n",
      "SKIPPING:  Jazeera\n",
      "SKIPPING:  eurozone\n",
      "SKIPPING:  eurozone\n",
      "SKIPPING:  EU\n",
      "SKIPPING:  eurozone\n",
      "SKIPPING:  eurozone\n",
      "SKIPPING:  EU\n",
      "SKIPPING:  Facebook\n",
      "SKIPPING:  Mariya\n",
      "SKIPPING:  Boyarintseva\n",
      "SKIPPING:  Boyarintseva\n",
      "SKIPPING:  n't\n",
      "SKIPPING:  Putin\n",
      "SKIPPING:  Putin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKIPPING:  Putin\n",
      "SKIPPING:  traumatised\n",
      "SKIPPING:  traumatised\n",
      "SKIPPING:  ambassadoe\n",
      "SKIPPING:  ambassadoe\n",
      "SKIPPING:  've\n",
      "SKIPPING:  Qaida\n",
      "SKIPPING:  U.S.\n",
      "SKIPPING:  Qaeda\n",
      "SKIPPING:  ’\n",
      "SKIPPING:  Barack\n",
      "SKIPPING:  Bagram\n",
      "SKIPPING:  Bagram\n",
      "SKIPPING:  Jaxs\n",
      "SKIPPING:  Jazlin\n",
      "SKIPPING:  Jaxs\n",
      "SKIPPING:  Jaxs\n",
      "SKIPPING:  Keqing\n",
      "SKIPPING:  Shoal\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  AFP\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  smartphone\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  Fla\n",
      "SKIPPING:  WFTV\n",
      "SKIPPING:  Jazlin\n",
      "SKIPPING:  Jaxs\n",
      "SKIPPING:  WFTV\n",
      "SKIPPING:  Fla\n",
      "SKIPPING:  WKMG\n",
      "SKIPPING:  euros\n",
      "SKIPPING:  Guindos\n",
      "SKIPPING:  Bankia\n",
      "SKIPPING:  Bagram\n",
      "SKIPPING:  AFP\n",
      "SKIPPING:  eurozone\n",
      "SKIPPING:  EU\n",
      "SKIPPING:  EU\n",
      "SKIPPING:  eurozone\n",
      "SKIPPING:  EU\n",
      "SKIPPING:  Olli\n",
      "SKIPPING:  Olli\n",
      "SKIPPING:  AP\n",
      "SKIPPING:  Barack\n",
      "SKIPPING:  Levanon\n",
      "SKIPPING:  Levanon\n",
      "SKIPPING:  EU\n",
      "SKIPPING:  EU\n",
      "SKIPPING:  EU\n",
      "SKIPPING:  Gunbattles\n",
      "SKIPPING:  EU\n",
      "SKIPPING:  EU\n",
      "SKIPPING:  Rastan\n",
      "SKIPPING:  Rastan\n",
      "SKIPPING:  Deir\n",
      "SKIPPING:  Ezzor\n",
      "SKIPPING:  n't\n",
      "SKIPPING:  n't\n",
      "SKIPPING:  Waziri\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  TechCrunch\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  Ballyfermot\n",
      "SKIPPING:  Cloverhill\n",
      "SKIPPING:  Tallaght\n",
      "SKIPPING:  Gardaí\n",
      "SKIPPING:  Ballyfermot\n",
      "SKIPPING:  n't\n",
      "SKIPPING:  Bellaghy\n",
      "SKIPPING:  Rastan\n",
      "SKIPPING:  EU\n",
      "SKIPPING:  EU\n",
      "SKIPPING:  destabilises\n",
      "SKIPPING:  destabilises\n",
      "SKIPPING:  falsifications\n",
      "SKIPPING:  falsifications\n",
      "SKIPPING:  falsifications\n",
      "SKIPPING:  Putin\n",
      "SKIPPING:  Putin\n",
      "SKIPPING:  Putin\n",
      "SKIPPING:  Putin\n",
      "SKIPPING:  nationalise\n",
      "SKIPPING:  nationalise\n",
      "SKIPPING:  Bankia\n",
      "SKIPPING:  Bankia\n",
      "SKIPPING:  Jaxs\n",
      "SKIPPING:  n't\n",
      "SKIPPING:  Rockledge\n",
      "SKIPPING:  Fla\n",
      "SKIPPING:  n't\n",
      "SKIPPING:  Guindos\n",
      "SKIPPING:  Jazeera\n",
      "SKIPPING:  euros\n",
      "SKIPPING:  eurozone\n",
      "SKIPPING:  eurozone\n",
      "SKIPPING:  euros\n",
      "[ 0.42440382 -0.0684492   0.08952273 -0.06006419  0.08403641 -0.19222509\n",
      "  0.09776722  0.83176672 -0.18792074 -0.08353858  0.05781932 -0.04958322\n",
      " -0.02577099 -0.03858221 -0.0073347 ]\n",
      "(12601, 15)\n",
      "(1401, 15)\n"
     ]
    }
   ],
   "source": [
    "def get_phon(df, data, testing=False):\n",
    "    arpabet = nltk.corpus.cmudict.dict()\n",
    "    phones = []\n",
    "    for idx, sub in enumerate(df[\"sub\"].values):\n",
    "        phs = []\n",
    "        for word in nltk.word_tokenize(sub.replace('-', ' ')):\n",
    "            p = arpabet.get(word.lower())\n",
    "            if not p:\n",
    "                print(\"SKIPPING: \", word)\n",
    "                continue\n",
    "            phs.extend(p[0])\n",
    "        phones.append(\" \".join(phs))\n",
    "    \n",
    "    if not testing:\n",
    "        tfidf = TfidfVectorizer().fit(phones)\n",
    "        data['phon'] = {\n",
    "            'tfidf': tfidf   \n",
    "        }\n",
    "    \n",
    "    tfidf = data['phon']['tfidf']\n",
    "    X_phon = tfidf.transform(phones)\n",
    "    \n",
    "    if not testing:\n",
    "        pca = TruncatedSVD(n_components=15)\n",
    "        pca.fit(X_phon)\n",
    "        data['phon']['pca'] = pca\n",
    "    pca = data['phon']['pca']\n",
    "    \n",
    "    X_phon = pca.transform(X_phon)\n",
    "    return X_phon\n",
    "        \n",
    "X_phon_train = get_phon(df_train, DATA)\n",
    "X_phon_val = get_phon(df_val, DATA, testing=True)\n",
    "print(X_phon_train[5])\n",
    "print(X_phon_train.shape)\n",
    "print(X_phon_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset\n",
      "Dataset loaded\n",
      "Training Doc2Vec\n",
      "DONE\n",
      "Skipping word: a.m\n",
      "murders happened around 4:30 a.m.\n",
      "Skipping word: a.m\n",
      "neighbor heard shots at 4:50 a.m.\n",
      "Skipping word: a.m\n",
      "a text from Thomas at about 3 a.m.\n",
      "Skipping word: a.m\n",
      "home to mostly foreigners at about 6:15 a.m.\n",
      "Skipping word: a.m\n",
      "been hitting the town since three a.m.\n",
      "Skipping word: said\n",
      "that killed Osama bin Laden,'' he said.\n",
      "Skipping word: .\n",
      "that killed Osama bin Laden,'' he said.\n",
      "Skipping word: said\n",
      "killed Osama bin Laden,'' he said.\n",
      "Skipping word: .\n",
      "killed Osama bin Laden,'' he said.\n",
      "Skipping word: a.m\n",
      "message to her neighbor around 3 a.m.\n",
      "[-9.35790002e-01  4.37780008e-01 -4.34281987e-02  1.74949914e-02\n",
      " -7.34104998e-02  7.34750181e-03 -5.98880008e-01  4.17229995e-01\n",
      " -3.14904988e-01  1.24536999e-01  4.04024988e-01  5.85644990e-01\n",
      "  4.58794996e-01  1.49545498e-01  6.66410021e-01 -1.33249983e-02\n",
      "  6.19604990e-01 -6.49274990e-01  3.49040002e-01  4.01990011e-01\n",
      "  1.40910000e-02  2.07582496e-01  1.79164995e-01  5.19950092e-02\n",
      "  5.71849942e-02  5.54799996e-01  3.34544986e-01 -3.65950018e-02\n",
      " -1.55115001e-01 -9.74580031e-02  2.91584998e-01 -5.49620003e-01\n",
      "  2.44934998e-01  2.03955002e-01  5.27564988e-01 -3.50890011e-01\n",
      " -2.37115003e-01 -2.93495014e-01  7.20355026e-02 -4.07284992e-02\n",
      " -4.96170014e-01 -1.03300214e-02 -7.26960003e-01  2.73979001e-01\n",
      "  3.88860032e-02  4.62530002e-01  5.73495016e-01  8.23484987e-01\n",
      " -2.52826998e-01  2.93125004e-01 -1.32733507e-01 -9.03535010e-02\n",
      "  5.25119975e-02 -1.05844004e-01  1.73600093e-02 -9.57539976e-01\n",
      " -6.70712483e-02 -7.78954983e-01  2.07131512e-01  6.72824979e-01\n",
      " -6.52145013e-01  8.38115007e-01  5.10235012e-01 -2.43744992e-01\n",
      "  7.54499435e-03  1.31540000e-01  7.30565012e-01 -5.04669994e-01\n",
      "  8.85369986e-01 -9.53450024e-01  1.22502498e-01  5.24729997e-01\n",
      "  3.63412006e-02  9.38849971e-02 -7.01944977e-02 -7.76559979e-01\n",
      "  2.22009994e-01  1.27062503e-01  8.89145017e-01  6.60104975e-01\n",
      "  3.00835006e-01 -3.69599909e-02  1.72970500e-01  4.33680020e-01\n",
      "  9.04780000e-01 -6.30284995e-01  9.20769989e-01 -1.04894996e-01\n",
      "  1.04749992e-01  3.44384991e-01 -2.83545017e-01 -7.31254995e-01\n",
      " -2.22619995e-01  2.87559986e-01  6.19935035e-03  5.46650022e-01\n",
      " -1.10338503e+00 -7.34189972e-01  5.07059991e-01  3.36599946e-02\n",
      " -1.13506985e-01  5.88616705e-02  1.26369728e-02 -3.54979991e-03\n",
      " -2.44816470e-03 -1.85016884e-02 -8.06088489e-02  4.72709795e-02\n",
      " -3.79789586e-02 -5.53843428e-03  6.01072805e-02  8.02801819e-02\n",
      "  7.70707909e-02  1.31123001e-02  7.75646992e-02 -4.02108931e-02\n",
      "  7.48994651e-02 -5.75222669e-02 -2.07544929e-02  5.52023244e-02\n",
      "  4.85735350e-02  4.27890242e-02  7.55429732e-02  2.12337522e-02\n",
      "  4.57597146e-03  6.88856731e-02  3.02219699e-02 -2.31124017e-02\n",
      " -1.42008879e-02 -1.29560706e-02  2.70332931e-02 -4.89784188e-02\n",
      "  1.57697253e-02  3.33143351e-02  6.77030696e-02 -4.23920263e-02\n",
      " -9.06637958e-03 -3.23175536e-03 -3.15852467e-03 -2.18391202e-02\n",
      " -8.34654848e-02 -2.48760339e-02 -6.39663453e-02  1.84353492e-02\n",
      "  1.66473859e-02  6.35410029e-02  8.94297752e-02  7.91264666e-02\n",
      " -2.34094989e-02  1.69630872e-02 -2.73625358e-02 -3.17697267e-03\n",
      "  5.41747799e-03  4.59357020e-02 -1.32890397e-02 -1.81118768e-01\n",
      "  9.21378575e-03 -1.23305714e-01  6.40945898e-02  1.15734350e-01\n",
      " -8.91125571e-02  1.12560918e-01  7.64418727e-02 -3.58087964e-02\n",
      "  4.67174457e-02  2.25902847e-02  9.29645809e-02 -3.57243984e-02\n",
      "  1.21483865e-01 -1.39765031e-01 -1.28505114e-03  5.35971463e-02\n",
      "  3.02058883e-02  7.88354810e-05 -5.75419243e-03 -6.38231381e-02\n",
      "  1.52115651e-02  2.23642166e-02  5.16364597e-02  8.20675524e-02\n",
      "  5.66095008e-02 -1.48438671e-02 -1.66332850e-02  6.60776467e-02\n",
      "  3.71617241e-02 -7.54385062e-02  1.01365109e-01 -5.75931525e-03\n",
      "  2.08752400e-02  1.16055972e-02 -1.98180278e-02 -6.92909990e-02\n",
      " -1.09781919e-02  1.57880921e-02 -3.26871800e-02  9.13580875e-02\n",
      " -1.40805688e-01 -1.03039116e-01  6.26896032e-02  3.73615923e-03\n",
      " -2.23751068e-01 -7.73702860e-02  1.55675650e-01  1.47210106e-01\n",
      "  3.55841517e-02  1.28617836e-03  3.01476777e-01 -5.07530272e-02\n",
      " -3.30514431e-01  1.50719330e-01  1.93758830e-01  5.58504090e-02\n",
      "  1.77363470e-01  3.01825166e-01 -1.55261934e-01  8.01672786e-02]\n",
      "(12601, 216)\n",
      "(1401, 216)\n"
     ]
    }
   ],
   "source": [
    "def get_emb(df, data, window_size=20, vector_size=16, use_tfidf=True, testing=False):\n",
    "    def preprocess_text(text):\n",
    "        return nltk.word_tokenize(text.lower())\n",
    "    \n",
    "    if not testing:\n",
    "        print(\"Loading dataset\")\n",
    "        dataset = list(api.load(\"text8\")) + [preprocess_text(text) \n",
    "                   for text in np.unique(df['text'])] \n",
    "        print(\"Dataset loaded\")\n",
    "        dct = gensim.corpora.Dictionary(dataset)\n",
    "        corpus = [dct.doc2bow(line) for line in dataset]\n",
    "        tfidf = gensim.models.TfidfModel(corpus) \n",
    "        documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(dataset)]\n",
    "        print(\"Training Doc2Vec\")\n",
    "        d2v_model = Doc2Vec(documents, vector_size=vector_size, window=2, min_count=1, workers=4)\n",
    "        print(\"DONE\")\n",
    "        data['emb'] = {\n",
    "            \"tfidf\": tfidf,\n",
    "            \"dct\": dct,\n",
    "            \"d2v\": d2v_model,\n",
    "        }\n",
    "    emb_size = W2V_EMB_SIZE\n",
    "    \n",
    "    X_emb = np.zeros((len(df), 2 * emb_size + vector_size))\n",
    "    for idx in range(len(df)):\n",
    "        sub = df['sub'].values[idx]\n",
    "        text = df['text'].values[idx]\n",
    "        \n",
    "        l, r = df['l'].values[idx], df['r'].values[idx]\n",
    "        text_window = text[max(0, l - window_size):(r + window_size)]\n",
    "        text_window = text_window.split(' ', 1)[1]\n",
    "        text_window = text_window.rsplit(' ', 1)[0]\n",
    "        \n",
    "        now_emb = np.zeros((emb_size,))\n",
    "        cnt = 0\n",
    "        \n",
    "        tfidf = data['emb']['tfidf']\n",
    "        dct = data['emb']['dct']\n",
    "        d2v_model = data['emb']['d2v']\n",
    "        \n",
    "        for token in nltk.word_tokenize(sub):\n",
    "            try:\n",
    "                emb = w2v_model[token]\n",
    "                now_emb += emb\n",
    "                cnt += 1\n",
    "            except KeyError:\n",
    "                #print(f\"{token} not found\")\n",
    "                #print(text_window)\n",
    "                pass\n",
    "        if cnt > 0:\n",
    "            now_emb /= cnt\n",
    "            \n",
    "        X_emb[idx, :emb_size] = now_emb\n",
    "        \n",
    "        now_emb = np.zeros((emb_size,))\n",
    "        cnt = 0\n",
    "        \n",
    "        tokens = preprocess_text(text)\n",
    "        text_coefs = dict(tfidf[dct.doc2bow(tokens)])\n",
    "        X_emb[idx, -vector_size:] = d2v_model.infer_vector(tokens)\n",
    "        \n",
    "        for token in nltk.word_tokenize(text_window):\n",
    "#             if token in sub:\n",
    "#                 continue     \n",
    "            if not use_tfidf:\n",
    "                coef = 1.\n",
    "            else:\n",
    "                if token in dct.token2id:\n",
    "                    token_id = dct.token2id[token]\n",
    "                    if token_id not in text_coefs:\n",
    "                        print(f\"Skipping word: {token}\")\n",
    "                        print(text_window)\n",
    "                        continue\n",
    "                    coef = text_coefs[token_id]\n",
    "                else:\n",
    "                    continue\n",
    "            try:\n",
    "                emb = w2v_model[token]\n",
    "                now_emb += emb * coef\n",
    "                cnt += 1\n",
    "            except KeyError:\n",
    "                pass\n",
    "        if cnt > 0:\n",
    "            now_emb /= cnt\n",
    "        \n",
    "        X_emb[idx, emb_size:2*emb_size] = now_emb\n",
    "    \n",
    "    return X_emb\n",
    "\n",
    "X_emb_train = get_emb(df_train, DATA)\n",
    "X_emb_val = get_emb(df_val, DATA, testing=True)\n",
    "print(X_emb_train[5])\n",
    "print(X_emb_train.shape)\n",
    "print(X_emb_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00489803 -0.00043715  0.00333822  0.00147761  0.00691381  0.01425793\n",
      " -0.00134855 -0.00022967  0.00058849  0.00267774  0.00026387  0.00391977\n",
      " -0.00162324  0.03954918 -0.00781259 -0.02051351 -0.00030832 -0.01968504\n",
      " -0.00118979  0.06359536  0.02754606 -0.01616442  0.00779618  0.01197732\n",
      " -0.00783731  0.01139048 -0.00472575  0.00146781  0.00214375  0.00157406\n",
      "  0.00051824 -0.00086279  0.00981891  0.00290875 -0.00437111 -0.00338223\n",
      " -0.0007209  -0.00093276 -0.00100657  0.00759088  0.00617698  0.00014938\n",
      "  0.00225338  0.0011104   0.00429761 -0.0093427   0.00434835  0.0056751\n",
      "  0.01270168  0.00029909 -0.003898    0.0018445   0.00298269 -0.00061546\n",
      "  0.00344711 -0.00455679  0.01002158 -0.00609832  0.00067433  0.0069031\n",
      "  0.00393161 -0.00713831 -0.0005772  -0.00144176  0.00962234 -0.00525464\n",
      "  0.00103503 -0.00223094 -0.00205426 -0.00157509  0.00178215  0.00323537\n",
      " -0.00219188 -0.00689994 -0.00127121  0.001787   -0.00684304  0.00757854\n",
      " -0.00236467 -0.01011315  0.00190429 -0.00111932  0.00790149  0.00444459\n",
      " -0.00475564 -0.00409789  0.00181982  0.00348951 -0.01410661  0.01388706\n",
      " -0.00739071  0.01415434 -0.00412156  0.01272614  0.01139109 -0.01481825\n",
      " -0.0169009  -0.01283929  0.01042975  0.01202266]\n",
      "(12601, 100)\n",
      "(1401, 100)\n"
     ]
    }
   ],
   "source": [
    "def get_grams(df, data, testing=False):\n",
    "\n",
    "    def preprocess_sub(sub):\n",
    "        sub = sub.lower()\n",
    "        tokenized = nltk.word_tokenize(sub)\n",
    "        ngrams = []\n",
    "        for token in tokenized:\n",
    "            ngrams.extend(nltk.everygrams(token, 3, 4))\n",
    "        ngrams = \" \".join([\"\".join(ngram) for ngram in ngrams])\n",
    "        return ngrams\n",
    "    \n",
    "    corpus = list(map(preprocess_sub, df[\"sub\"].values))\n",
    "    \n",
    "    if not testing:\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        vectorizer.fit(corpus)\n",
    "        data['grams'] = {\n",
    "            'vectorizer': vectorizer,\n",
    "        }\n",
    "    vectorizer = data['grams']['vectorizer']\n",
    "    \n",
    "    X_grams = vectorizer.transform(corpus)\n",
    "    \n",
    "    if not testing:\n",
    "        pca = TruncatedSVD(n_components=100)\n",
    "        pca.fit(X_grams)\n",
    "        data['grams'][\"pca\"] = pca\n",
    "    pca = data['grams']['pca']\n",
    "    \n",
    "    X_grams = pca.transform(X_grams)\n",
    "    \n",
    "    return X_grams\n",
    "\n",
    "X_grams_train = get_grams(df_train, DATA)\n",
    "X_grams_val = get_grams(df_val, DATA, testing=True)\n",
    "print(X_grams_train[5])\n",
    "print(X_grams_train.shape)\n",
    "print(X_grams_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['g', 'u', 'd', 'o', 'í', 'j', 'a', 't', 'w', 'k', 'm', 'p', 'x', 'q', 'y', 's', 'n', 'f', 'b', 'e', 'r', 'v', 'i', 'h', 'l', 'z', 'c', ' ']\n",
      "(12601, 15, 28)\n",
      "(1401, 15, 28)\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "def get_lstm_repr(df, data, size=15, testing=False):\n",
    "    if not testing:\n",
    "        char_enc = LabelEncoder()\n",
    "        chars = list(set([c for s in df[\"sub\"].values for c in s.lower()]))\n",
    "        chars = [c for c in chars if c.isalpha()] + [' ']\n",
    "        print(chars)\n",
    "        char_enc.fit(chars)\n",
    "        data[\"lstm\"] = {\n",
    "            \"char_enc\": char_enc,\n",
    "        }\n",
    "    char_enc = data[\"lstm\"][\"char_enc\"]\n",
    "    \n",
    "    X_lstm = np.zeros((len(df), size, len(char_enc.classes_)))\n",
    "    for idx, s in enumerate(df[\"sub\"].values):\n",
    "        s = s.lower()\n",
    "        s = re.sub('[^a-zA-Z]+', ' ', s)\n",
    "        if len(s) > size:\n",
    "            s = s[:size//2] + ' ' + s[-size//2:]\n",
    "        s = s[:size]\n",
    "        while len(s) + 1 < size:\n",
    "            s = \" \" + s + \" \"\n",
    "        if len(s) < size:\n",
    "            s = s + \" \"\n",
    "            \n",
    "        chars = np.array(list(s.lower()))\n",
    "        enc = char_enc.transform(chars)\n",
    "        for jdx, x in enumerate(enc):\n",
    "            X_lstm[idx][jdx][x] = 1\n",
    "            \n",
    "    return X_lstm\n",
    "\n",
    "X_lstm_train = get_lstm_repr(df_train, DATA)\n",
    "X_lstm_val = get_lstm_repr(df_val, DATA, testing=True)\n",
    "print(X_lstm_train.shape)\n",
    "print(X_lstm_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train[\"p\"].values\n",
    "y_val = df_val[\"p\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(X_syn, X_emb, X_grams, data, testing=False):\n",
    "    if not testing:\n",
    "        scaler_syn = StandardScaler()\n",
    "        scaler_syn.fit(X_syn)\n",
    "        scaler_emb = StandardScaler()\n",
    "        scaler_emb.fit(X_emb)\n",
    "        scaler_grams = StandardScaler()\n",
    "        scaler_grams.fit(X_grams)\n",
    "        data[\"scaler\"] = {\n",
    "            \"syn\": scaler_syn,\n",
    "            \"emb\": scaler_emb,\n",
    "            \"grams\": scaler_grams,\n",
    "        }\n",
    "    scaler_syn = data[\"scaler\"][\"syn\"]\n",
    "    X_syn = scaler_syn.transform(X_syn)\n",
    "    scaler_emb = data[\"scaler\"][\"emb\"]\n",
    "    X_emb = scaler_emb.transform(X_emb)\n",
    "    scaler_grams = data[\"scaler\"][\"grams\"]\n",
    "    X_grams = scaler_grams.transform(X_grams)\n",
    "    \n",
    "    return X_syn, X_emb, X_grams\n",
    "\n",
    "#X_syn_train, X_emb_train, X_grams_train = scale(X_syn_train, X_emb_train, X_grams_train, DATA)\n",
    "#X_syn_val, X_emb_val, X_grams_val = scale(X_syn_val, X_emb_val, X_grams_val, DATA, testing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def loss(y_true, y_pred):\n",
    "    return np.mean(np.abs(y_true - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abs_loss(y_true, y_pred):\n",
    "    y_pred = tf.clip_by_value(y_pred, 0, 1)\n",
    "    return tf.math.abs(y_true - y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(syn_dim, phon_dim,emb_dim, gram_dim, lstm_shape, \n",
    "              use_syn=True, use_phon=True, use_emb=True, use_grams=True, use_lstm=True):\n",
    "    syn_i = tfkl.Input(shape=(syn_dim,), name='syn_i')\n",
    "    phon_i = tfkl.Input(shape=(phon_dim,), name='phon_i')\n",
    "    emb_i = tfkl.Input(shape=(emb_dim,), name='emb_i')\n",
    "    gram_i = tfkl.Input(shape=(gram_dim,), name='gram_i')\n",
    "    lstm_i = tfkl.Input(shape=lstm_shape, name='lstm_i')\n",
    "    \n",
    "    syn_o = syn_i\n",
    "    phon_o = phon_i\n",
    "    emb_o = tfkl.Dropout(0.2, name='emb_dropout')(emb_i)\n",
    "    #emb_o = emb_i\n",
    "    gram_o = gram_i\n",
    "    lstm_o = tfkl.Bidirectional(tfkl.LSTM(256), name='bilstm')(lstm_i)\n",
    "    \n",
    "    use = []\n",
    "    if use_syn: use.append(syn_o)\n",
    "    if use_phon: use.append(phon_o)\n",
    "    if use_emb: use.append(emb_o)\n",
    "    if use_grams: use.append(gram_o)\n",
    "    if use_lstm: use.append(lstm_o)\n",
    "    \n",
    "    x = tfkl.Concatenate()(use)\n",
    "    x = tfkl.Dense(512, activation='relu')(x)\n",
    "    #x = tfkl.Dropout(0.3)(x)\n",
    "    pred = tfkl.Dense(1)(x)\n",
    "    \n",
    "    return tfk.Model([syn_i, phon_i, emb_i, gram_i, lstm_i], pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_13\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "emb_i (InputLayer)              [(None, 216)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_i (InputLayer)             [(None, 15, 28)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "syn_i (InputLayer)              [(None, 8)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "phon_i (InputLayer)             [(None, 15)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "emb_dropout (Dropout)           (None, 216)          0           emb_i[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "gram_i (InputLayer)             [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bilstm (Bidirectional)          (None, 512)          583680      lstm_i[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 851)          0           syn_i[0][0]                      \n",
      "                                                                 phon_i[0][0]                     \n",
      "                                                                 emb_dropout[0][0]                \n",
      "                                                                 gram_i[0][0]                     \n",
      "                                                                 bilstm[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 512)          436224      concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 1)            513         dense_26[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,020,417\n",
      "Trainable params: 1,020,417\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train on 12601 samples, validate on 1401 samples\n",
      "Epoch 1/1000\n",
      "12601/12601 [==============================] - 26s 2ms/sample - loss: 0.1036 - abs_loss: 0.0886 - val_loss: 0.0820 - val_abs_loss: 0.0771\n",
      "Epoch 2/1000\n",
      "12601/12601 [==============================] - 19s 2ms/sample - loss: 0.0848 - abs_loss: 0.0755 - val_loss: 0.0792 - val_abs_loss: 0.0672\n",
      "Epoch 3/1000\n",
      "12601/12601 [==============================] - 21s 2ms/sample - loss: 0.0802 - abs_loss: 0.0714 - val_loss: 0.0724 - val_abs_loss: 0.0651\n",
      "Epoch 4/1000\n",
      " 4832/12601 [==========>...................] - ETA: 12s - loss: 0.0768 - abs_loss: 0.0693"
     ]
    }
   ],
   "source": [
    "model = get_model(\n",
    "    X_syn_train.shape[1], \n",
    "    X_phon_train.shape[1],\n",
    "    X_emb_train.shape[1], \n",
    "    X_grams_train.shape[1],\n",
    "    X_lstm_train.shape[1:],\n",
    "    #use_lstm=False,\n",
    ")\n",
    "optimizer = tfk.optimizers.Adam(learning_rate=1e-4)\n",
    "model.compile(loss=tfk.losses.MAE, optimizer=optimizer, metrics=[abs_loss])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "reduce_lr = tfk.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', \n",
    "    factor=np.sqrt(0.1),\n",
    "    patience=5, \n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "model.fit([X_syn_train, X_phon_train, X_emb_train, \n",
    "           X_grams_train, X_lstm_train], y_train, \n",
    "    batch_size=32,\n",
    "    epochs=1000,\n",
    "    validation_data=([X_syn_val, X_phon_val, X_emb_val, \n",
    "                      X_grams_val, X_lstm_val], y_val),\n",
    "    callbacks=[reduce_lr],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.052389806133198366\n"
     ]
    }
   ],
   "source": [
    "preds = np.zeros(X_syn_val.shape[0])\n",
    "preds = model.predict([X_syn_val, X_phon_val, X_emb_val, X_grams_val, X_lstm_val]).ravel()\n",
    "preds = np.clip(preds, 0, 1)\n",
    "#preds = np.round(preds, 1)\n",
    "print(loss(y_val, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer.learning_rate = optimizer.learning_rate * 10\n",
    "#model.fit([X_syn, X_emb, X_grams, X_lstm], y, batch_size=32, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>l</th>\n",
       "      <th>r</th>\n",
       "      <th>n1</th>\n",
       "      <th>n2</th>\n",
       "      <th>s_sub_token_len</th>\n",
       "      <th>s_sub_char_len</th>\n",
       "      <th>s_sub_mean_word_len</th>\n",
       "      <th>s_capitalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1764.000000</td>\n",
       "      <td>1764.000000</td>\n",
       "      <td>1764.000000</td>\n",
       "      <td>1764.0</td>\n",
       "      <td>1764.0</td>\n",
       "      <td>1764.000000</td>\n",
       "      <td>1764.000000</td>\n",
       "      <td>1764.000000</td>\n",
       "      <td>1764.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>14884.500000</td>\n",
       "      <td>81.154762</td>\n",
       "      <td>89.397959</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.221088</td>\n",
       "      <td>8.243197</td>\n",
       "      <td>6.714957</td>\n",
       "      <td>0.277211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>509.367255</td>\n",
       "      <td>57.635872</td>\n",
       "      <td>57.726803</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.627312</td>\n",
       "      <td>4.921544</td>\n",
       "      <td>2.078128</td>\n",
       "      <td>0.545930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>14003.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>14443.750000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>14884.500000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.750000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>15325.250000</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>15766.000000</td>\n",
       "      <td>272.000000</td>\n",
       "      <td>284.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                idx            l            r      n1      n2  \\\n",
       "count   1764.000000  1764.000000  1764.000000  1764.0  1764.0   \n",
       "mean   14884.500000    81.154762    89.397959    10.0    10.0   \n",
       "std      509.367255    57.635872    57.726803     0.0     0.0   \n",
       "min    14003.000000     0.000000     2.000000    10.0    10.0   \n",
       "25%    14443.750000    34.000000    42.000000    10.0    10.0   \n",
       "50%    14884.500000    72.000000    82.000000    10.0    10.0   \n",
       "75%    15325.250000   119.000000   128.000000    10.0    10.0   \n",
       "max    15766.000000   272.000000   284.000000    10.0    10.0   \n",
       "\n",
       "       s_sub_token_len  s_sub_char_len  s_sub_mean_word_len  s_capitalized  \n",
       "count      1764.000000     1764.000000          1764.000000    1764.000000  \n",
       "mean          1.221088        8.243197             6.714957       0.277211  \n",
       "std           0.627312        4.921544             2.078128       0.545930  \n",
       "min           1.000000        2.000000             2.000000       0.000000  \n",
       "25%           1.000000        5.000000             5.000000       0.000000  \n",
       "50%           1.000000        7.000000             6.750000       0.000000  \n",
       "75%           1.000000        9.000000             8.000000       0.000000  \n",
       "max           6.000000       42.000000            15.000000       5.000000  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = read_df('data/test.txt', COL_NAMES[:-3])\n",
    "test_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "      <th>l</th>\n",
       "      <th>r</th>\n",
       "      <th>sub</th>\n",
       "      <th>n1</th>\n",
       "      <th>n2</th>\n",
       "      <th>s_sub_token_len</th>\n",
       "      <th>s_sub_char_len</th>\n",
       "      <th>s_sub_mean_word_len</th>\n",
       "      <th>s_capitalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14003</td>\n",
       "      <td>Syrian troops shelled a rebel-held town on Mon...</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>troops</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14004</td>\n",
       "      <td>Syrian troops shelled a rebel-held town on Mon...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>Syrian</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14005</td>\n",
       "      <td>Syrian troops shelled a rebel-held town on Mon...</td>\n",
       "      <td>14</td>\n",
       "      <td>21</td>\n",
       "      <td>shelled</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14006</td>\n",
       "      <td>Syrian troops shelled a rebel-held town on Mon...</td>\n",
       "      <td>24</td>\n",
       "      <td>34</td>\n",
       "      <td>rebel-held</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14007</td>\n",
       "      <td>Syrian troops shelled a rebel-held town on Mon...</td>\n",
       "      <td>51</td>\n",
       "      <td>59</td>\n",
       "      <td>sparking</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     idx                                               text   l   r  \\\n",
       "0  14003  Syrian troops shelled a rebel-held town on Mon...   7  13   \n",
       "1  14004  Syrian troops shelled a rebel-held town on Mon...   0   6   \n",
       "2  14005  Syrian troops shelled a rebel-held town on Mon...  14  21   \n",
       "3  14006  Syrian troops shelled a rebel-held town on Mon...  24  34   \n",
       "4  14007  Syrian troops shelled a rebel-held town on Mon...  51  59   \n",
       "\n",
       "          sub  n1  n2  s_sub_token_len  s_sub_char_len  s_sub_mean_word_len  \\\n",
       "0      troops  10  10                1               6                  6.0   \n",
       "1      Syrian  10  10                1               6                  6.0   \n",
       "2     shelled  10  10                1               7                  7.0   \n",
       "3  rebel-held  10  10                1              10                 10.0   \n",
       "4    sparking  10  10                1               8                  8.0   \n",
       "\n",
       "   s_capitalized  \n",
       "0              0  \n",
       "1              1  \n",
       "2              0  \n",
       "3              0  \n",
       "4              0  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('', 304), ('ADJ', 170), ('NOUN', 979), ('VERB', 311)]\n",
      "[array(['', 'ADJ', 'NOUN', 'VERB'], dtype='<U4')]\n",
      "SKIPPING:  Rastan\n",
      "SKIPPING:  Rastan\n",
      "SKIPPING:  Rastan\n",
      "SKIPPING:  Rastan\n",
      "SKIPPING:  NBC\n",
      "SKIPPING:  NBC\n",
      "SKIPPING:  WESH\n",
      "SKIPPING:  Ruqayya\n",
      "SKIPPING:  Ruqayya\n",
      "SKIPPING:  U.N.\n",
      "SKIPPING:  U.N.\n",
      "SKIPPING:  n't\n",
      "SKIPPING:  Jaxs\n",
      "SKIPPING:  Jazlin\n",
      "SKIPPING:  kilometres\n",
      "SKIPPING:  n't\n",
      "SKIPPING:  BBs\n",
      "SKIPPING:  BBs\n",
      "SKIPPING:  euros\n",
      "SKIPPING:  euros\n",
      "SKIPPING:  Rajoy\n",
      "SKIPPING:  Bankia\n",
      "SKIPPING:  euros\n",
      "SKIPPING:  euros\n",
      "SKIPPING:  refocussed\n",
      "SKIPPING:  refocussed\n",
      "SKIPPING:  eurozone\n",
      "SKIPPING:  BBVA\n",
      "SKIPPING:  euros\n",
      "SKIPPING:  BBVA\n",
      "SKIPPING:  BBVA\n",
      "SKIPPING:  euros\n",
      "SKIPPING:  euros\n",
      "SKIPPING:  Bankia\n",
      "SKIPPING:  euros\n",
      "SKIPPING:  Bankia\n",
      "SKIPPING:  euros\n",
      "SKIPPING:  euros\n",
      "SKIPPING:  euros\n",
      "SKIPPING:  Facebook\n",
      "SKIPPING:  Kolosvetov\n",
      "SKIPPING:  U.S.\n",
      "SKIPPING:  Karzai\n",
      "SKIPPING:  Qaida\n",
      "SKIPPING:  Adem\n",
      "SKIPPING:  Ozkose\n",
      "SKIPPING:  Hamit\n",
      "SKIPPING:  Coskun\n",
      "SKIPPING:  Ozkose\n",
      "SKIPPING:  Coskun\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  Levanon\n",
      "SKIPPING:  Levanon\n",
      "SKIPPING:  Barack\n",
      "SKIPPING:  Gurkha\n",
      "SKIPPING:  Karzai\n",
      "SKIPPING:  neighbour\n",
      "SKIPPING:  Lt\n",
      "SKIPPING:  Lt\n",
      "SKIPPING:  neighbour\n",
      "SKIPPING:  Lt\n",
      "SKIPPING:  AFP\n",
      "SKIPPING:  neighbourhood\n",
      "SKIPPING:  Jabal\n",
      "SKIPPING:  Mohsen\n",
      "SKIPPING:  Jabal\n",
      "SKIPPING:  Mohsen\n",
      "SKIPPING:  Alawite\n",
      "SKIPPING:  Alawite\n",
      "SKIPPING:  neighbourhood\n",
      "SKIPPING:  Jabal\n",
      "SKIPPING:  Mohsen\n",
      "SKIPPING:  Tebbaneh\n",
      "SKIPPING:  Tebbaneh\n",
      "SKIPPING:  EU\n",
      "SKIPPING:  Rastan\n",
      "SKIPPING:  Ya'akov\n",
      "SKIPPING:  Levanon\n",
      "SKIPPING:  Alawite\n",
      "SKIPPING:  U.N.\n",
      "SKIPPING:  U.N.\n",
      "SKIPPING:  nationalisation\n",
      "SKIPPING:  Jazeera\n",
      "SKIPPING:  nationalisation\n",
      "SKIPPING:  Bankia\n",
      "SKIPPING:  euros\n",
      "SKIPPING:  eurozone\n",
      "SKIPPING:  eurozone\n",
      "SKIPPING:  euros\n",
      "SKIPPING:  Qaeda\n",
      "SKIPPING:  Qaeda\n",
      "SKIPPING:  Qaeda\n",
      "SKIPPING:  Rajoy\n",
      "SKIPPING:  Putin\n",
      "SKIPPING:  Putin\n",
      "SKIPPING:  Putin\n",
      "SKIPPING:  Guindos\n",
      "SKIPPING:  Guindos\n",
      "SKIPPING:  euros\n",
      "SKIPPING:  Guindos\n",
      "SKIPPING:  Guindos\n",
      "Skipping word: report\n",
      "said of the neighbor's report.\n",
      "Skipping word: .\n",
      "said of the neighbor's report.\n",
      "Skipping word: report\n",
      "of the neighbor's report.\n",
      "Skipping word: .\n",
      "of the neighbor's report.\n"
     ]
    }
   ],
   "source": [
    "X_syn_test = get_syn(test_df, DATA, testing=True)\n",
    "X_phon_test = get_phon(test_df, DATA, testing=True)\n",
    "X_emb_test = get_emb(test_df, DATA, testing=True)\n",
    "X_grams_test = get_grams(test_df, DATA, testing=True)\n",
    "X_lstm_test = get_lstm_repr(test_df, DATA, testing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_test = gate_clf.predict(np.hstack([X_syn_test, X_emb_test, X_grams_test])) == 1\n",
    "print(mask_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1764,)\n",
      "[0.00240639 0.         0.64978576 0.38083166 0.4716571 ]\n"
     ]
    }
   ],
   "source": [
    "y_pred_test = np.zeros(X_syn_test.shape[0])\n",
    "y_pred_test = model.predict([X_syn_test, X_phon_test, X_emb_test, \n",
    "                             X_grams_test, X_lstm_test]).ravel()\n",
    "y_pred_test = np.clip(y_pred_test, 0, 1)\n",
    "\n",
    "print(y_pred_test.shape)\n",
    "print(y_pred_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('new_submission.txt', 'w') as file:\n",
    "    lines = [\"id,label\"]\n",
    "    for idx, pred in zip(test_df[\"idx\"], y_pred_test):\n",
    "        lines.append(f\"{idx},{pred}\")\n",
    "    file.write('\\n'.join(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(f1, f2):\n",
    "    preds1 = []\n",
    "    preds2 = []\n",
    "    with open(f1, 'r') as l1:\n",
    "        for line in list(l1)[1:]:\n",
    "            preds1.append(float(line.split(',')[1]))\n",
    "    with open(f2, 'r') as l2:\n",
    "        for line in list(l2)[1:]:\n",
    "            preds2.append(float(line.split(',')[1]))\n",
    "    \n",
    "    diff = 0.\n",
    "    for idx in range(len(preds1)):\n",
    "        diff += abs(preds1[idx] - preds2[idx])\n",
    "    return diff / len(preds1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.038867770108239136"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare('new_submission.txt', 'submission_prob_050.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
