{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVR, SVC\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as tfk\n",
    "import tensorflow.keras.layers as tfkl\n",
    "import gensim\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/roboself/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /Users/roboself/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n",
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     /Users/roboself/nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('universal_tagset')\n",
    "nltk.download('cmudict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_df(filename, names):\n",
    "    df = pd.read_csv(filename, sep='\\t', header=None, names=names)\n",
    "    df[\"s_sub_token_len\"] = [len(s.split()) for s in df[\"sub\"]]\n",
    "    df[\"s_sub_char_len\"] = [len(s) for s in df[\"sub\"]]\n",
    "    df[\"s_capitalized\"] = [len([c for c in s if c.isupper()]) for s in df['sub']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>l</th>\n",
       "      <th>r</th>\n",
       "      <th>n1</th>\n",
       "      <th>n2</th>\n",
       "      <th>c1</th>\n",
       "      <th>c2</th>\n",
       "      <th>p</th>\n",
       "      <th>s_sub_token_len</th>\n",
       "      <th>s_sub_char_len</th>\n",
       "      <th>s_capitalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>14002.00000</td>\n",
       "      <td>14002.000000</td>\n",
       "      <td>14002.000000</td>\n",
       "      <td>14002.0</td>\n",
       "      <td>14002.0</td>\n",
       "      <td>14002.000000</td>\n",
       "      <td>14002.000000</td>\n",
       "      <td>14002.000000</td>\n",
       "      <td>14002.000000</td>\n",
       "      <td>14002.000000</td>\n",
       "      <td>14002.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7001.50000</td>\n",
       "      <td>83.727753</td>\n",
       "      <td>92.100486</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.902014</td>\n",
       "      <td>0.860591</td>\n",
       "      <td>0.088130</td>\n",
       "      <td>1.220968</td>\n",
       "      <td>8.372732</td>\n",
       "      <td>0.282745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4042.17357</td>\n",
       "      <td>66.602408</td>\n",
       "      <td>66.819266</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.949611</td>\n",
       "      <td>1.894848</td>\n",
       "      <td>0.181183</td>\n",
       "      <td>0.630302</td>\n",
       "      <td>5.086451</td>\n",
       "      <td>0.577635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3501.25000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7001.50000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>10501.75000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>129.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>14002.00000</td>\n",
       "      <td>647.000000</td>\n",
       "      <td>656.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               idx             l             r       n1       n2  \\\n",
       "count  14002.00000  14002.000000  14002.000000  14002.0  14002.0   \n",
       "mean    7001.50000     83.727753     92.100486     10.0     10.0   \n",
       "std     4042.17357     66.602408     66.819266      0.0      0.0   \n",
       "min        1.00000      0.000000      2.000000     10.0     10.0   \n",
       "25%     3501.25000     32.000000     40.000000     10.0     10.0   \n",
       "50%     7001.50000     71.000000     79.000000     10.0     10.0   \n",
       "75%    10501.75000    120.000000    129.000000     10.0     10.0   \n",
       "max    14002.00000    647.000000    656.000000     10.0     10.0   \n",
       "\n",
       "                 c1            c2             p  s_sub_token_len  \\\n",
       "count  14002.000000  14002.000000  14002.000000     14002.000000   \n",
       "mean       0.902014      0.860591      0.088130         1.220968   \n",
       "std        1.949611      1.894848      0.181183         0.630302   \n",
       "min        0.000000      0.000000      0.000000         1.000000   \n",
       "25%        0.000000      0.000000      0.000000         1.000000   \n",
       "50%        0.000000      0.000000      0.000000         1.000000   \n",
       "75%        1.000000      1.000000      0.100000         1.000000   \n",
       "max       10.000000     10.000000      1.000000        11.000000   \n",
       "\n",
       "       s_sub_char_len  s_capitalized  \n",
       "count    14002.000000   14002.000000  \n",
       "mean         8.372732       0.282745  \n",
       "std          5.086451       0.577635  \n",
       "min          2.000000       0.000000  \n",
       "25%          5.000000       0.000000  \n",
       "50%          7.000000       0.000000  \n",
       "75%          9.000000       0.000000  \n",
       "max         49.000000       9.000000  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COL_NAMES = [\"idx\", \"text\", \"l\", \"r\", \"sub\", \"n1\", \"n2\", \"c1\", \"c2\", \"p\"]\n",
    "df = read_df('data/train_full.txt', COL_NAMES)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "      <th>l</th>\n",
       "      <th>r</th>\n",
       "      <th>sub</th>\n",
       "      <th>n1</th>\n",
       "      <th>n2</th>\n",
       "      <th>c1</th>\n",
       "      <th>c2</th>\n",
       "      <th>p</th>\n",
       "      <th>s_sub_token_len</th>\n",
       "      <th>s_sub_char_len</th>\n",
       "      <th>s_capitalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11556</th>\n",
       "      <td>11557</td>\n",
       "      <td>The Taliban said it was in response to Obama's...</td>\n",
       "      <td>64</td>\n",
       "      <td>73</td>\n",
       "      <td>strategic</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11551</th>\n",
       "      <td>11552</td>\n",
       "      <td>The Taliban claimed responsibility for the att...</td>\n",
       "      <td>196</td>\n",
       "      <td>206</td>\n",
       "      <td>passers-by</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4887</th>\n",
       "      <td>4888</td>\n",
       "      <td>The incident followed the killing in August of...</td>\n",
       "      <td>106</td>\n",
       "      <td>115</td>\n",
       "      <td>militants</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6500</th>\n",
       "      <td>6501</td>\n",
       "      <td>The other children -- Joel Johnson, 12, Jazlin...</td>\n",
       "      <td>105</td>\n",
       "      <td>111</td>\n",
       "      <td>Thomas</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12380</th>\n",
       "      <td>12381</td>\n",
       "      <td>PRESIDENT Barack Obama, speaking to a US telev...</td>\n",
       "      <td>78</td>\n",
       "      <td>82</td>\n",
       "      <td>Base</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5328</th>\n",
       "      <td>5329</td>\n",
       "      <td>Rastan has in the past been a major source of ...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>Rastan</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2241</th>\n",
       "      <td>2242</td>\n",
       "      <td>Officials and witnesses said a suicide car bom...</td>\n",
       "      <td>72</td>\n",
       "      <td>81</td>\n",
       "      <td>disguised</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1177</th>\n",
       "      <td>1178</td>\n",
       "      <td>Another man and a woman, both aged 28, were ar...</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>man</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2309</th>\n",
       "      <td>2310</td>\n",
       "      <td>Part of the plan includes the deployment in fl...</td>\n",
       "      <td>75</td>\n",
       "      <td>86</td>\n",
       "      <td>UN military</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9742</th>\n",
       "      <td>9743</td>\n",
       "      <td>He credited the Afghan security forces for q...</td>\n",
       "      <td>205</td>\n",
       "      <td>212</td>\n",
       "      <td>members</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1549</th>\n",
       "      <td>1550</td>\n",
       "      <td>The shooting happened in Port St. John, about ...</td>\n",
       "      <td>175</td>\n",
       "      <td>183</td>\n",
       "      <td>location</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12034</th>\n",
       "      <td>12035</td>\n",
       "      <td>As a result, with the recession, so many peopl...</td>\n",
       "      <td>82</td>\n",
       "      <td>91</td>\n",
       "      <td>mortgages</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6430</th>\n",
       "      <td>6431</td>\n",
       "      <td>Market doubts about the full extent of the los...</td>\n",
       "      <td>120</td>\n",
       "      <td>133</td>\n",
       "      <td>property boom</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.15</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7369</th>\n",
       "      <td>7370</td>\n",
       "      <td>Spain, the eurozone's fourth-largest economy, ...</td>\n",
       "      <td>116</td>\n",
       "      <td>131</td>\n",
       "      <td>swollen deficit</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>0.65</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6562</th>\n",
       "      <td>6563</td>\n",
       "      <td>Hundreds of protesters have been detained in b...</td>\n",
       "      <td>50</td>\n",
       "      <td>56</td>\n",
       "      <td>cities</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6797</th>\n",
       "      <td>6798</td>\n",
       "      <td>But on Tuesday, when the Gregorio del Pilar di...</td>\n",
       "      <td>168</td>\n",
       "      <td>175</td>\n",
       "      <td>vessels</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.15</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2537</th>\n",
       "      <td>2538</td>\n",
       "      <td>Thirty-three-year-old Tonya Thomas fatally sho...</td>\n",
       "      <td>28</td>\n",
       "      <td>34</td>\n",
       "      <td>Thomas</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7137</th>\n",
       "      <td>7138</td>\n",
       "      <td>Spain is set to intensify the clean-up of its ...</td>\n",
       "      <td>134</td>\n",
       "      <td>141</td>\n",
       "      <td>details</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8097</th>\n",
       "      <td>8098</td>\n",
       "      <td>That next day, when she woke him up for school...</td>\n",
       "      <td>24</td>\n",
       "      <td>28</td>\n",
       "      <td>woke</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>342</td>\n",
       "      <td>Witnesses at the scene of the shooting describ...</td>\n",
       "      <td>68</td>\n",
       "      <td>75</td>\n",
       "      <td>leaving</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6273</th>\n",
       "      <td>6274</td>\n",
       "      <td>The attack occurred at the gate of a compound ...</td>\n",
       "      <td>27</td>\n",
       "      <td>31</td>\n",
       "      <td>gate</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11198</th>\n",
       "      <td>11199</td>\n",
       "      <td>In Pictures: Winning hearts and minds in Afgha...</td>\n",
       "      <td>13</td>\n",
       "      <td>27</td>\n",
       "      <td>Winning hearts</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13997</th>\n",
       "      <td>13998</td>\n",
       "      <td>Stephen Biddle, a defense analyst at the Counc...</td>\n",
       "      <td>185</td>\n",
       "      <td>201</td>\n",
       "      <td>security partner</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2426</th>\n",
       "      <td>2427</td>\n",
       "      <td>The Embassy is still engaged in discussions wi...</td>\n",
       "      <td>190</td>\n",
       "      <td>204</td>\n",
       "      <td>well preserved</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1272</th>\n",
       "      <td>1273</td>\n",
       "      <td>Putin was expected to formally register later ...</td>\n",
       "      <td>22</td>\n",
       "      <td>30</td>\n",
       "      <td>formally</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4079</th>\n",
       "      <td>4080</td>\n",
       "      <td>Asserting that the US had largely achieved its...</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>Asserting</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>0.95</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>278</td>\n",
       "      <td>Success in Afghanistan is vital to our nation'...</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>Success</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>149</td>\n",
       "      <td>The Spanish government source said the 30-bill...</td>\n",
       "      <td>247</td>\n",
       "      <td>253</td>\n",
       "      <td>figure</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8037</th>\n",
       "      <td>8038</td>\n",
       "      <td>The neighbors called police immediately who he...</td>\n",
       "      <td>64</td>\n",
       "      <td>71</td>\n",
       "      <td>arrival</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1672</th>\n",
       "      <td>1673</td>\n",
       "      <td>Tuesday's incident was the second deadly shoot...</td>\n",
       "      <td>41</td>\n",
       "      <td>49</td>\n",
       "      <td>shooting</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1850</th>\n",
       "      <td>1851</td>\n",
       "      <td>Also, the statement claims that the Huangyan I...</td>\n",
       "      <td>95</td>\n",
       "      <td>108</td>\n",
       "      <td>integral part</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9653</th>\n",
       "      <td>9654</td>\n",
       "      <td>Not only is he the commander in chief who can ...</td>\n",
       "      <td>124</td>\n",
       "      <td>133</td>\n",
       "      <td>Americans</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3325</th>\n",
       "      <td>3326</td>\n",
       "      <td>The ambassador's arrival had not been announce...</td>\n",
       "      <td>75</td>\n",
       "      <td>82</td>\n",
       "      <td>secrecy</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9587</th>\n",
       "      <td>9588</td>\n",
       "      <td>RELATED Afghanistan: 5 areas of concern after ...</td>\n",
       "      <td>171</td>\n",
       "      <td>180</td>\n",
       "      <td>extremely</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10978</th>\n",
       "      <td>10979</td>\n",
       "      <td>Mangled bodies were seen lying in the road aft...</td>\n",
       "      <td>25</td>\n",
       "      <td>30</td>\n",
       "      <td>lying</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11471</th>\n",
       "      <td>11472</td>\n",
       "      <td>Omar Hamza said via Skype that three people we...</td>\n",
       "      <td>49</td>\n",
       "      <td>55</td>\n",
       "      <td>killed</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>45</td>\n",
       "      <td>The Philippine navy was sending additional ves...</td>\n",
       "      <td>75</td>\n",
       "      <td>79</td>\n",
       "      <td>lies</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3485</th>\n",
       "      <td>3486</td>\n",
       "      <td>Although Sunday's election results signal Puti...</td>\n",
       "      <td>153</td>\n",
       "      <td>169</td>\n",
       "      <td>reduced majority</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12993</th>\n",
       "      <td>12994</td>\n",
       "      <td>Three Egyptian civilians were killed and a tot...</td>\n",
       "      <td>80</td>\n",
       "      <td>86</td>\n",
       "      <td>police</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8938</th>\n",
       "      <td>8939</td>\n",
       "      <td>Obama's visit came a year after U.S. special f...</td>\n",
       "      <td>52</td>\n",
       "      <td>58</td>\n",
       "      <td>troops</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9785</th>\n",
       "      <td>9786</td>\n",
       "      <td>Obama flew into Kabul in secret in the dead of...</td>\n",
       "      <td>88</td>\n",
       "      <td>93</td>\n",
       "      <td>Hamid</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>432</td>\n",
       "      <td>In northern Lebanon, meanwhile, residents said...</td>\n",
       "      <td>195</td>\n",
       "      <td>202</td>\n",
       "      <td>opposed</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12269</th>\n",
       "      <td>12270</td>\n",
       "      <td>He also said that \"the tide has turned\" over t...</td>\n",
       "      <td>23</td>\n",
       "      <td>38</td>\n",
       "      <td>tide has turned</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.15</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11516</th>\n",
       "      <td>11517</td>\n",
       "      <td>Suicide bombers attacked a compound housing We...</td>\n",
       "      <td>77</td>\n",
       "      <td>82</td>\n",
       "      <td>hours</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12863</th>\n",
       "      <td>12864</td>\n",
       "      <td>France, the eurozone's second largest economy,...</td>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td>eurozone</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7782</th>\n",
       "      <td>7783</td>\n",
       "      <td>Libertyville, Illinois-based Motorola Mobility...</td>\n",
       "      <td>61</td>\n",
       "      <td>78</td>\n",
       "      <td>regulatory filing</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.40</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2511</th>\n",
       "      <td>2512</td>\n",
       "      <td>The government has already forced banks to mak...</td>\n",
       "      <td>43</td>\n",
       "      <td>47</td>\n",
       "      <td>make</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>554</td>\n",
       "      <td>Putin officially registered on Wednesday to ru...</td>\n",
       "      <td>135</td>\n",
       "      <td>141</td>\n",
       "      <td>showed</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>256</td>\n",
       "      <td>However, after the trip, presumptive Republica...</td>\n",
       "      <td>90</td>\n",
       "      <td>110</td>\n",
       "      <td>somewhat somber tone</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12415</th>\n",
       "      <td>12416</td>\n",
       "      <td>Lieutenant Goodyear said at a news conference ...</td>\n",
       "      <td>55</td>\n",
       "      <td>66</td>\n",
       "      <td>authorities</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         idx                                               text    l    r  \\\n",
       "11556  11557  The Taliban said it was in response to Obama's...   64   73   \n",
       "11551  11552  The Taliban claimed responsibility for the att...  196  206   \n",
       "4887    4888  The incident followed the killing in August of...  106  115   \n",
       "6500    6501  The other children -- Joel Johnson, 12, Jazlin...  105  111   \n",
       "12380  12381  PRESIDENT Barack Obama, speaking to a US telev...   78   82   \n",
       "5328    5329  Rastan has in the past been a major source of ...    0    6   \n",
       "2241    2242  Officials and witnesses said a suicide car bom...   72   81   \n",
       "1177    1178  Another man and a woman, both aged 28, were ar...    8   11   \n",
       "2309    2310  Part of the plan includes the deployment in fl...   75   86   \n",
       "9742    9743    He credited the Afghan security forces for q...  205  212   \n",
       "1549    1550  The shooting happened in Port St. John, about ...  175  183   \n",
       "12034  12035  As a result, with the recession, so many peopl...   82   91   \n",
       "6430    6431  Market doubts about the full extent of the los...  120  133   \n",
       "7369    7370  Spain, the eurozone's fourth-largest economy, ...  116  131   \n",
       "6562    6563  Hundreds of protesters have been detained in b...   50   56   \n",
       "6797    6798  But on Tuesday, when the Gregorio del Pilar di...  168  175   \n",
       "2537    2538  Thirty-three-year-old Tonya Thomas fatally sho...   28   34   \n",
       "7137    7138  Spain is set to intensify the clean-up of its ...  134  141   \n",
       "8097    8098  That next day, when she woke him up for school...   24   28   \n",
       "341      342  Witnesses at the scene of the shooting describ...   68   75   \n",
       "6273    6274  The attack occurred at the gate of a compound ...   27   31   \n",
       "11198  11199  In Pictures: Winning hearts and minds in Afgha...   13   27   \n",
       "13997  13998  Stephen Biddle, a defense analyst at the Counc...  185  201   \n",
       "2426    2427  The Embassy is still engaged in discussions wi...  190  204   \n",
       "1272    1273  Putin was expected to formally register later ...   22   30   \n",
       "4079    4080  Asserting that the US had largely achieved its...    0    9   \n",
       "277      278  Success in Afghanistan is vital to our nation'...    0    7   \n",
       "148      149  The Spanish government source said the 30-bill...  247  253   \n",
       "8037    8038  The neighbors called police immediately who he...   64   71   \n",
       "1672    1673  Tuesday's incident was the second deadly shoot...   41   49   \n",
       "1850    1851  Also, the statement claims that the Huangyan I...   95  108   \n",
       "9653    9654  Not only is he the commander in chief who can ...  124  133   \n",
       "3325    3326  The ambassador's arrival had not been announce...   75   82   \n",
       "9587    9588  RELATED Afghanistan: 5 areas of concern after ...  171  180   \n",
       "10978  10979  Mangled bodies were seen lying in the road aft...   25   30   \n",
       "11471  11472  Omar Hamza said via Skype that three people we...   49   55   \n",
       "44        45  The Philippine navy was sending additional ves...   75   79   \n",
       "3485    3486  Although Sunday's election results signal Puti...  153  169   \n",
       "12993  12994  Three Egyptian civilians were killed and a tot...   80   86   \n",
       "8938    8939  Obama's visit came a year after U.S. special f...   52   58   \n",
       "9785    9786  Obama flew into Kabul in secret in the dead of...   88   93   \n",
       "431      432  In northern Lebanon, meanwhile, residents said...  195  202   \n",
       "12269  12270  He also said that \"the tide has turned\" over t...   23   38   \n",
       "11516  11517  Suicide bombers attacked a compound housing We...   77   82   \n",
       "12863  12864  France, the eurozone's second largest economy,...   12   20   \n",
       "7782    7783  Libertyville, Illinois-based Motorola Mobility...   61   78   \n",
       "2511    2512  The government has already forced banks to mak...   43   47   \n",
       "553      554  Putin officially registered on Wednesday to ru...  135  141   \n",
       "255      256  However, after the trip, presumptive Republica...   90  110   \n",
       "12415  12416  Lieutenant Goodyear said at a news conference ...   55   66   \n",
       "\n",
       "                        sub  n1  n2  c1  c2     p  s_sub_token_len  \\\n",
       "11556             strategic  10  10   1   3  0.20                1   \n",
       "11551            passers-by  10  10   0   0  0.00                1   \n",
       "4887              militants  10  10   6   2  0.40                1   \n",
       "6500                 Thomas  10  10   0   0  0.00                1   \n",
       "12380                  Base  10  10   0   0  0.00                1   \n",
       "5328                 Rastan  10  10   0   1  0.05                1   \n",
       "2241              disguised  10  10   5   9  0.70                1   \n",
       "1177                    man  10  10   0   0  0.00                1   \n",
       "2309            UN military  10  10   0   0  0.00                2   \n",
       "9742                members  10  10   0   0  0.00                1   \n",
       "1549               location  10  10   0   0  0.00                1   \n",
       "12034             mortgages  10  10  10  10  1.00                1   \n",
       "6430          property boom  10  10   3   0  0.15                2   \n",
       "7369        swollen deficit  10  10   6   7  0.65                2   \n",
       "6562                 cities  10  10   0   0  0.00                1   \n",
       "6797                vessels  10  10   2   1  0.15                1   \n",
       "2537                 Thomas  10  10   0   0  0.00                1   \n",
       "7137                details  10  10   0   0  0.00                1   \n",
       "8097                   woke  10  10   0   0  0.00                1   \n",
       "341                 leaving  10  10   0   0  0.00                1   \n",
       "6273                   gate  10  10   0   0  0.00                1   \n",
       "11198        Winning hearts  10  10   1   0  0.05                2   \n",
       "13997      security partner  10  10   2   0  0.10                2   \n",
       "2426         well preserved  10  10   1   1  0.10                2   \n",
       "1272               formally  10  10   0   0  0.00                1   \n",
       "4079              Asserting  10  10  10   9  0.95                1   \n",
       "277                 Success  10  10   0   0  0.00                1   \n",
       "148                  figure  10  10   0   0  0.00                1   \n",
       "8037                arrival  10  10   0   0  0.00                1   \n",
       "1672               shooting  10  10   0   1  0.05                1   \n",
       "1850          integral part  10  10   0   2  0.10                2   \n",
       "9653              Americans  10  10   0   0  0.00                1   \n",
       "3325                secrecy  10  10   7   3  0.50                1   \n",
       "9587              extremely  10  10   2   3  0.25                1   \n",
       "10978                 lying  10  10   0   0  0.00                1   \n",
       "11471                killed  10  10   0   0  0.00                1   \n",
       "44                     lies  10  10   0   0  0.00                1   \n",
       "3485       reduced majority  10  10   1   0  0.05                2   \n",
       "12993                police  10  10   0   0  0.00                1   \n",
       "8938                 troops  10  10   0   0  0.00                1   \n",
       "9785                  Hamid  10  10   0   0  0.00                1   \n",
       "431                 opposed  10  10   0   0  0.00                1   \n",
       "12269       tide has turned  10  10   1   2  0.15                3   \n",
       "11516                 hours  10  10   0   0  0.00                1   \n",
       "12863              eurozone  10  10   0   0  0.00                1   \n",
       "7782      regulatory filing  10  10   5   3  0.40                2   \n",
       "2511                   make  10  10   0   0  0.00                1   \n",
       "553                  showed  10  10   0   0  0.00                1   \n",
       "255    somewhat somber tone  10  10   0   0  0.00                3   \n",
       "12415           authorities  10  10   2   0  0.10                1   \n",
       "\n",
       "       s_sub_char_len  s_capitalized  \n",
       "11556               9              0  \n",
       "11551              10              0  \n",
       "4887                9              0  \n",
       "6500                6              1  \n",
       "12380               4              1  \n",
       "5328                6              1  \n",
       "2241                9              0  \n",
       "1177                3              0  \n",
       "2309               11              2  \n",
       "9742                7              0  \n",
       "1549                8              0  \n",
       "12034               9              0  \n",
       "6430               13              0  \n",
       "7369               15              0  \n",
       "6562                6              0  \n",
       "6797                7              0  \n",
       "2537                6              1  \n",
       "7137                7              0  \n",
       "8097                4              0  \n",
       "341                 7              0  \n",
       "6273                4              0  \n",
       "11198              14              1  \n",
       "13997              16              0  \n",
       "2426               14              0  \n",
       "1272                8              0  \n",
       "4079                9              1  \n",
       "277                 7              1  \n",
       "148                 6              0  \n",
       "8037                7              0  \n",
       "1672                8              0  \n",
       "1850               13              0  \n",
       "9653                9              1  \n",
       "3325                7              0  \n",
       "9587                9              0  \n",
       "10978               5              0  \n",
       "11471               6              0  \n",
       "44                  4              0  \n",
       "3485               16              0  \n",
       "12993               6              0  \n",
       "8938                6              0  \n",
       "9785                5              1  \n",
       "431                 7              0  \n",
       "12269              15              0  \n",
       "11516               5              0  \n",
       "12863               8              0  \n",
       "7782               17              0  \n",
       "2511                4              0  \n",
       "553                 6              0  \n",
       "255                20              0  \n",
       "12415              11              0  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model!\n"
     ]
    }
   ],
   "source": [
    "w2v_model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    './GoogleNews-vectors-negative300.bin', binary=True) \n",
    "print(\"Loaded model!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_val = train_test_split(df, test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_TAG_CACHE = {}\n",
    "\n",
    "def pos_tag(text):\n",
    "    global POS_TAG_CACHE\n",
    "    if text not in POS_TAG_CACHE:\n",
    "        POS_TAG_CACHE[text] = nltk.pos_tag(\n",
    "            nltk.word_tokenize(text),\n",
    "            tagset='universal',\n",
    "        )\n",
    "    return POS_TAG_CACHE[text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(['', 'ADJ', 'ADP', 'ADV', 'CONJ', 'DET', 'NOUN', 'NUM', 'PRON',\n",
      "       'PRT', 'VERB', 'X'], dtype='<U4')]\n",
      "[array(['', 'ADJ', 'ADP', 'ADV', 'CONJ', 'DET', 'NOUN', 'NUM', 'PRON',\n",
      "       'PRT', 'VERB', 'X'], dtype='<U4')]\n",
      "[ 1.00000000e+00  3.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -5.76321607e-18 -1.11022302e-16]\n",
      "(11201, 6)\n"
     ]
    }
   ],
   "source": [
    "def get_syn(df, data, include_pos=True, testing=False):\n",
    "    cols = []\n",
    "    for col in df.columns:\n",
    "        if col.startswith(\"s_\"):\n",
    "            cols.append(df[col].values)\n",
    "    ret = np.column_stack(cols)\n",
    "    \n",
    "    if include_pos:\n",
    "        pos_tags = []\n",
    "        for idx in range(len(df)):\n",
    "            target = nltk.word_tokenize(df['sub'].values[idx])[0]\n",
    "            tag = \"\"\n",
    "            for w, t in pos_tag(df['text'].values[idx]):\n",
    "                if w == target:\n",
    "                    tag = t\n",
    "                    break\n",
    "            pos_tags.append(tag)\n",
    "        pos_tags = np.array(pos_tags).reshape(len(pos_tags), 1)\n",
    "        # print(pos_tags)\n",
    "        \n",
    "        if not testing:\n",
    "            enc = OneHotEncoder()\n",
    "            enc.fit(pos_tags)\n",
    "            data['syn'] = {\n",
    "                'pos_enc': enc,\n",
    "            }\n",
    "        enc = data['syn']['pos_enc']\n",
    "        print(enc.categories_)\n",
    "        \n",
    "        pos_tags_onehot = enc.transform(pos_tags)\n",
    "        \n",
    "        if not testing:\n",
    "            pca = TruncatedSVD(n_components=3)\n",
    "            pca.fit(pos_tags_onehot.toarray())\n",
    "            data[\"syn\"][\"pos_pca\"] = pca\n",
    "        pca = data[\"syn\"][\"pos_pca\"]\n",
    "        \n",
    "        pos_tags_enc = pca.transform(pos_tags_onehot.toarray())\n",
    "        # pos_tags_enc = pos_tags_onehot.toarray()\n",
    "\n",
    "        ret = np.hstack([ret, pos_tags_enc])\n",
    "    return ret\n",
    "\n",
    "X_syn_train = get_syn(df_train, DATA)\n",
    "X_syn_val = get_syn(df_val, DATA, testing=True)\n",
    "print(X_syn_train[10])\n",
    "print(X_syn_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKIPPING:  outcrops\n",
      "SKIPPING:  outcrops\n",
      "SKIPPING:  shoal\n",
      "SKIPPING:  euros\n",
      "SKIPPING:  n't\n",
      "SKIPPING:  London-based\n",
      "SKIPPING:  euros\n",
      "SKIPPING:  euros\n",
      "SKIPPING:  euros\n",
      "SKIPPING:  Bankia\n",
      "SKIPPING:  long-time\n",
      "SKIPPING:  Rastan\n",
      "SKIPPING:  Britain-based\n",
      "SKIPPING:  Britain-based\n",
      "SKIPPING:  rebel-held\n",
      "SKIPPING:  rebel-held\n",
      "SKIPPING:  Rastan\n",
      "SKIPPING:  al-Assad\n",
      "SKIPPING:  al-Assad\n",
      "SKIPPING:  Cloverhill\n",
      "SKIPPING:  Tallaght\n",
      "SKIPPING:  Inchicore\n",
      "SKIPPING:  Gardai\n",
      "SKIPPING:  Ballyfermot\n",
      "SKIPPING:  Ex-Soviet\n",
      "SKIPPING:  Hayaleen\n",
      "SKIPPING:  gunbattles\n",
      "SKIPPING:  Alawite\n",
      "SKIPPING:  Alawite\n",
      "SKIPPING:  “\n",
      "SKIPPING:  “\n",
      "SKIPPING:  Twitter\n",
      "SKIPPING:  Karzai\n",
      "SKIPPING:  ,\n",
      "SKIPPING:  've\n",
      "SKIPPING:  're\n",
      "SKIPPING:  al-Qaeda\n",
      "SKIPPING:  Putin\n",
      "SKIPPING:  pro-Putin\n",
      "SKIPPING:  pro-Putin\n",
      "SKIPPING:  Putin\n",
      "SKIPPING:  Putin\n",
      "SKIPPING:  Afghan-US\n",
      "SKIPPING:  commander-in-chief\n",
      "SKIPPING:  election-year\n",
      "SKIPPING:  Al-Qaeda\n",
      "SKIPPING:  Al-Qaeda\n",
      "SKIPPING:  decade-long\n",
      "SKIPPING:  UK-based\n",
      "SKIPPING:  GMT\n",
      "SKIPPING:  Rastan\n",
      "SKIPPING:  Rastan\n",
      "SKIPPING:  neighbouring\n",
      "SKIPPING:  Jazeera\n",
      "SKIPPING:  armoured\n",
      "SKIPPING:  armoured\n",
      "SKIPPING:  Rastan\n",
      "SKIPPING:  Rastan\n",
      "SKIPPING:  NNA\n",
      "SKIPPING:  rocket-propelled\n",
      "SKIPPING:  rocket-propelled\n",
      "SKIPPING:  pro-Syrian\n",
      "SKIPPING:  pro-Syrian\n",
      "SKIPPING:  pro-Putin\n",
      "SKIPPING:  pro-Putin\n",
      "SKIPPING:  state-owned\n",
      "SKIPPING:  state-owned\n",
      "SKIPPING:  Putin\n",
      "SKIPPING:  Hajar\n",
      "SKIPPING:  Daraa\n",
      "SKIPPING:  ,\n",
      "SKIPPING:  Idleb\n",
      "SKIPPING:  state-run\n",
      "SKIPPING:  Daraa\n",
      "SKIPPING:  Idleb\n",
      "SKIPPING:  n't\n",
      "SKIPPING:  LCC\n",
      "SKIPPING:  LCC\n",
      "SKIPPING:  Qaeda\n",
      "SKIPPING:  Qaeda\n",
      "SKIPPING:  Bellaghy\n",
      "SKIPPING:  Toomebridge\n",
      "SKIPPING:  PSNI\n",
      "SKIPPING:  Rostov-on-Don\n",
      "SKIPPING:  pro-Putin\n",
      "SKIPPING:  pro-Putin\n",
      "SKIPPING:  Putin\n",
      "SKIPPING:  n't\n",
      "SKIPPING:  nationalises\n",
      "SKIPPING:  Bankia\n",
      "SKIPPING:  Bankia\n",
      "SKIPPING:  bn\n",
      "SKIPPING:  bn\n",
      "SKIPPING:  California-based\n",
      "SKIPPING:  Lewes\n",
      "SKIPPING:  al-Assad\n",
      "SKIPPING:  Rastan\n",
      "SKIPPING:  UN-backed\n",
      "SKIPPING:  UN-backed\n",
      "SKIPPING:  EU\n",
      "SKIPPING:  Rastan\n",
      "SKIPPING:  next-door\n",
      "SKIPPING:  next-door\n",
      "SKIPPING:  neighbour\n",
      "SKIPPING:  neighbour\n",
      "SKIPPING:  neighbour\n",
      "SKIPPING:  Lt\n",
      "SKIPPING:  awoken\n",
      "SKIPPING:  awoken\n",
      "SKIPPING:  shoal\n",
      "SKIPPING:  shoal\n",
      "SKIPPING:  emphasise\n",
      "SKIPPING:  AFP\n",
      "SKIPPING:  flare-up\n",
      "SKIPPING:  DFA\n",
      "SKIPPING:  PHL\n",
      "SKIPPING:  Huangyan\n",
      "SKIPPING:  Panatag\n",
      "SKIPPING:  Shoal\n",
      "SKIPPING:  lagnoon\n",
      "SKIPPING:  Huangyan\n",
      "SKIPPING:  Rajoy\n",
      "SKIPPING:  nationalised\n",
      "SKIPPING:  fourth-biggest\n",
      "SKIPPING:  Bankia\n",
      "SKIPPING:  Guindos\n",
      "SKIPPING:  IBEX-35\n",
      "SKIPPING:  eurozone\n",
      "SKIPPING:  number-two\n",
      "SKIPPING:  BBVA\n",
      "SKIPPING:  Bankia\n",
      "SKIPPING:  al-Tamana\n",
      "SKIPPING:  Britain-based\n",
      "SKIPPING:  III\n",
      "SKIPPING:  rules-based\n",
      "SKIPPING:  rules-based\n",
      "SKIPPING:  Spratlys\n",
      "SKIPPING:  Spratlys\n",
      "SKIPPING:  Spratlys\n",
      "SKIPPING:  DFA\n",
      "SKIPPING:  Panatag\n",
      "SKIPPING:  (\n",
      "SKIPPING:  Shoal\n",
      "SKIPPING:  Panatag\n",
      "SKIPPING:  Panatag\n",
      "SKIPPING:  Shoal\n",
      "SKIPPING:  base-point\n",
      "SKIPPING:  Zambales\n",
      "SKIPPING:  Karzai\n",
      "SKIPPING:  burqas\n",
      "SKIPPING:  U.S.\n",
      "SKIPPING:  U.S.\n",
      "SKIPPING:  Syria-linked\n",
      "SKIPPING:  Syria-linked\n",
      "SKIPPING:  Toome\n",
      "SKIPPING:  DFA\n",
      "SKIPPING:  DFA\n",
      "SKIPPING:  China-Philippine\n",
      "SKIPPING:  DFA\n",
      "SKIPPING:  DFA\n",
      "SKIPPING:  clean-up\n",
      "SKIPPING:  clean-up\n",
      "SKIPPING:  last-minute\n",
      "SKIPPING:  last-minute\n",
      "SKIPPING:  euros\n",
      "SKIPPING:  euros\n",
      "SKIPPING:  Thirty-three-year-old\n",
      "SKIPPING:  pro-Putin\n",
      "SKIPPING:  pro-Putin\n",
      "SKIPPING:  Putin\n",
      "SKIPPING:  Putin\n",
      "SKIPPING:  vote-rigging\n",
      "SKIPPING:  vote-rigging\n",
      "SKIPPING:  Kremlin-dominated\n",
      "SKIPPING:  Kremlin-dominated\n",
      "SKIPPING:  Barack\n",
      "SKIPPING:  Bagram\n",
      "SKIPPING:  Rastan\n",
      "SKIPPING:  Bankia\n",
      "SKIPPING:  euros\n",
      "SKIPPING:  eurozone\n",
      "SKIPPING:  eurozone\n",
      "SKIPPING:  eurozone\n",
      "SKIPPING:  IMF\n",
      "SKIPPING:  Peston\n",
      "SKIPPING:  JPMorgan\n",
      "SKIPPING:  bn\n",
      "SKIPPING:  Soraya\n",
      "SKIPPING:  Santamaria\n",
      "SKIPPING:  Soraya\n",
      "SKIPPING:  Santamaria\n",
      "SKIPPING:  Guindos\n",
      "SKIPPING:  Guindos\n",
      "SKIPPING:  euros\n",
      "SKIPPING:  euros\n",
      "SKIPPING:  bn\n",
      "SKIPPING:  euros\n",
      "SKIPPING:  Guindos\n",
      "SKIPPING:  Guindos\n",
      "SKIPPING:  euros\n",
      "SKIPPING:  five-year\n",
      "SKIPPING:  five-year\n",
      "SKIPPING:  incentivise\n",
      "SKIPPING:  incentivise\n",
      "SKIPPING:  incentivise\n",
      "SKIPPING:  offload\n",
      "SKIPPING:  offload\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  Motorolla\n",
      "SKIPPING:  VentureBeat\n",
      "SKIPPING:  VentureBeat\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  hold-out\n",
      "SKIPPING:  hold-out\n",
      "SKIPPING:  hold-out\n",
      "SKIPPING:  go-ahead\n",
      "SKIPPING:  go-ahead\n",
      "SKIPPING:  BBs\n",
      "SKIPPING:  AFP\n",
      "SKIPPING:  AFP\n",
      "SKIPPING:  “\n",
      "SKIPPING:  ”\n",
      "SKIPPING:  ”\n",
      "SKIPPING:  “\n",
      "SKIPPING:  Karzai\n",
      "SKIPPING:  Toome\n",
      "SKIPPING:  traumatised\n",
      "SKIPPING:  SDLP\n",
      "SKIPPING:  Mid-Ulster\n",
      "SKIPPING:  SDLP\n",
      "SKIPPING:  Mid-Ulster\n",
      "SKIPPING:  MLA\n",
      "SKIPPING:  Levanon\n",
      "SKIPPING:  AFP\n",
      "SKIPPING:  Levanon\n",
      "SKIPPING:  destabilizes\n",
      "SKIPPING:  destabilizes\n",
      "SKIPPING:  falsifications\n",
      "SKIPPING:  falsifications\n",
      "SKIPPING:  falsifications\n",
      "SKIPPING:  Putin\n",
      "SKIPPING:  Putin\n",
      "SKIPPING:  pro-Kremlin\n",
      "SKIPPING:  pro-Kremlin\n",
      "SKIPPING:  Putin\n",
      "SKIPPING:  neighbouring\n",
      "SKIPPING:  neighbouring\n",
      "SKIPPING:  Shoal\n",
      "SKIPPING:  Shoal\n",
      "SKIPPING:  Keqing\n",
      "SKIPPING:  enquiries\n",
      "SKIPPING:  enquiries\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  GOOG\n",
      "SKIPPING:  MMI\n",
      "SKIPPING:  AAPL\n",
      "SKIPPING:  ,\n",
      "SKIPPING:  ”\n",
      "SKIPPING:  e-mailed\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  smartphone\n",
      "SKIPPING:  Libertyville\n",
      "SKIPPING:  Illinois-based\n",
      "SKIPPING:  Yaakov\n",
      "SKIPPING:  Anti-Israeli\n",
      "SKIPPING:  Anti-Israeli\n",
      "SKIPPING:  Bellaghy\n",
      "SKIPPING:  EU\n",
      "SKIPPING:  Rastan\n",
      "SKIPPING:  U.N.-backed\n",
      "SKIPPING:  Rastan\n",
      "SKIPPING:  Rastan\n",
      "SKIPPING:  km\n",
      "SKIPPING:  U.N.\n",
      "SKIPPING:  Lt\n",
      "SKIPPING:  Merseyside\n",
      "SKIPPING:  Bidston\n",
      "SKIPPING:  Bidston\n",
      "SKIPPING:  Birkenhead\n",
      "SKIPPING:  Merseyside\n",
      "SKIPPING:  Wirral\n",
      "SKIPPING:  Merseyside\n",
      "SKIPPING:  post-mortem\n",
      "SKIPPING:  post-mortem\n",
      "SKIPPING:  Bidston\n",
      "SKIPPING:  Bidston\n",
      "SKIPPING:  Deupities\n",
      "SKIPPING:  BB\n",
      "SKIPPING:  BB\n",
      "SKIPPING:  WFTV\n",
      "SKIPPING:  Jaxs\n",
      "SKIPPING:  n't\n",
      "SKIPPING:  n't\n",
      "SKIPPING:  Rockledge\n",
      "SKIPPING:  Fla\n",
      "SKIPPING:  al-Qaeda\n",
      "SKIPPING:  counter-terrorism\n",
      "SKIPPING:  counter-terrorism\n",
      "SKIPPING:  Barack\n",
      "SKIPPING:  Abdul-Rahman\n",
      "SKIPPING:  Rastan\n",
      "SKIPPING:  minatory\n",
      "SKIPPING:  minatory\n",
      "SKIPPING:  Alawite\n",
      "SKIPPING:  Alawite\n",
      "SKIPPING:  Alawite\n",
      "SKIPPING:  Alawite\n",
      "SKIPPING:  shabiha\n",
      "SKIPPING:  Alawite\n",
      "SKIPPING:  Barack\n",
      "SKIPPING:  decade-long\n",
      "SKIPPING:  decade-long\n",
      "SKIPPING:  Karzai\n",
      "SKIPPING:  Karzai\n",
      "SKIPPING:  Bagram\n",
      "SKIPPING:  Zabihullah\n",
      "SKIPPING:  Mujahid\n",
      "SKIPPING:  Haqqani\n",
      "SKIPPING:  Haqqani\n",
      "SKIPPING:  Waziristan\n",
      "SKIPPING:  vote-rigging\n",
      "SKIPPING:  vote-rigging\n",
      "SKIPPING:  Kremlin-dominated\n",
      "SKIPPING:  Kremlin-dominated\n",
      "SKIPPING:  Facebook\n",
      "SKIPPING:  Huangyan\n",
      "SKIPPING:  GMA\n",
      "SKIPPING:  Levanon\n",
      "SKIPPING:  Levanon\n",
      "SKIPPING:  scandal-marred\n",
      "SKIPPING:  scandal-marred\n",
      "SKIPPING:  Putin\n",
      "SKIPPING:  Putin\n",
      "SKIPPING:  vote-rigging\n",
      "SKIPPING:  Kremlin-dominated\n",
      "SKIPPING:  Kremlin-dominated\n",
      "SKIPPING:  Levanon\n",
      "SKIPPING:  Lior\n",
      "SKIPPING:  Ben-Dor\n",
      "SKIPPING:  anti-Israel\n",
      "SKIPPING:  anti-Israel\n",
      "SKIPPING:  Levanon\n",
      "SKIPPING:  clean-up\n",
      "SKIPPING:  clean-up\n",
      "SKIPPING:  clean-up\n",
      "SKIPPING:  clean-up\n",
      "SKIPPING:  centre-right\n",
      "SKIPPING:  Rajoy\n",
      "SKIPPING:  EU\n",
      "SKIPPING:  EU\n",
      "SKIPPING:  four-year\n",
      "SKIPPING:  Bankia\n",
      "SKIPPING:  Levanon\n",
      "SKIPPING:  high-rise\n",
      "SKIPPING:  high-rise\n",
      "SKIPPING:  Israeli-Egyptian\n",
      "SKIPPING:  Quraya\n",
      "SKIPPING:  Deir\n",
      "SKIPPING:  Ezzor\n",
      "SKIPPING:  Deir\n",
      "SKIPPING:  Ezzor\n",
      "SKIPPING:  Qaboon\n",
      "SKIPPING:  UN-Arab\n",
      "SKIPPING:  Rajoy\n",
      "SKIPPING:  fourth-biggest\n",
      "SKIPPING:  Bankia\n",
      "SKIPPING:  Guindos\n",
      "SKIPPING:  Guindos\n",
      "SKIPPING:  asset-management\n",
      "SKIPPING:  asset-management\n",
      "SKIPPING:  fire-sale\n",
      "SKIPPING:  Guindos\n",
      "SKIPPING:  Deficit-cutting\n",
      "SKIPPING:  Deficit-cutting\n",
      "SKIPPING:  annnounced\n",
      "SKIPPING:  deficit-cutting\n",
      "SKIPPING:  deficit-cutting\n",
      "SKIPPING:  Ballyfermot\n",
      "SKIPPING:  Ballyfermot\n",
      "SKIPPING:  Finnegans\n",
      "SKIPPING:  Inchicore\n",
      "SKIPPING:  Tallaght\n",
      "SKIPPING:  life-threatening\n",
      "SKIPPING:  Gardaí\n",
      "SKIPPING:  Goldenbridge\n",
      "SKIPPING:  Inchicore\n",
      "SKIPPING:  IBEX-35\n",
      "SKIPPING:  eurozone\n",
      "SKIPPING:  BBVA\n",
      "SKIPPING:  euros\n",
      "SKIPPING:  number-two\n",
      "SKIPPING:  Bankia\n",
      "SKIPPING:  euros\n",
      "SKIPPING:  euros\n",
      "SKIPPING:  Bankia\n",
      "SKIPPING:  bn\n",
      "SKIPPING:  euros\n",
      "SKIPPING:  Alawite\n",
      "SKIPPING:  Alawite\n",
      "SKIPPING:  Alawite\n",
      "SKIPPING:  Alawite\n",
      "SKIPPING:  Alawite\n",
      "SKIPPING:  Alawite\n",
      "SKIPPING:  Shi'ite\n",
      "SKIPPING:  Rastan\n",
      "SKIPPING:  Alawite\n",
      "SKIPPING:  Rastan\n",
      "SKIPPING:  wireless-equipment\n",
      "SKIPPING:  wireless-equipment\n",
      "SKIPPING:  California-based\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  e-mailed\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  Bellaghy\n",
      "SKIPPING:  Gardaí\n",
      "SKIPPING:  Karzai\n",
      "SKIPPING:  al-Qaida\n",
      "SKIPPING:  Sediq\n",
      "SKIPPING:  Sediqi\n",
      "SKIPPING:  passer-by\n",
      "SKIPPING:  passer-by\n",
      "SKIPPING:  thumbs-up\n",
      "SKIPPING:  thumbs-up\n",
      "SKIPPING:  vote-rigging\n",
      "SKIPPING:  vote-rigging\n",
      "SKIPPING:  Kirill\n",
      "SKIPPING:  Kudryavtsev\n",
      "SKIPPING:  Nemtsov\n",
      "SKIPPING:  Triumfalnaya\n",
      "SKIPPING:  Martinez-Abascal\n",
      "SKIPPING:  IESE\n",
      "SKIPPING:  Bankia\n",
      "SKIPPING:  Bankia\n",
      "SKIPPING:  self-inflicted\n",
      "SKIPPING:  self-inflicted\n",
      "SKIPPING:  nation-building\n",
      "SKIPPING:  Barack\n",
      "SKIPPING:  NYSE\n",
      "SKIPPING:  :\n",
      "SKIPPING:  MOT\n",
      "SKIPPING:  NYSE\n",
      "SKIPPING:  MOT\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  monetize\n",
      "SKIPPING:  Gmail\n",
      "SKIPPING:  YouTube\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  Web-based\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  AdWords\n",
      "SKIPPING:  AdSense\n",
      "SKIPPING:  Goldenbridge\n",
      "SKIPPING:  Gardaí\n",
      "SKIPPING:  Tallaght\n",
      "SKIPPING:  Ballyfermot\n",
      "SKIPPING:  ’\n",
      "SKIPPING:  Cloverhill\n",
      "SKIPPING:  non-life-threatening\n",
      "SKIPPING:  Inchicore\n",
      "SKIPPING:  Jazlin\n",
      "SKIPPING:  Jaxs\n",
      "SKIPPING:  texted\n",
      "SKIPPING:  Zabiullah\n",
      "SKIPPING:  Mujahid\n",
      "SKIPPING:  mujahedin\n",
      "SKIPPING:  Zabiullah\n",
      "SKIPPING:  Mujahid\n",
      "SKIPPING:  nationalisation\n",
      "SKIPPING:  Bankia\n",
      "SKIPPING:  nationalising\n",
      "SKIPPING:  fourth-biggest\n",
      "SKIPPING:  Bankia\n",
      "SKIPPING:  Rajoy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKIPPING:  Jazlin\n",
      "SKIPPING:  Jaxs\n",
      "SKIPPING:  ex-Soviet\n",
      "SKIPPING:  Putin\n",
      "SKIPPING:  two-thirds\n",
      "SKIPPING:  vote-rigging\n",
      "SKIPPING:  ballot-box\n",
      "SKIPPING:  ballot-box\n",
      "SKIPPING:  ballot-box\n",
      "SKIPPING:  n't\n",
      "SKIPPING:  Zabiullah\n",
      "SKIPPING:  Mujahid\n",
      "SKIPPING:  Zabiullah\n",
      "SKIPPING:  Mujahid\n",
      "SKIPPING:  Karzai\n",
      "SKIPPING:  Karzai\n",
      "SKIPPING:  n't\n",
      "SKIPPING:  Jaxs\n",
      "SKIPPING:  Jazzlyn\n",
      "SKIPPING:  n't\n",
      "SKIPPING:  DFA\n",
      "SKIPPING:  BRP\n",
      "SKIPPING:  PF-15\n",
      "SKIPPING:  Palawan\n",
      "SKIPPING:  Zhonggou\n",
      "SKIPPING:  Shoal\n",
      "SKIPPING:  Haijian\n",
      "SKIPPING:  Zhonggou\n",
      "SKIPPING:  Haijian\n",
      "SKIPPING:  PF-15\n",
      "SKIPPING:  al-Qaeda\n",
      "SKIPPING:  war-weary\n",
      "SKIPPING:  war-weary\n",
      "SKIPPING:  Post-ABC\n",
      "SKIPPING:  decade-long\n",
      "SKIPPING:  Post-ABC\n",
      "SKIPPING:  two-thirds\n",
      "SKIPPING:  decade-long\n",
      "SKIPPING:  Keqing\n",
      "SKIPPING:  Shoal\n",
      "SKIPPING:  shoal\n",
      "SKIPPING:  north-western\n",
      "SKIPPING:  Zambales\n",
      "SKIPPING:  shoal\n",
      "SKIPPING:  Spratly\n",
      "SKIPPING:  BRP\n",
      "SKIPPING:  five-year\n",
      "SKIPPING:  still-troubled\n",
      "SKIPPING:  still-troubled\n",
      "SKIPPING:  clean-up\n",
      "SKIPPING:  clean-up\n",
      "SKIPPING:  clean-up\n",
      "SKIPPING:  last-minute\n",
      "SKIPPING:  euros\n",
      "SKIPPING:  euros\n",
      "SKIPPING:  Zhonggou\n",
      "SKIPPING:  Haijian\n",
      "SKIPPING:  Zhonggou\n",
      "SKIPPING:  Haijian\n",
      "SKIPPING:  Shoal\n",
      "SKIPPING:  Huangyan\n",
      "SKIPPING:  shoal\n",
      "SKIPPING:  Panatag\n",
      "SKIPPING:  Spratlys\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  neighbouring\n",
      "SKIPPING:  neighbouring\n",
      "SKIPPING:  Mid-Ulster\n",
      "SKIPPING:  MP\n",
      "SKIPPING:  Mid-Ulster\n",
      "SKIPPING:  MP\n",
      "SKIPPING:  PSNI\n",
      "SKIPPING:  fourth-largest\n",
      "SKIPPING:  eurozone\n",
      "SKIPPING:  GDP\n",
      "SKIPPING:  GDP\n",
      "SKIPPING:  LCC\n",
      "SKIPPING:  Qaboun\n",
      "SKIPPING:  Qaboun\n",
      "SKIPPING:  state-run\n",
      "SKIPPING:  state-run\n",
      "SKIPPING:  centre-right\n",
      "SKIPPING:  centre-right\n",
      "SKIPPING:  high-interest\n",
      "SKIPPING:  high-interest\n",
      "SKIPPING:  clean-up\n",
      "SKIPPING:  clean-up\n",
      "SKIPPING:  clean-up\n",
      "SKIPPING:  euros\n",
      "SKIPPING:  euros\n",
      "SKIPPING:  Libertyville\n",
      "SKIPPING:  Illinois-based\n",
      "SKIPPING:  California-based\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  e-mailed\n",
      "SKIPPING:  wireless-equipment\n",
      "SKIPPING:  wireless-equipment\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  HTC\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  Karzai\n",
      "SKIPPING:  election-year\n",
      "SKIPPING:  armoured\n",
      "SKIPPING:  armoured\n",
      "SKIPPING:  've\n",
      "SKIPPING:  pre-dawn\n",
      "SKIPPING:  pre-dawn\n",
      "SKIPPING:  Karzai\n",
      "SKIPPING:  Rajoy\n",
      "SKIPPING:  Rastan\n",
      "SKIPPING:  UK-based\n",
      "SKIPPING:  EMTs\n",
      "SKIPPING:  Fla\n",
      "SKIPPING:  n't\n",
      "SKIPPING:  Jaxs\n",
      "SKIPPING:  Toome\n",
      "SKIPPING:  Fr\n",
      "SKIPPING:  UTV\n",
      "SKIPPING:  Fr\n",
      "SKIPPING:  clean-ups\n",
      "SKIPPING:  clean-ups\n",
      "SKIPPING:  clean-ups\n",
      "SKIPPING:  after-effects\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  fleshing\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  fleshing\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  smartphones\n",
      "SKIPPING:  in-house\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  HTC\n",
      "SKIPPING:  Gardai\n",
      "SKIPPING:  Ballyfermot\n",
      "SKIPPING:  targetted\n",
      "SKIPPING:  Tallaght\n",
      "SKIPPING:  Inchicore\n",
      "SKIPPING:  Goldenbridge\n",
      "SKIPPING:  gardai\n",
      "SKIPPING:  stand-off\n",
      "SKIPPING:  stand-off\n",
      "SKIPPING:  Zhonggou\n",
      "SKIPPING:  Haijian\n",
      "SKIPPING:  Zhonggou\n",
      "SKIPPING:  Haijian\n",
      "SKIPPING:  Shoal\n",
      "SKIPPING:  Huangyan\n",
      "SKIPPING:  Shoal\n",
      "SKIPPING:  shoal\n",
      "SKIPPING:  Panatag\n",
      "SKIPPING:  Spratlys\n",
      "SKIPPING:  Putin\n",
      "SKIPPING:  two-thirds\n",
      "SKIPPING:  vote-rigging\n",
      "SKIPPING:  vote-rigging\n",
      "SKIPPING:  ballot-box\n",
      "SKIPPING:  ballot-box\n",
      "SKIPPING:  ballot-box\n",
      "SKIPPING:  ballot-box\n",
      "SKIPPING:  al-Qaida\n",
      "SKIPPING:  Bagram\n",
      "SKIPPING:  Karzai\n",
      "SKIPPING:  U.S.\n",
      "SKIPPING:  state-run\n",
      "SKIPPING:  state-run\n",
      "SKIPPING:  al-Assad\n",
      "SKIPPING:  Soraya\n",
      "SKIPPING:  Santamaria\n",
      "SKIPPING:  MP\n",
      "SKIPPING:  MP\n",
      "SKIPPING:  traumatised\n",
      "SKIPPING:  traumatised\n",
      "SKIPPING:  neighbouring\n",
      "SKIPPING:  neighbouring\n",
      "SKIPPING:  Bellaghy\n",
      "SKIPPING:  Bellaghy\n",
      "SKIPPING:  Fr\n",
      "SKIPPING:  n't\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  anti-monopoly\n",
      "SKIPPING:  anti-monopoly\n",
      "SKIPPING:  anti-monopoly\n",
      "SKIPPING:  anti-monopoly\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  n't\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  clamp-down\n",
      "SKIPPING:  clamp-down\n",
      "SKIPPING:  Qaeda\n",
      "SKIPPING:  Rastan\n",
      "SKIPPING:  al-Assad\n",
      "SKIPPING:  al-Assad\n",
      "SKIPPING:  Quraya\n",
      "SKIPPING:  Deir\n",
      "SKIPPING:  az-Zour\n",
      "SKIPPING:  Deir\n",
      "SKIPPING:  az-Zour\n",
      "SKIPPING:  Qaboon\n",
      "SKIPPING:  Qaeda\n",
      "SKIPPING:  —\n",
      "SKIPPING:  Karzai\n",
      "SKIPPING:  off-message\n",
      "SKIPPING:  Karzai\n",
      "SKIPPING:  off-message\n",
      "SKIPPING:  Karzai\n",
      "SKIPPING:  Karzai\n",
      "SKIPPING:  al-Qaida\n",
      "SKIPPING:  pro-Assad\n",
      "SKIPPING:  pro-Assad\n",
      "SKIPPING:  Rastan\n",
      "SKIPPING:  km\n",
      "SKIPPING:  Orontes\n",
      "SKIPPING:  Aleppo\n",
      "SKIPPING:  Ki-moon\n",
      "SKIPPING:  Ki-moon\n",
      "SKIPPING:  full-scale\n",
      "SKIPPING:  full-scale\n",
      "SKIPPING:  six-point\n",
      "SKIPPING:  six-point\n",
      "SKIPPING:  Bellaghy\n",
      "SKIPPING:  Toome\n",
      "SKIPPING:  Caoimhe\n",
      "SKIPPING:  councillor\n",
      "SKIPPING:  Magherafelt\n",
      "SKIPPING:  tight-knit\n",
      "SKIPPING:  Bellaghy\n",
      "SKIPPING:  tight-knit\n",
      "SKIPPING:  councillor\n",
      "SKIPPING:  councillor\n",
      "SKIPPING:  Bellaghy\n",
      "SKIPPING:  BRP\n",
      "SKIPPING:  Haqyar\n",
      "SKIPPING:  pre-positioned\n",
      "SKIPPING:  alighting\n",
      "SKIPPING:  alighting\n",
      "SKIPPING:  pre-dawn\n",
      "SKIPPING:  pre-dawn\n",
      "SKIPPING:  Karzai\n",
      "SKIPPING:  're\n",
      "SKIPPING:  Karzai\n",
      "SKIPPING:  Karzai\n",
      "SKIPPING:  Karzai\n",
      "SKIPPING:  EU\n",
      "SKIPPING:  EU\n",
      "SKIPPING:  Karzai\n",
      "SKIPPING:  inked\n",
      "SKIPPING:  inked\n",
      "SKIPPING:  non-NATO\n",
      "SKIPPING:  non-NATO\n",
      "SKIPPING:  ,\n",
      "SKIPPING:  Karzai\n",
      "SKIPPING:  Karzai\n",
      "SKIPPING:  one-year\n",
      "SKIPPING:  heavy-handed\n",
      "SKIPPING:  heavy-handed\n",
      "SKIPPING:  Ben-Dor\n",
      "SKIPPING:  Levanon\n",
      "SKIPPING:  Ya'akov\n",
      "SKIPPING:  Amita\n",
      "SKIPPING:  closed-circuit\n",
      "SKIPPING:  closed-circuit\n",
      "SKIPPING:  Keqing\n",
      "SKIPPING:  Shoal\n",
      "SKIPPING:  Shoal\n",
      "SKIPPING:  Spratly\n",
      "SKIPPING:  shoal\n",
      "SKIPPING:  Guindos\n",
      "SKIPPING:  Guindos\n",
      "SKIPPING:  Guindos\n",
      "SKIPPING:  Cdn\n",
      "SKIPPING:  rainy-day\n",
      "SKIPPING:  rainy-day\n",
      "SKIPPING:  falsifications\n",
      "SKIPPING:  falsifications\n",
      "SKIPPING:  falsifications\n",
      "SKIPPING:  AFP\n",
      "SKIPPING:  Putin\n",
      "SKIPPING:  euros\n",
      "SKIPPING:  n't\n",
      "SKIPPING:  London-based\n",
      "SKIPPING:  euros\n",
      "SKIPPING:  euros\n",
      "SKIPPING:  five-year\n",
      "SKIPPING:  ,\n",
      "SKIPPING:  still-troubled\n",
      "SKIPPING:  still-troubled\n",
      "SKIPPING:  Baset\n",
      "SKIPPING:  al-Megrahi\n",
      "SKIPPING:  Al-Megrahi\n",
      "SKIPPING:  al-Megrahi\n",
      "SKIPPING:  al-Megrahi\n",
      "SKIPPING:  al-Megrahi\n",
      "SKIPPING:  cancer-related\n",
      "SKIPPING:  cancer-related\n",
      "SKIPPING:  shoal\n",
      "SKIPPING:  north-western\n",
      "SKIPPING:  Zambales\n",
      "SKIPPING:  shoal\n",
      "SKIPPING:  shoal\n",
      "SKIPPING:  Spratly\n",
      "SKIPPING:  Spratly\n",
      "SKIPPING:  BRP\n",
      "SKIPPING:  Bankia\n",
      "SKIPPING:  Guindos\n",
      "SKIPPING:  euros\n",
      "SKIPPING:  Guindos\n",
      "SKIPPING:  n't\n",
      "SKIPPING:  Barack\n",
      "SKIPPING:  one-year\n",
      "SKIPPING:  Karzai\n",
      "SKIPPING:  burqas\n",
      "SKIPPING:  GMT\n",
      "SKIPPING:  Kargar\n",
      "SKIPPING:  Noorughli\n",
      "SKIPPING:  Kargar\n",
      "SKIPPING:  Noorughli\n",
      "SKIPPING:  AFP\n",
      "SKIPPING:  Zabiullah\n",
      "SKIPPING:  Mujahid\n",
      "SKIPPING:  U.S.-backed\n",
      "SKIPPING:  Zabihullah\n",
      "SKIPPING:  Mujahid\n",
      "SKIPPING:  Haqqani\n",
      "SKIPPING:  Waziristan\n",
      "SKIPPING:  LCC\n",
      "SKIPPING:  Al-Burnhanieh\n",
      "SKIPPING:  Syian\n",
      "SKIPPING:  n't\n",
      "SKIPPING:  al-Tebbaneh\n",
      "SKIPPING:  Alawite\n",
      "SKIPPING:  Jabal\n",
      "SKIPPING:  Mohsen\n",
      "SKIPPING:  al-Assad\n",
      "SKIPPING:  Chadi\n",
      "SKIPPING:  Mawlawi\n",
      "SKIPPING:  Al-Qaeda-inspired\n",
      "SKIPPING:  Hajar\n",
      "SKIPPING:  Mawlawi\n",
      "SKIPPING:  Mawlawi\n",
      "SKIPPING:  US-Afghanistan\n",
      "SKIPPING:  II\n",
      "SKIPPING:  Merseyside\n",
      "SKIPPING:  Bidston\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  GOOG\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  MMI\n",
      "SKIPPING:  ,\n",
      "SKIPPING:  ”\n",
      "SKIPPING:  e-mailed\n",
      "SKIPPING:  co-founder\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  iPhone\n",
      "SKIPPING:  smartphone\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  Hamza\n",
      "SKIPPING:  Skype\n",
      "SKIPPING:  Barack\n",
      "SKIPPING:  Gurkha\n",
      "SKIPPING:  Gurkha\n",
      "SKIPPING:  passers-by\n",
      "SKIPPING:  Karzai\n",
      "SKIPPING:  clamp-down\n",
      "SKIPPING:  clamp-down\n",
      "SKIPPING:  clamp-down\n",
      "SKIPPING:  GMT\n",
      "SKIPPING:  Rastan\n",
      "SKIPPING:  Rastan\n",
      "SKIPPING:  Rastan\n",
      "SKIPPING:  FSA\n",
      "SKIPPING:  FSA\n",
      "SKIPPING:  FSA\n",
      "SKIPPING:  British-based\n",
      "SKIPPING:  British-based\n",
      "SKIPPING:  six-hour\n",
      "SKIPPING:  incubated\n",
      "SKIPPING:  Qaeda\n",
      "SKIPPING:  Qaeda\n",
      "SKIPPING:  Qaeda\n",
      "SKIPPING:  Qaeda\n",
      "SKIPPING:  Barack\n",
      "SKIPPING:  Barack\n",
      "SKIPPING:  burqas\n",
      "SKIPPING:  burqas\n",
      "SKIPPING:  Spratly\n",
      "SKIPPING:  Spratly\n",
      "SKIPPING:  face-off\n",
      "SKIPPING:  BRP\n",
      "SKIPPING:  GMA\n",
      "SKIPPING:  Shoal\n",
      "SKIPPING:  Panatag\n",
      "SKIPPING:  Keqing\n",
      "SKIPPING:  Huangyan\n",
      "SKIPPING:  Olli\n",
      "SKIPPING:  Olli\n",
      "SKIPPING:  EU\n",
      "SKIPPING:  euros\n",
      "SKIPPING:  after-effects\n",
      "SKIPPING:  Jazeera\n",
      "SKIPPING:  eurozone\n",
      "SKIPPING:  eurozone\n",
      "SKIPPING:  EU\n",
      "SKIPPING:  eurozone\n",
      "SKIPPING:  eurozone\n",
      "SKIPPING:  EU\n",
      "SKIPPING:  Kremlin-dominated\n",
      "SKIPPING:  Kremlin-dominated\n",
      "SKIPPING:  Facebook\n",
      "SKIPPING:  Mariya\n",
      "SKIPPING:  Boyarintseva\n",
      "SKIPPING:  Boyarintseva\n",
      "SKIPPING:  n't\n",
      "SKIPPING:  Anti-Putin\n",
      "SKIPPING:  Anti-Putin\n",
      "SKIPPING:  scandal-hit\n",
      "SKIPPING:  scandal-hit\n",
      "SKIPPING:  Putin\n",
      "SKIPPING:  traumatised\n",
      "SKIPPING:  traumatised\n",
      "SKIPPING:  ambassadoe\n",
      "SKIPPING:  ambassadoe\n",
      "SKIPPING:  re-election\n",
      "SKIPPING:  re-election\n",
      "SKIPPING:  re-election\n",
      "SKIPPING:  've\n",
      "SKIPPING:  al-Qaida\n",
      "SKIPPING:  U.S.\n",
      "SKIPPING:  al-Qaeda\n",
      "SKIPPING:  ’\n",
      "SKIPPING:  Barack\n",
      "SKIPPING:  Bagram\n",
      "SKIPPING:  Bagram\n",
      "SKIPPING:  Jaxs\n",
      "SKIPPING:  Jazlin\n",
      "SKIPPING:  Jaxs\n",
      "SKIPPING:  Jaxs\n",
      "SKIPPING:  stand-off\n",
      "SKIPPING:  Keqing\n",
      "SKIPPING:  Shoal\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  AFP\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  smartphone\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  anti-competitive\n",
      "SKIPPING:  anti-competitive\n",
      "SKIPPING:  Fla\n",
      "SKIPPING:  WFTV\n",
      "SKIPPING:  Jazlin\n",
      "SKIPPING:  Jaxs\n",
      "SKIPPING:  self-inflicted\n",
      "SKIPPING:  WFTV\n",
      "SKIPPING:  Fla\n",
      "SKIPPING:  WKMG-TV\n",
      "SKIPPING:  euros\n",
      "SKIPPING:  Guindos\n",
      "SKIPPING:  Bankia\n",
      "SKIPPING:  real-estate\n",
      "SKIPPING:  real-estate\n",
      "SKIPPING:  long-awaited\n",
      "SKIPPING:  long-awaited\n",
      "SKIPPING:  Bagram\n",
      "SKIPPING:  self-interest\n",
      "SKIPPING:  AFP\n",
      "SKIPPING:  eurozone\n",
      "SKIPPING:  EU\n",
      "SKIPPING:  EU\n",
      "SKIPPING:  eurozone\n",
      "SKIPPING:  EU\n",
      "SKIPPING:  Olli\n",
      "SKIPPING:  Olli\n",
      "SKIPPING:  AP\n",
      "SKIPPING:  Barack\n",
      "SKIPPING:  Levanon\n",
      "SKIPPING:  Levanon\n",
      "SKIPPING:  EU\n",
      "SKIPPING:  EU\n",
      "SKIPPING:  EU\n",
      "SKIPPING:  Syria-linked\n",
      "SKIPPING:  Syria-linked\n",
      "SKIPPING:  Gunbattles\n",
      "SKIPPING:  al-Assad\n",
      "SKIPPING:  EU\n",
      "SKIPPING:  EU\n",
      "SKIPPING:  Rastan\n",
      "SKIPPING:  Rastan\n",
      "SKIPPING:  Deir\n",
      "SKIPPING:  Ezzor\n",
      "SKIPPING:  n't\n",
      "SKIPPING:  n't\n",
      "SKIPPING:  Waziri\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  TechCrunch\n",
      "SKIPPING:  full-time\n",
      "SKIPPING:  Google\n",
      "SKIPPING:  Google-Motorola\n",
      "SKIPPING:  Ballyfermot\n",
      "SKIPPING:  Cloverhill\n",
      "SKIPPING:  life-threatening\n",
      "SKIPPING:  Tallaght\n",
      "SKIPPING:  Gardaí\n",
      "SKIPPING:  Ballyfermot\n",
      "SKIPPING:  n't\n",
      "SKIPPING:  Bellaghy\n",
      "SKIPPING:  Rastan\n",
      "SKIPPING:  EU\n",
      "SKIPPING:  EU\n",
      "SKIPPING:  UN-backed\n",
      "SKIPPING:  UN-backed\n",
      "SKIPPING:  al-Assad\n",
      "SKIPPING:  destabilises\n",
      "SKIPPING:  destabilises\n",
      "SKIPPING:  falsifications\n",
      "SKIPPING:  falsifications\n",
      "SKIPPING:  falsifications\n",
      "SKIPPING:  Putin\n",
      "SKIPPING:  Putin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKIPPING:  pro-Putin\n",
      "SKIPPING:  pro-Putin\n",
      "SKIPPING:  Ex-Soviet\n",
      "SKIPPING:  part-nationalise\n",
      "SKIPPING:  part-nationalise\n",
      "SKIPPING:  Bankia\n",
      "SKIPPING:  Bankia\n",
      "SKIPPING:  fourth-largest\n",
      "SKIPPING:  Jaxs\n",
      "SKIPPING:  n't\n",
      "SKIPPING:  Rockledge\n",
      "SKIPPING:  Fla\n",
      "SKIPPING:  n't\n",
      "SKIPPING:  Guindos\n",
      "SKIPPING:  Jazeera\n",
      "SKIPPING:  euros\n",
      "SKIPPING:  decade-long\n",
      "SKIPPING:  decade-long\n",
      "SKIPPING:  eurozone\n",
      "SKIPPING:  eurozone\n",
      "SKIPPING:  euros\n",
      "SKIPPING:  clean-ups\n",
      "SKIPPING:  war-zone\n",
      "SKIPPING:  war-zone\n",
      "SKIPPING:  campaign-related\n",
      "[[0.         0.         0.46783728 0.         0.88381462 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]]\n",
      "(11201, 15)\n",
      "(2801, 15)\n"
     ]
    }
   ],
   "source": [
    "def get_phon(df, data, testing=False):\n",
    "    arpabet = nltk.corpus.cmudict.dict()\n",
    "    phones = []\n",
    "    for idx, sub in enumerate(df[\"sub\"].values):\n",
    "        phs = []\n",
    "        for word in nltk.word_tokenize(sub):\n",
    "            p = arpabet.get(word.lower())\n",
    "            if not p:\n",
    "                print(\"SKIPPING: \", word)\n",
    "                continue\n",
    "            phs.extend(p[0])\n",
    "        phones.append(\" \".join(phs))\n",
    "    \n",
    "    if not testing:\n",
    "        tfidf = TfidfVectorizer(max_features=15).fit(phones)\n",
    "        data['phon'] = {\n",
    "            'tfidf': tfidf   \n",
    "        }\n",
    "    \n",
    "    tfidf = data['phon']['tfidf']\n",
    "    X_phon = tfidf.transform(phones).todense()\n",
    "    return X_phon\n",
    "        \n",
    "X_phon_train = get_phon(df_train, DATA)\n",
    "X_phon_val = get_phon(df_val, DATA, testing=True)\n",
    "print(X_phon_train[5])\n",
    "print(X_phon_train.shape)\n",
    "print(X_phon_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping word: St\n",
      "attempt to hold an unsanctioned rally in St.\n",
      "Skipping word: St\n",
      "of Russians who rallied in Moscow and St.\n",
      "Skipping word: a.m\n",
      "murders happened around 4:30 a.m.\n",
      "Skipping word: Mr\n",
      "former President Bill Clinton touted Mr.\n",
      "Skipping word: a.m\n",
      "neighbor heard shots at 4:50 a.m.\n",
      "Skipping word: a.m\n",
      "a text from Thomas at about 3 a.m.\n",
      "Skipping word: St\n",
      "of Russians have rallied in Moscow and St.\n",
      "Skipping word: Co\n",
      "companies such as Samsung Electronics Co.\n",
      "Skipping word: Mr\n",
      "arriving, about midnight local time, Mr.\n",
      "Skipping word: St\n",
      "of Russians rallied in Moscow and St.\n",
      "Skipping word: a.m\n",
      "home to mostly foreigners at about 6:15 a.m.\n",
      "Skipping word: a.m\n",
      "been hitting the town since three a.m.\n",
      "Skipping word: said\n",
      "that killed Osama bin Laden,'' he said.\n",
      "Skipping word: .\n",
      "that killed Osama bin Laden,'' he said.\n",
      "Skipping word: said\n",
      "killed Osama bin Laden,'' he said.\n",
      "Skipping word: .\n",
      "killed Osama bin Laden,'' he said.\n",
      "Skipping word: Ms\n",
      "reported having heard gun shots from Ms.\n",
      "Skipping word: Ms\n",
      "having heard gun shots from Ms.\n",
      "Skipping word: Ms\n",
      "she opened the door, three of Ms.\n",
      "Skipping word: a.m\n",
      "message to her neighbor around 3 a.m.\n",
      "[-6.03027344e-02  1.09863281e-01 -2.37792969e-01  1.40563965e-01\n",
      "  4.36172485e-02 -1.46850586e-01 -2.09228516e-01 -2.19238281e-01\n",
      " -4.39453125e-02  3.95019531e-01 -4.00390625e-02 -1.77978516e-01\n",
      "  5.56640625e-02 -4.00390625e-02 -4.98046875e-02  5.18188477e-02\n",
      " -3.39843750e-01  3.44482422e-01 -2.87597656e-01 -1.07421875e-02\n",
      " -4.06738281e-01 -2.03613281e-01  1.58691406e-03 -1.87744141e-01\n",
      "  5.61523438e-03 -2.08496094e-01 -2.06481934e-01 -7.66601562e-02\n",
      "  3.80859375e-02 -3.01757812e-01 -1.22558594e-01  1.27197266e-01\n",
      "  1.15234375e-01 -1.64062500e-01 -1.88476562e-01 -1.89453125e-01\n",
      "  1.38732910e-01 -8.53881836e-02  3.90625000e-02 -1.30737305e-01\n",
      " -1.02050781e-01  1.18896484e-01  6.49414062e-02  1.63574219e-01\n",
      "  8.88671875e-02 -3.39843750e-01 -1.59790039e-01 -2.16796875e-01\n",
      "  2.77099609e-02 -1.04980469e-01  1.36230469e-01 -2.87597656e-01\n",
      "  2.46582031e-01  1.15722656e-01  1.47460938e-01  3.29101562e-01\n",
      " -6.78710938e-02 -2.85644531e-02 -5.44433594e-02  5.73730469e-03\n",
      " -2.91503906e-01 -2.02148438e-01  1.70410156e-01 -1.86523438e-01\n",
      " -1.74621582e-01  4.46472168e-02 -1.79687500e-01 -1.78466797e-01\n",
      "  2.67578125e-01 -5.10253906e-02 -2.55859375e-01 -2.76367188e-01\n",
      "  1.40014648e-01 -2.63183594e-01 -3.51074219e-01 -2.28027344e-01\n",
      "  1.16699219e-01 -1.90185547e-01 -1.18347168e-01 -1.85546875e-02\n",
      "  5.49316406e-02  4.07714844e-02 -1.52832031e-01 -4.25781250e-01\n",
      "  9.93652344e-02  1.43737793e-02  1.38183594e-01 -4.78515625e-02\n",
      " -6.59179688e-02  1.07910156e-01 -1.79443359e-01 -2.09472656e-01\n",
      " -2.31933594e-01  3.72070312e-01  2.11423874e-01  2.54364014e-02\n",
      "  2.39746094e-01 -1.29150391e-01  2.86132812e-01 -1.83227539e-01\n",
      " -8.12988281e-02 -1.90185547e-01 -1.42089844e-01 -2.22167969e-02\n",
      " -1.96777344e-01 -1.78710938e-01  2.13134766e-01  8.89892578e-02\n",
      "  6.05468750e-02  1.76269531e-01 -4.35058594e-01 -1.67907715e-01\n",
      "  3.81347656e-01  5.76171875e-02  2.35595703e-01  1.66015625e-02\n",
      "  2.29248047e-01 -1.73095703e-01  6.87255859e-02  5.85937500e-03\n",
      " -1.80175781e-01  2.49511719e-01  1.23962402e-01  1.43554688e-01\n",
      "  6.36444092e-02 -2.57453918e-02  1.07421875e-02  1.49169922e-01\n",
      " -2.53906250e-01 -2.34375000e-02 -2.29492188e-01 -8.59375000e-02\n",
      "  1.19262695e-01  4.27734375e-01 -1.71386719e-01 -5.49926758e-02\n",
      " -1.84402466e-01 -3.14819336e-01  5.34667969e-02  5.39672852e-01\n",
      " -1.81396484e-01 -2.56835938e-01  5.72509766e-02  4.39453125e-01\n",
      "  2.13378906e-01  1.50146484e-01  4.88281250e-03  2.43652344e-01\n",
      " -3.12500000e-01 -1.76269531e-01  4.10156250e-02 -2.57324219e-01\n",
      " -3.85742188e-01  1.16210938e-01  2.37792969e-01  1.55273438e-01\n",
      "  8.20617676e-02 -2.99804688e-01  1.10107422e-01  2.81738281e-01\n",
      "  2.82226562e-01  3.25683594e-01  3.02246094e-01  1.33056641e-01\n",
      " -1.59423828e-01 -7.93457031e-02 -1.46484375e-02 -2.08007812e-01\n",
      " -2.64160156e-01  1.48925781e-01  1.52587891e-01  3.29101562e-01\n",
      "  5.71289062e-02  2.70019531e-01 -8.74938965e-02  1.35742188e-01\n",
      "  2.94799805e-02 -1.83105469e-02  6.04248047e-02 -4.12597656e-02\n",
      " -3.75976562e-01 -1.97830200e-01 -9.64355469e-02  1.11633301e-01\n",
      " -7.54547119e-02  1.18568420e-01  4.65087891e-02  3.10058594e-02\n",
      "  9.83886719e-02 -1.22802734e-01 -9.25292969e-02 -3.09570312e-01\n",
      " -1.37443542e-01  1.09130859e-01  3.89404297e-02  3.25195312e-01\n",
      "  5.14648438e-01 -9.26513672e-02  2.84667969e-01 -1.66503906e-01\n",
      "  2.62695312e-01 -9.83886719e-02 -2.84179688e-01 -5.72509766e-02\n",
      "  7.16552734e-02  2.81494141e-01  1.63574219e-01  3.07617188e-02\n",
      " -7.52563477e-02 -4.99267578e-02 -1.65222168e-01  1.07421875e-02\n",
      " -1.03637695e-01  1.53808594e-01  1.04003906e-01  2.41699219e-01\n",
      "  4.24919128e-02 -1.95800781e-01  2.85156250e-01  5.85937500e-02\n",
      "  2.75878906e-01  7.19528198e-02 -8.36181641e-02  2.91748047e-01\n",
      " -9.99755859e-02  1.18164062e-01  5.87615967e-02  2.06542969e-01\n",
      "  3.32031250e-01  7.58666992e-02 -3.29589844e-02 -1.23535156e-01\n",
      " -1.55090332e-01 -2.00195312e-02 -1.71142578e-01 -5.76171875e-02\n",
      "  9.84649658e-02  1.74316406e-01  3.88671875e-01  2.08007812e-01\n",
      "  2.49511719e-01 -3.03710938e-01  3.02734375e-02 -1.67724609e-01\n",
      " -2.20214844e-01  9.39941406e-02 -7.22656250e-02  1.61132812e-01\n",
      " -1.07421875e-02  8.76464844e-02 -8.03222656e-02 -3.37890625e-01\n",
      "  2.92968750e-02 -2.44140625e-03  1.23046875e-01 -3.57421875e-01\n",
      " -7.56835938e-03 -7.83691406e-02  1.18164062e-01  1.65039062e-01\n",
      "  2.49023438e-02 -1.67480469e-01 -1.56005859e-01 -1.36718750e-02\n",
      "  1.81640625e-01  4.19433594e-01 -3.13964844e-01  6.83593750e-03\n",
      "  2.98828125e-01 -3.42285156e-01 -5.27343750e-01  3.26171875e-01\n",
      "  7.91015625e-02  1.37390137e-01  6.11572266e-02  3.83789062e-01\n",
      "  1.36230469e-01 -2.33825684e-01  1.47705078e-01  1.01806641e-01\n",
      " -9.32617188e-02 -1.45751953e-01  4.27734375e-01 -8.94775391e-02\n",
      " -4.88281250e-04 -5.33203125e-01  8.32519531e-02  2.14843750e-02\n",
      "  2.85034180e-02  2.72949219e-01 -4.39453125e-02  2.29873657e-02\n",
      " -1.00097656e-02 -9.48486328e-02  4.02832031e-02  7.98339844e-02\n",
      " -1.36718750e-01 -1.45996094e-01  4.85351562e-01  4.15039062e-02\n",
      " -2.74729359e-03  2.81953479e-03 -2.26798567e-02  9.05494715e-03\n",
      "  7.54644753e-04 -1.05519110e-02 -2.97040415e-02 -1.61311216e-02\n",
      "  1.61838272e-02  5.39127393e-02  1.27143233e-03 -3.93492473e-02\n",
      " -1.01268311e-03 -9.24189281e-03 -8.26979568e-03  1.82339815e-02\n",
      " -4.01253438e-02  4.82588678e-02 -2.66584178e-02  2.94531808e-02\n",
      " -2.76054596e-02 -2.84848089e-02  1.90648432e-02 -1.90637100e-02\n",
      "  1.95165587e-02 -3.87613506e-02 -4.43752735e-02 -1.22722311e-02\n",
      "  1.78796981e-02 -3.68101617e-02 -1.44856731e-02  2.00336070e-02\n",
      " -5.62529332e-03 -4.87730117e-03  7.87757410e-03 -2.61756878e-02\n",
      "  4.26935035e-03 -1.38244048e-02  2.65357204e-03 -8.64800868e-03\n",
      "  2.25438869e-04  3.80823892e-02  9.18934995e-03  1.76373640e-02\n",
      "  1.82548677e-02 -5.10190877e-02 -7.20265482e-03 -2.42204944e-02\n",
      " -1.93558384e-03 -1.09085972e-02  2.67159487e-02 -3.20274916e-02\n",
      "  3.21531188e-02  3.20958769e-03  1.36786959e-02  3.70128084e-02\n",
      " -1.79643737e-02 -2.12416247e-02 -1.89771965e-03 -1.00474145e-02\n",
      " -3.25426514e-02 -1.68835611e-02  1.35883468e-02 -2.21022908e-02\n",
      " -4.88393029e-03  1.03968471e-02 -2.42670080e-02  4.46927202e-04\n",
      "  1.30945499e-02 -1.34351793e-02 -3.22207517e-02 -1.80322577e-02\n",
      "  2.86887549e-02 -1.90960175e-02 -5.52642620e-02 -4.16409424e-02\n",
      " -4.75786129e-04 -1.21985895e-02 -6.63727971e-03 -1.17446416e-02\n",
      "  1.02684485e-02 -2.07488831e-03 -1.87778232e-02 -4.60926362e-02\n",
      "  3.66965907e-03  5.16776049e-03  2.34904749e-03 -5.60540912e-03\n",
      "  1.25466640e-02  4.18071505e-03 -3.61466866e-03 -1.96483498e-02\n",
      " -2.36442259e-02  2.79232971e-02  1.90706726e-02  8.15405196e-03\n",
      "  2.21592502e-02 -2.27585232e-02  4.20723949e-02 -1.91256504e-02\n",
      "  9.81871371e-03 -6.80514784e-03  9.42650517e-04  1.07866057e-02\n",
      " -1.62899459e-02 -1.44416320e-02  2.00783691e-02  2.55630398e-03\n",
      "  2.04073254e-02  2.64651638e-02 -4.72689695e-02 -1.30582090e-02\n",
      "  5.00402899e-02  1.53125474e-02  1.78807657e-02 -1.16807790e-02\n",
      "  3.01320347e-02 -2.51735639e-02 -1.05996151e-03  6.96417536e-03\n",
      " -3.95030242e-02  2.81029300e-02  1.09605161e-02  2.98510379e-02\n",
      " -5.17767185e-03 -5.08927142e-03 -1.42386133e-03  1.22602793e-02\n",
      " -2.91104450e-02  6.85101456e-03 -2.85634292e-03 -1.46788352e-02\n",
      "  1.21941123e-02  3.96601204e-02 -1.23875663e-02 -1.30836648e-02\n",
      " -8.77583377e-03 -3.03763508e-02  1.25346130e-02  6.98553249e-02\n",
      " -2.70062215e-02 -2.86462291e-02  1.64915623e-02  5.06701977e-02\n",
      "  2.37137028e-02  2.14709355e-02 -3.56237537e-03  2.89392570e-02\n",
      " -2.95100700e-02 -1.30174980e-02  1.94691177e-02 -1.30698695e-02\n",
      " -5.91046084e-02  1.61025679e-02  2.33860167e-02  1.56429088e-02\n",
      " -1.66278499e-04 -4.52912856e-02  2.01945658e-02  3.13455224e-02\n",
      "  2.57062831e-02  4.39938855e-02  3.28135640e-02  1.00103461e-02\n",
      " -1.85241462e-02 -1.31436904e-02 -1.89641344e-02 -2.24651971e-02\n",
      " -2.53089220e-02  1.91979514e-02  4.63039095e-03  3.95773514e-02\n",
      "  7.55457790e-03  1.24335575e-02 -1.94785097e-02  5.90658374e-03\n",
      "  2.01134580e-03  1.82115586e-03 -1.22390819e-02  6.19134407e-03\n",
      " -4.70172126e-02 -3.82459523e-02 -1.82782895e-02  1.57880538e-02\n",
      " -1.01911170e-02 -7.26968677e-03 -2.94350836e-03  1.97653818e-03\n",
      "  4.59235338e-02 -5.21550294e-03 -1.83158933e-02 -3.60498299e-02\n",
      " -1.96957591e-02  6.60507226e-03 -1.03823513e-02  4.25333193e-02\n",
      "  5.66268653e-02 -4.19310524e-03  1.87409255e-02 -4.53202186e-02\n",
      "  4.55391470e-02 -1.57622266e-02 -4.50915525e-02 -7.74560970e-03\n",
      " -1.32832302e-02  3.25089634e-02  2.24289546e-02 -6.80948265e-03\n",
      " -1.12781363e-02 -5.11417691e-03 -1.41955136e-02 -2.43349001e-03\n",
      " -1.28888104e-02  3.05198461e-02  3.31221042e-03  1.24668316e-02\n",
      "  2.06691610e-02 -2.40422819e-02  3.49607339e-02  9.66551769e-03\n",
      "  4.37038203e-02 -1.39730367e-03 -2.27117709e-02  7.90271735e-03\n",
      " -9.28606213e-03  7.61522413e-04  1.48162819e-02  4.38474061e-02\n",
      "  3.21051983e-02  2.01824223e-02 -6.31735811e-03 -2.65678152e-02\n",
      " -9.69183457e-03 -1.41348015e-02  9.16033438e-03 -5.35943900e-03\n",
      "  1.58359499e-02  1.26896840e-02  4.65829833e-02  1.01656759e-02\n",
      "  3.35991873e-02 -2.63000964e-02  8.02123976e-03 -2.39412014e-02\n",
      " -1.74975372e-02  3.83871933e-03  1.07651340e-03  2.19720019e-02\n",
      "  1.80718372e-02  1.46670961e-02 -2.91179129e-02 -4.21577173e-02\n",
      "  8.54677448e-03  1.98332405e-02  7.00970546e-03 -5.19649940e-02\n",
      "  1.20314764e-02 -6.70415945e-03  1.00602135e-03  1.55482649e-02\n",
      "  1.96886158e-02 -2.95401556e-02 -1.45194126e-02  7.74345627e-03\n",
      "  7.33029646e-03  5.28648199e-02 -4.24008262e-02 -8.30969374e-03\n",
      "  2.67095824e-02 -4.16550405e-02 -5.39921653e-02  3.76028806e-02\n",
      "  2.05727041e-03  3.13212732e-02  6.09416751e-03  4.33898662e-02\n",
      " -9.37810881e-03 -2.88868833e-02  8.55607513e-03  3.47941462e-03\n",
      " -1.87285580e-02 -2.65673151e-02  5.17414735e-02 -2.17101415e-02\n",
      "  1.59216278e-02 -7.31862017e-02 -5.24113623e-03  1.28223727e-03\n",
      "  5.82893612e-03  3.37256458e-02 -6.14405018e-03 -6.69505243e-03\n",
      " -1.44348558e-02 -5.86858252e-03  5.44699511e-03  1.25020369e-02\n",
      " -3.47182742e-02 -2.77290761e-02  8.28526539e-02  3.05799847e-03]\n",
      "(11201, 600)\n",
      "(2801, 600)\n"
     ]
    }
   ],
   "source": [
    "def get_emb(df, data, window_size=20, use_tfidf=True, testing=False):\n",
    "    def preprocess_text(text):\n",
    "        return nltk.word_tokenize(text)\n",
    "    \n",
    "    if not testing:\n",
    "        dataset = [preprocess_text(text) \n",
    "                   for text in np.unique(df['text'])]\n",
    "        dct = gensim.corpora.Dictionary(dataset)\n",
    "        corpus = [dct.doc2bow(line) for line in dataset]\n",
    "        tfidf = gensim.models.TfidfModel(corpus) \n",
    "        data['emb'] = {\n",
    "            \"tfidf\": tfidf,\n",
    "            \"dct\": dct,\n",
    "        }\n",
    "        \n",
    "    X_emb = np.zeros((len(df), 600))\n",
    "    for idx in range(len(df)):\n",
    "        sub = df['sub'].values[idx]\n",
    "        text = df['text'].values[idx]\n",
    "        \n",
    "        l, r = df['l'].values[idx], df['r'].values[idx]\n",
    "        text_window = text[max(0, l - window_size):(r + window_size)]\n",
    "        text_window = text_window.split(' ', 1)[1]\n",
    "        text_window = text_window.rsplit(' ', 1)[0]\n",
    "        \n",
    "        now_emb = np.zeros((300,))\n",
    "        cnt = 0\n",
    "        \n",
    "        tfidf = data['emb']['tfidf']\n",
    "        dct = data['emb']['dct']\n",
    "        \n",
    "        for token in nltk.word_tokenize(sub):\n",
    "            try:\n",
    "                emb = w2v_model[token]\n",
    "                now_emb += emb\n",
    "                cnt += 1\n",
    "            except KeyError:\n",
    "                #print(f\"{token} not found\")\n",
    "                #print(text_window)\n",
    "                pass\n",
    "        if cnt > 0:\n",
    "            now_emb /= cnt\n",
    "            \n",
    "        X_emb[idx, :300] = now_emb\n",
    "        \n",
    "        now_emb = np.zeros((300,))\n",
    "        cnt = 0\n",
    "        \n",
    "        text_coefs = dict(tfidf[dct.doc2bow(preprocess_text(text))])\n",
    "        for token in nltk.word_tokenize(text_window):\n",
    "#             if token in sub:\n",
    "#                 continue     \n",
    "            if not use_tfidf:\n",
    "                coef = 1.\n",
    "            else:\n",
    "                if token in dct.token2id:\n",
    "                    token_id = dct.token2id[token]\n",
    "                    if token_id not in text_coefs:\n",
    "                        print(f\"Skipping word: {token}\")\n",
    "                        print(text_window)\n",
    "                        continue\n",
    "                    coef = text_coefs[token_id]\n",
    "                else:\n",
    "                    continue\n",
    "            try:\n",
    "                emb = w2v_model[token]\n",
    "                now_emb += emb * coef\n",
    "                cnt += 1\n",
    "            except KeyError:\n",
    "                pass\n",
    "        if cnt > 0:\n",
    "            now_emb /= cnt\n",
    "        \n",
    "        X_emb[idx, 300:] = now_emb\n",
    "    return X_emb\n",
    "\n",
    "X_emb_train = get_emb(df_train, DATA)\n",
    "X_emb_val = get_emb(df_val, DATA, testing=True)\n",
    "print(X_emb_train[5])\n",
    "print(X_emb_train.shape)\n",
    "print(X_emb_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5.68464175e-03  3.65903864e-04  5.14053923e-04  4.06450197e-03\n",
      "  5.85791469e-03  1.49643251e-02 -2.44619311e-03  3.20479226e-03\n",
      "  3.19487528e-03 -7.64507253e-04  2.00284016e-03  4.66683134e-03\n",
      " -2.68313051e-03  3.13842289e-02  9.90042090e-03  3.33393338e-03\n",
      "  3.83956314e-02 -2.73281188e-02 -9.78660702e-03 -3.86733837e-03\n",
      "  5.18673064e-02  4.14641186e-02  1.18302935e-02  5.55241006e-03\n",
      " -5.73246729e-03  1.22192292e-02 -2.90223010e-03  3.13566649e-03\n",
      "  8.55245669e-04  6.99988262e-04  3.89572345e-04 -5.45310874e-03\n",
      " -1.26023194e-03  4.56785985e-03  6.79204436e-04 -2.66888028e-03\n",
      " -8.34357879e-04 -2.14457753e-03 -4.56859748e-03  5.01982281e-04\n",
      "  2.88203247e-03  7.20423536e-03 -4.64491386e-03 -2.39016018e-03\n",
      " -2.08460759e-03  9.83078783e-04  2.11373650e-03 -6.66563613e-04\n",
      "  5.96244790e-03 -4.46345847e-03 -5.22437238e-03  4.89143089e-03\n",
      " -4.96537609e-03  1.20619810e-02  5.79279757e-04 -8.08210633e-03\n",
      "  1.79185149e-03  2.44107417e-03 -9.11098441e-04  2.03612901e-03\n",
      "  2.98630768e-03 -7.23641581e-03 -9.85554919e-03  5.87508838e-04\n",
      "  2.54511151e-04 -2.87131707e-03  4.62641805e-04  6.13753437e-03\n",
      " -4.83997610e-03  2.49449379e-03  5.80308163e-05  6.38556967e-03\n",
      " -2.84138395e-03  1.22096840e-04  1.80453523e-03 -1.50371673e-03\n",
      "  1.76372612e-03  1.91904755e-03 -3.03963178e-03 -2.94511643e-03\n",
      " -9.38479013e-03  3.71844459e-04  6.81276093e-03 -2.13700952e-04\n",
      "  3.46639911e-03 -1.36355883e-02 -2.72282236e-03 -3.92732676e-03\n",
      "  2.07593136e-04  1.02414294e-03 -2.35355064e-03 -7.11485037e-04\n",
      " -2.42269938e-03  8.18000937e-03  5.04734427e-03 -1.70977768e-02\n",
      "  7.08564778e-05 -5.12091023e-04  9.28429797e-03 -7.69105106e-04\n",
      "  1.72143267e-03  1.30711116e-03 -7.40681681e-04 -4.10036059e-04\n",
      " -8.12615749e-03  1.66559482e-03 -4.65690397e-03  9.38544255e-04\n",
      " -1.87847490e-04 -3.49973297e-03  1.27470291e-03  1.12341272e-02\n",
      "  5.87926719e-03 -1.14883065e-02  5.16328686e-03 -2.60828601e-03\n",
      "  1.98985423e-02 -1.35522871e-02  1.95350014e-02  3.34892295e-03\n",
      " -4.03902156e-03 -1.00727490e-02  5.06958753e-03 -3.17363348e-03\n",
      " -1.04644232e-03 -1.75233236e-02  1.36849053e-03 -4.69141245e-03\n",
      " -1.01236751e-03  3.35654277e-04  8.20447564e-03  4.70153104e-03\n",
      "  1.78921003e-03  1.67543543e-02  1.12295756e-02  2.84853049e-03\n",
      "  1.69740283e-02 -2.26585157e-02 -1.48596299e-02  2.79694744e-02\n",
      "  8.21491542e-03  2.47841055e-02  6.91095047e-03 -3.22228417e-02\n",
      " -8.22335988e-03  1.81141834e-02 -1.73082296e-02 -1.06975350e-02\n",
      " -5.93345209e-04  1.91874851e-02 -1.47975252e-02  2.35531379e-02\n",
      "  1.18791166e-02  4.88065743e-02  2.85559460e-02  1.23794324e-02\n",
      " -1.51451849e-02  1.79495341e-02 -1.58840497e-02 -5.98499435e-03\n",
      "  1.11886292e-02  3.22915454e-04  6.64330529e-03 -8.78347563e-03\n",
      "  2.69154142e-02 -6.53112004e-03 -1.08234263e-02  2.29782400e-04\n",
      " -9.99115671e-03  2.16040376e-02  6.95476034e-03 -1.00464323e-02\n",
      "  2.32530806e-03  2.07187576e-02  9.09767379e-04  7.55697027e-03\n",
      "  1.40899444e-02  9.67540509e-05  3.78185480e-05 -1.23904261e-02\n",
      " -1.45705355e-02 -2.01325656e-02  1.03459014e-02 -3.08154090e-02\n",
      " -2.92093948e-03  3.83854599e-02 -4.76646457e-03  1.17581693e-02\n",
      "  1.25013902e-02 -6.60455511e-03 -1.27953905e-02  2.55484135e-02\n",
      " -5.65376497e-03 -3.72641016e-03  1.09001684e-02 -1.01381012e-02\n",
      "  2.72247579e-03 -4.43152691e-03  8.40966648e-03  2.88585387e-03\n",
      " -1.52045755e-02 -3.13485006e-03 -9.91981751e-03 -5.18495594e-03\n",
      "  6.84552603e-03 -8.02962223e-03 -6.56073236e-03  1.00009658e-02\n",
      "  1.79971758e-02 -9.60288877e-03 -8.17658885e-03  1.57801913e-02\n",
      " -4.52173665e-03  2.96613920e-03 -8.56579900e-03 -8.95395824e-03\n",
      " -1.21786895e-02  1.34525253e-02 -2.33076421e-02 -3.98813343e-03\n",
      "  1.64044292e-02 -3.96381649e-02 -1.71772199e-02  1.32128484e-02\n",
      "  2.56020156e-02  6.34515097e-03 -2.08864451e-03 -2.77094781e-02\n",
      "  2.94486333e-02 -5.99481262e-03  5.96698683e-04  2.84239371e-02\n",
      "  7.42241333e-03 -1.11571491e-02  1.60758317e-02 -7.81933267e-03\n",
      "  1.44341192e-03  7.09343306e-03  2.45575561e-02 -1.23752334e-02\n",
      " -1.41340697e-02  1.11680862e-02 -1.10712704e-02  1.20829500e-02\n",
      " -3.75896722e-03  2.13687134e-02  1.16311338e-02  1.43550060e-02\n",
      "  1.19949525e-02  4.66621757e-03  3.25058521e-02  2.93320384e-02\n",
      " -6.24759337e-02  3.33334073e-02 -7.80173869e-03  2.01833457e-02\n",
      "  3.01462791e-03  1.73790935e-02 -2.71030766e-02 -2.41497217e-02\n",
      " -3.57962887e-02  2.05100130e-02  3.95437896e-02  1.86731313e-02\n",
      "  4.00890316e-02  2.08833364e-02  6.00170103e-03 -1.23204771e-03\n",
      "  3.73817714e-02 -3.25141530e-02  8.51126053e-04  2.25658151e-02\n",
      " -9.73563644e-03  1.22783304e-02  4.83206534e-02 -2.69498801e-02\n",
      "  3.14020071e-02  1.60237151e-02  8.76052817e-03  1.77906209e-02\n",
      "  5.73678278e-03 -2.58645810e-02  2.18489461e-02 -1.27951202e-02\n",
      "  2.54884425e-02 -2.37969042e-02 -1.45958148e-02 -1.56920521e-02\n",
      " -1.10783658e-02 -9.45521984e-03 -2.49574960e-02 -9.60909675e-03\n",
      "  5.93920216e-03  1.48539871e-02 -2.92539656e-02  6.06437063e-03\n",
      "  8.85498358e-03 -2.41124263e-02  1.72970119e-02 -6.85726306e-03]\n",
      "(11201, 300)\n",
      "(2801, 300)\n"
     ]
    }
   ],
   "source": [
    "def get_grams(df, data, testing=False):\n",
    "\n",
    "    def preprocess_sub(sub):\n",
    "        sub = sub.lower()\n",
    "        tokenized = nltk.word_tokenize(sub)\n",
    "        ngrams = []\n",
    "        for token in tokenized:\n",
    "            ngrams.extend(nltk.everygrams(token, 3, 4))\n",
    "        ngrams = \" \".join([\"\".join(ngram) for ngram in ngrams])\n",
    "        return ngrams\n",
    "    \n",
    "    corpus = list(map(preprocess_sub, df[\"sub\"].values))\n",
    "    \n",
    "    if not testing:\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        vectorizer.fit(corpus)\n",
    "        data['grams'] = {\n",
    "            'vectorizer': vectorizer,\n",
    "        }\n",
    "    vectorizer = data['grams']['vectorizer']\n",
    "    \n",
    "    X_grams = vectorizer.transform(corpus)\n",
    "    \n",
    "    if not testing:\n",
    "        pca = TruncatedSVD(n_components=300)\n",
    "        pca.fit(X_grams)\n",
    "        data['grams'][\"pca\"] = pca\n",
    "    pca = data['grams']['pca']\n",
    "    \n",
    "    X_grams = pca.transform(X_grams)\n",
    "    \n",
    "    return X_grams\n",
    "\n",
    "X_grams_train = get_grams(df_train, DATA)\n",
    "X_grams_val = get_grams(df_val, DATA, testing=True)\n",
    "print(X_grams_train[5])\n",
    "print(X_grams_train.shape)\n",
    "print(X_grams_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(X, data, testing=False):\n",
    "    if not testing:\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X)\n",
    "        data[\"scaler\"] = scaler\n",
    "    scaler = data[\"scaler\"]\n",
    "    \n",
    "    return scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def loss(y_true, y_pred):\n",
    "    return np.mean(np.abs(y_true - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abs_loss(y_true, y_pred):\n",
    "    ret = tf.math.abs(y_true - y_pred)\n",
    "    return tf.math.maximum(ret, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11201, 921)\n",
      "(11201,)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.hstack([X_syn_train, X_phon_train, X_emb_train, X_grams_train])\n",
    "y_train = df_train[\"p\"].values\n",
    "X_train = scale(X_train, DATA)\n",
    "\n",
    "X_val = np.hstack([X_syn_val, X_phon_val, X_emb_val, X_grams_val])\n",
    "y_val = df_val[\"p\"].values\n",
    "X_val = scale(X_val, DATA, testing=True)\n",
    "\n",
    "# idx = (y > 0)\n",
    "# X = X[idx]\n",
    "# y = y[idx]\n",
    "# print(idx)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2993874044919776e-17\n",
      "0.9999999999999999\n",
      "-0.0025842353106073435\n",
      "1.0335617417418765\n"
     ]
    }
   ],
   "source": [
    "print(X_train.mean())\n",
    "print(X_train.std())\n",
    "print(X_val.mean())\n",
    "print(X_val.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(input_dim, keep_dim):\n",
    "    inp = tfkl.Input(shape=(input_dim,))\n",
    "    inp_drop = tfkl.Lambda(lambda t: t[:, keep_dim:])(inp)\n",
    "    inp_drop = tfkl.Dropout(0.3)(inp_drop)\n",
    "    inp_keep = tfkl.Lambda(lambda t: t[:, :keep_dim])(inp)\n",
    "    inp_conc = tfkl.Concatenate()([inp_drop, inp_keep])\n",
    "    h = tfkl.Dense(300, activation='relu')(inp_conc)\n",
    "    pred = tfkl.Dense(1)(h)\n",
    "    return tfk.Model(inp, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 916)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 900)          0           input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 900)          0           lambda_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 16)           0           input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 916)          0           dropout_3[0][0]                  \n",
      "                                                                 lambda_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 300)          275100      concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 1)            301         dense_6[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 275,401\n",
      "Trainable params: 275,401\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train on 11201 samples, validate on 2801 samples\n",
      "Epoch 1/1000\n",
      "11201/11201 [==============================] - 3s 301us/sample - loss: 0.6695 - val_loss: 0.2342\n",
      "Epoch 2/1000\n",
      "11201/11201 [==============================] - 3s 228us/sample - loss: 0.1563 - val_loss: 0.1053\n",
      "Epoch 3/1000\n",
      "11201/11201 [==============================] - 3s 224us/sample - loss: 0.0952 - val_loss: 0.1135\n",
      "Epoch 4/1000\n",
      "11201/11201 [==============================] - 2s 221us/sample - loss: 0.0940 - val_loss: 0.0907\n",
      "Epoch 5/1000\n",
      "11201/11201 [==============================] - 2s 213us/sample - loss: 0.0904 - val_loss: 0.0924\n",
      "Epoch 6/1000\n",
      "11201/11201 [==============================] - 2s 207us/sample - loss: 0.0910 - val_loss: 0.0905\n",
      "Epoch 7/1000\n",
      "11201/11201 [==============================] - 2s 206us/sample - loss: 0.0865 - val_loss: 0.0804\n",
      "Epoch 8/1000\n",
      "11201/11201 [==============================] - 2s 201us/sample - loss: 0.0854 - val_loss: 0.0839\n",
      "Epoch 9/1000\n",
      "11201/11201 [==============================] - 2s 205us/sample - loss: 0.0844 - val_loss: 0.0800\n",
      "Epoch 10/1000\n",
      "11201/11201 [==============================] - 2s 203us/sample - loss: 0.0814 - val_loss: 0.0960\n",
      "Epoch 11/1000\n",
      "11201/11201 [==============================] - 2s 197us/sample - loss: 0.0831 - val_loss: 0.0827\n",
      "Epoch 12/1000\n",
      "11201/11201 [==============================] - 2s 201us/sample - loss: 0.0804 - val_loss: 0.0927\n",
      "Epoch 13/1000\n",
      "11201/11201 [==============================] - 2s 200us/sample - loss: 0.0801 - val_loss: 0.0831\n",
      "Epoch 14/1000\n",
      "10976/11201 [============================>.] - ETA: 0s - loss: 0.0816\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.00031622778103685084.\n",
      "11201/11201 [==============================] - 2s 186us/sample - loss: 0.0814 - val_loss: 0.0890\n",
      "Epoch 15/1000\n",
      "11201/11201 [==============================] - 2s 192us/sample - loss: 0.0691 - val_loss: 0.0677\n",
      "Epoch 16/1000\n",
      "11201/11201 [==============================] - 2s 191us/sample - loss: 0.0658 - val_loss: 0.0685\n",
      "Epoch 17/1000\n",
      "11201/11201 [==============================] - 2s 185us/sample - loss: 0.0656 - val_loss: 0.0752\n",
      "Epoch 18/1000\n",
      "11201/11201 [==============================] - 2s 191us/sample - loss: 0.0646 - val_loss: 0.0719\n",
      "Epoch 19/1000\n",
      "11201/11201 [==============================] - 2s 192us/sample - loss: 0.0647 - val_loss: 0.0662\n",
      "Epoch 20/1000\n",
      "11201/11201 [==============================] - 2s 188us/sample - loss: 0.0634 - val_loss: 0.0670\n",
      "Epoch 21/1000\n",
      "11201/11201 [==============================] - 2s 190us/sample - loss: 0.0625 - val_loss: 0.0667\n",
      "Epoch 22/1000\n",
      "11201/11201 [==============================] - 2s 200us/sample - loss: 0.0628 - val_loss: 0.0662\n",
      "Epoch 23/1000\n",
      "11201/11201 [==============================] - 2s 201us/sample - loss: 0.0618 - val_loss: 0.0657\n",
      "Epoch 24/1000\n",
      "11201/11201 [==============================] - 2s 202us/sample - loss: 0.0628 - val_loss: 0.0658\n",
      "Epoch 25/1000\n",
      "11201/11201 [==============================] - 2s 216us/sample - loss: 0.0611 - val_loss: 0.0742\n",
      "Epoch 26/1000\n",
      "11201/11201 [==============================] - 2s 220us/sample - loss: 0.0610 - val_loss: 0.0694\n",
      "Epoch 27/1000\n",
      "11201/11201 [==============================] - 2s 218us/sample - loss: 0.0603 - val_loss: 0.0663\n",
      "Epoch 28/1000\n",
      "11200/11201 [============================>.] - ETA: 0s - loss: 0.0596\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.00010000000639606199.\n",
      "11201/11201 [==============================] - 2s 219us/sample - loss: 0.0596 - val_loss: 0.0671\n",
      "Epoch 29/1000\n",
      "11201/11201 [==============================] - 3s 234us/sample - loss: 0.0553 - val_loss: 0.0631\n",
      "Epoch 30/1000\n",
      "11201/11201 [==============================] - 3s 236us/sample - loss: 0.0547 - val_loss: 0.0623\n",
      "Epoch 31/1000\n",
      "11201/11201 [==============================] - 3s 240us/sample - loss: 0.0538 - val_loss: 0.0642\n",
      "Epoch 32/1000\n",
      "11201/11201 [==============================] - 3s 236us/sample - loss: 0.0537 - val_loss: 0.0624\n",
      "Epoch 33/1000\n",
      "11201/11201 [==============================] - 3s 239us/sample - loss: 0.0536 - val_loss: 0.0625\n",
      "Epoch 34/1000\n",
      "11201/11201 [==============================] - 3s 241us/sample - loss: 0.0541 - val_loss: 0.0622\n",
      "Epoch 35/1000\n",
      "11201/11201 [==============================] - 3s 238us/sample - loss: 0.0531 - val_loss: 0.0648\n",
      "Epoch 36/1000\n",
      "11201/11201 [==============================] - 3s 236us/sample - loss: 0.0524 - val_loss: 0.0622\n",
      "Epoch 37/1000\n",
      "11201/11201 [==============================] - 3s 240us/sample - loss: 0.0525 - val_loss: 0.0631\n",
      "Epoch 38/1000\n",
      "11201/11201 [==============================] - 3s 243us/sample - loss: 0.0526 - val_loss: 0.0628\n",
      "Epoch 39/1000\n",
      "11008/11201 [============================>.] - ETA: 0s - loss: 0.0519\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 3.1622778103685084e-05.\n",
      "11201/11201 [==============================] - 3s 236us/sample - loss: 0.0521 - val_loss: 0.0627\n",
      "Epoch 40/1000\n",
      "11201/11201 [==============================] - 2s 221us/sample - loss: 0.0507 - val_loss: 0.0617\n",
      "Epoch 41/1000\n",
      "11201/11201 [==============================] - 2s 207us/sample - loss: 0.0497 - val_loss: 0.0612\n",
      "Epoch 42/1000\n",
      "11201/11201 [==============================] - 2s 198us/sample - loss: 0.0499 - val_loss: 0.0615\n",
      "Epoch 43/1000\n",
      "11201/11201 [==============================] - 2s 161us/sample - loss: 0.0495 - val_loss: 0.0618\n",
      "Epoch 44/1000\n",
      "11201/11201 [==============================] - 2s 156us/sample - loss: 0.0493 - val_loss: 0.0627\n",
      "Epoch 45/1000\n",
      "11201/11201 [==============================] - 2s 151us/sample - loss: 0.0497 - val_loss: 0.0621\n",
      "Epoch 46/1000\n",
      "11200/11201 [============================>.] - ETA: 0s - loss: 0.0493\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 1.0000000409520217e-05.\n",
      "11201/11201 [==============================] - 2s 148us/sample - loss: 0.0493 - val_loss: 0.0616\n",
      "Epoch 47/1000\n",
      "11201/11201 [==============================] - 2s 144us/sample - loss: 0.0489 - val_loss: 0.0617\n",
      "Epoch 48/1000\n",
      "11201/11201 [==============================] - 2s 137us/sample - loss: 0.0485 - val_loss: 0.0616\n",
      "Epoch 49/1000\n",
      "11201/11201 [==============================] - 2s 136us/sample - loss: 0.0489 - val_loss: 0.0625\n",
      "Epoch 50/1000\n",
      "11201/11201 [==============================] - 1s 132us/sample - loss: 0.0484 - val_loss: 0.0624\n",
      "Epoch 51/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11200/11201 [============================>.] - ETA: 0s - loss: 0.0488\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "11201/11201 [==============================] - 1s 132us/sample - loss: 0.0488 - val_loss: 0.0615\n",
      "Epoch 52/1000\n",
      "11201/11201 [==============================] - 1s 126us/sample - loss: 0.0485 - val_loss: 0.0619\n",
      "Epoch 53/1000\n",
      "11201/11201 [==============================] - 1s 126us/sample - loss: 0.0485 - val_loss: 0.0612\n",
      "Epoch 54/1000\n",
      "11201/11201 [==============================] - 1s 125us/sample - loss: 0.0483 - val_loss: 0.0616\n",
      "Epoch 55/1000\n",
      "11201/11201 [==============================] - 1s 124us/sample - loss: 0.0487 - val_loss: 0.0611\n",
      "Epoch 56/1000\n",
      "11201/11201 [==============================] - 1s 125us/sample - loss: 0.0488 - val_loss: 0.0619\n",
      "Epoch 57/1000\n",
      "11201/11201 [==============================] - 1s 126us/sample - loss: 0.0482 - val_loss: 0.0619\n",
      "Epoch 58/1000\n",
      "11201/11201 [==============================] - 1s 125us/sample - loss: 0.0480 - val_loss: 0.0614\n",
      "Epoch 59/1000\n",
      "11201/11201 [==============================] - 1s 124us/sample - loss: 0.0482 - val_loss: 0.0616\n",
      "Epoch 60/1000\n",
      "11201/11201 [==============================] - 1s 124us/sample - loss: 0.0480 - val_loss: 0.0622\n",
      "Epoch 61/1000\n",
      "11201/11201 [==============================] - 1s 124us/sample - loss: 0.0479 - val_loss: 0.0620\n",
      "Epoch 62/1000\n",
      " 5696/11201 [==============>...............] - ETA: 0s - loss: 0.0468WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: loss,lr\n"
     ]
    },
    {
     "ename": "_NotOkStatusException",
     "evalue": "InvalidArgumentError: Error while reading CompositeTensor._type_spec.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_NotOkStatusException\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-51ad79632af4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreduce_lr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m )\n",
      "\u001b[0;32m~/.local/share/virtualenvs/nlu-_DNmEEd7/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/.local/share/virtualenvs/nlu-_DNmEEd7/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/nlu-_DNmEEd7/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/nlu-_DNmEEd7/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/nlu-_DNmEEd7/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/nlu-_DNmEEd7/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    485\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/nlu-_DNmEEd7/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1820\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1822\u001b[0;31m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1823\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/nlu-_DNmEEd7/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2107\u001b[0m           *args, **kwargs)\n\u001b[1;32m   2108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2109\u001b[0;31m     \u001b[0mcache_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2111\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/nlu-_DNmEEd7/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_cache_key\u001b[0;34m(self, args, kwargs, include_tensor_ranks_only)\u001b[0m\n\u001b[1;32m   1956\u001b[0m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1957\u001b[0m       input_signature = pywrap_tensorflow.TFE_Py_EncodeArg(\n\u001b[0;32m-> 1958\u001b[0;31m           inputs, include_tensor_ranks_only)\n\u001b[0m\u001b[1;32m   1959\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31m_NotOkStatusException\u001b[0m: InvalidArgumentError: Error while reading CompositeTensor._type_spec."
     ]
    }
   ],
   "source": [
    "model = get_model(X_train.shape[1], X_syn_train.shape[1] + X_phon_train.shape[1])\n",
    "optimizer = tfk.optimizers.Adam(learning_rate=1e-3)\n",
    "model.compile(loss=abs_loss, optimizer=optimizer)\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "reduce_lr = tfk.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', \n",
    "    factor=np.sqrt(0.1),\n",
    "    patience=5, \n",
    "    min_lr=1e-5,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    batch_size=32,\n",
    "    epochs=1000,\n",
    "    callbacks=[reduce_lr],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = read_df('data/test.txt', COL_NAMES[:-3])\n",
    "test_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.hstack([\n",
    "    get_syn(test_df, DATA, testing=True), \n",
    "    get_emb(test_df, DATA, testing=True), \n",
    "    get_grams(test_df, DATA, testing=True),\n",
    "])\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test).ravel()\n",
    "predictions = np.clip(predictions, 0, 1)\n",
    "print(predictions.shape)\n",
    "print(predictions[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('submission.txt', 'w') as file:\n",
    "    lines = [\"id,label\"]\n",
    "    for idx, pred in zip(test_df[\"idx\"], predictions):\n",
    "        lines.append(f\"{idx},{pred:.1f}\")\n",
    "    file.write('\\n'.join(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
